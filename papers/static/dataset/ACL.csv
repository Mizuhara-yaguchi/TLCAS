index,year,title,author,abstract,link,pdf,affliation,name
1,2013,A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation,Yang Liu,"We introduce a shift-reduce parsing algorithm for phrase-based string-to- dependency translation. As the algorithm generates dependency trees for partial translations left-to-right in decoding, it allows for efficient integration of both n -gram and dependency language mod- els. To resolve conflicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data. As our approach combines the merits of phrase-based and string-to- dependency models, it achieves significant improvements over the two baselines on the NIST Chinese-English datasets. ",,,,ACL
2,2013,Integrating Translation Memory into Phrase-Based Machine Translation during Decoding,"Kun Wang, Chengqing Zong, Keh-Yih Su","Since statistical machine translation (SMT) and translation memory (TM) complement each other in matched and unmatched regions, integrated models are proposed in this paper to incorporate TM information into phrase-based SMT. Unlike previous multi-stage pipeline approaches, which directly merge TM result into the final output, the proposed models refer to the corresponding TM information associat- ed with each phrase at SMT decoding. On a Chinese – English TM database, our experi- ments show that the proposed integrated Mod- el-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Be- sides, the proposed models also outperform previous approaches significantly. ",,,,ACL
3,2013,Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment,Thomas Schoenemann,"We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and first order pa- rameters, are nondeficient. Subsequently, we proceed to derive a method to com- pute a likely alignment and its neighbors as well as give a solution of EM training. The arising M-step energies are non-trivial and handled via projected gradient ascent. Our evaluation on gold alignments shows substantial improvements (in weighted F- measure) for the IBM-3. For the IBM- 4 there are no consistent improvements. Training the nondeficient IBM-5 in the regular way gives surprisingly good re- sults. Using the resulting alignments for phrase- based translation systems offers no clear insights w.r.t. BLEU scores. ",,,,ACL
4,2013,Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation,"Trevor Cohn, Lucia Specia","Annotating linguistic data is often a com- plex, time consuming and expensive en- deavour. Even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpretations of the task and lev- els of consistency. We present novel tech- niques for learning from the outputs of multiple annotators while accounting for annotator specific behaviour. These tech- niques use multi-task Gaussian Processes to learn jointly a series of annotator and metadata specific models, while explicitly representing correlations between models which can be learned directly from data. Our experiments on two machine trans- lation quality estimation datasets show uniform significant accuracy gains from multi-task learning, and consistently out- perform strong baselines. ",,,,ACL
5,2013,Smoothed marginal distribution constraints for language modeling,"Brian Roark, Cyril Allauzen, Michael Riley","We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of well- known Kneser-Ney (1995) smoothing. Unlike Kneser-Ney, our approach is de- signed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retain- ing the benefits of such marginal distribu- tion constraints. We present experimen- tal results for heavily pruned backoff n- gram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm has been released as part of the OpenGrm ngram library. 1 ",,,,ACL
6,2013,Grounded Language Learning from Video Described with Sentences,"Haonan Yu, Jeffrey Mark Siskind","We present a method that learns repre- sentations for word meanings from short video clips paired with sentences. Un- like prior work on learning language from symbolic input, our input consists of video of people interacting with multiple com- plex objects in outdoor environments. Un- like prior computer-vision approaches that learn from videos with verb labels or im- ages with noun labels, our labels are sen- tences containing nouns, verbs, preposi- tions, adjectives, and adverbs. The cor- respondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts si- multaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video. ",,,,ACL
7,2013,"Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation","Mehdi Manshadi, Daniel Gildea, James Allen","Recent work on statistical quantifier scope disambiguation (QSD) has improved upon earlier work by scoping an arbitrary num- ber and type of noun phrases. No corpus- based method, however, has yet addressed QSD when incorporating the implicit uni- versal of plurals and/or operators such as negation. In this paper we report early, though promising, results for automatic QSD when handling both phenomena. We also present a general model for learning to build partial orders from a set of pair- wise preferences. We give an n log n algo- rithm for finding a guaranteed approxima- tion of the optimal solution, which works very well in practice. Finally, we signifi- cantly improve the performance of the pre- vious model using a rich set of automati- cally generated features. ",,,,ACL
8,2013,Joint Event Extraction via Structured Prediction with Global Features,"Qi Li, Heng Ji, Liang Huang","Traditional approaches to the task of ACE event extraction usually rely on sequential pipelines with multiple stages, which suf- fer from error propagation since event trig- gers and arguments are predicted in isola- tion by independent local classifiers. By contrast, we propose a joint framework based on structured prediction which ex- tracts triggers and arguments together so that the local predictions can be mutu- ally improved. In addition, we propose to incorporate global features which ex- plicitly capture the dependencies of multi- ple triggers and arguments. Experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features fur- ther improves the performance signifi- cantly. Our approach advances state-of- the-art sentence-level event extraction, and even outperforms previous argument la- beling methods which use external knowl- edge from other sentences and documents. ",,,,ACL
9,2013,Language-Independent Discriminative Parsing of Temporal Expressions,"Gabor Angeli, Jakob Uszkoreit","Temporal resolution systems are tradition- ally tuned to a particular language, re- quiring significant human effort to trans- late them to new languages. We present a language independent semantic parser for learning the interpretation of tempo- ral phrases given only a corpus of utter- ances and the times they reference. We make use of a latent parse that encodes a language-flexible representation of time, and extract rich features over both the parse and associated temporal semantics. The parameters of the model are learned using a weakly supervised bootstrapping approach, without the need for manually tuned parameters or any other language expertise. We achieve state-of-the-art ac- curacy on all languages in the TempEval- 2 temporal normalization task, reporting a 4% improvement in both English and Spanish accuracy, and to our knowledge the first results for four other languages. ",,,,ACL
10,2013,Graph-based Local Coherence Modeling,"Camille Guinaudeau, Michael Strube","We propose a computationally efficient graph-based approach for local coherence modeling. We evaluate our system on three tasks: sentence ordering, summary coherence rating and readability assess- ment. The performance is comparable to entity grid based approaches though these rely on a computationally expensive train- ing phase and face data sparsity problems. ",,,,ACL
11,2013,Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms,"Elijah Mayfield, David Adamson, Carolyn Penstein Rosé","Automated annotation of social behavior in conversation is necessary for large-scale analysis of real-world conversational data. Important behavioral categories, though, are often sparse and often appear only in specific subsections of a conversation. This makes supervised machine learning difficult, through a combination of noisy features and unbalanced class distribu- tions. We propose within-instance con- tent selection, using cue features to selec- tively suppress sections of text and bias- ing the remaining representation towards minority classes. We show the effective- ness of this technique in automated anno- tation of empowerment language in online support group chatrooms. Our technique is significantly more accurate than multi- ple baselines, especially when prioritizing high precision. ",,,,ACL
12,2013,Decentralized Entity-Level Modeling for Coreference Resolution,"Greg Durrett, David Hall, Dan Klein","Efficiently incorporating entity-level in- formation is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We de- scribe an end-to-end discriminative prob- abilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent men- tions. This model can be represented as a factor graph for each document that ad- mits efficient inference via belief propaga- tion. We show that our method can use entity-level information to outperform a basic pairwise system. ",,,,ACL
13,2013,Chinese Parsing Exploiting Characters,"Meishan Zhang, Yue Zhang, Wanxiang Che, Ting Liu","Characters play an important role in the Chinese language, yet computational pro- cessing of Chinese has been dominated by word-based approaches, with leaves in syntax trees being words. We investigate Chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. We demonstrate the importance of character-level information to Chinese processing by building a joint segmen- tation, part-of-speech (POS) tagging and phrase-structure parsing system that inte- grates character-structure features. Our joint system significantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test, and gives the best published results for Chinese parsing. ",,,,ACL
14,2013,A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy,"Francesco Sartorio, Giorgio Satta, Joakim Nivre","We present a novel transition-based, greedy dependency parser which implements a flexible mix of bottom-up and top-down strategies. The new strategy allows the parser to postpone difficult decisions until the relevant information becomes available. The novel parser has a ～ 12% error reduc- tion in unlabeled attachment score over an arc-eager parser, with a slow-down factor of 2.8. ",,,,ACL
15,2013,General binarization for parsing and translation,"Matthias Büchse, Alexander Koller, Heiko Vogler",Binarization of grammars is crucial for im- proving the complexity and performance of parsing and translation. We present a versatile binarization algorithm that can be tailored to a number of grammar for- malisms by simply varying a formal pa- rameter. We apply our algorithm to bi- narizing tree-to-string transducers used in syntax-based machine translation. ,,,,ACL
16,2013,Distortion Model Considering Rich Context for Statistical Machine Translation,"Isao Goto, Masao Utiyama, Eiichiro Sumita, Akihiro Tamura","This paper proposes new distortion mod- els for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by dis- criminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. ",,,,ACL
17,2013,Word Alignment Modeling with Context Dependent Deep Neural Network,"Nan Yang, Shujie Liu, Mu Li, Ming Zhou","In this paper, we explore a novel bilin- gual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in var- ious machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNN- HMM (Dahl et al., 2012) method intro- duced in speech recognition to the HMM- based word alignment model, in which bilingual word embedding is discrimina- tively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being ca- pable to model the rich bilingual corre- spondence, our method generates a very compact model with much fewer parame- ters. Experiments on a large scale English- Chinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score. ",,,,ACL
18,2013,Microblogs as Parallel Corpora,"Wang Ling, Guang Xiang, Chris Dyer, Alan Black","In the ever-expanding sea of microblog data, there is a surprising amount of naturally occurring par- allel text: some users create post multilingual mes- sages targeting international audiences while oth- ers “retweet” translations. We present an efficient method for detecting these messages and extract- ing parallel segments from them. We have been able to extract over 1M Chinese-English parallel segments from Sina Weibo (the Chinese counter- part of Twitter) using only their public APIs. As a supplement to existing parallel training data, our automatically extracted parallel data yields sub- stantial translation quality improvements in trans- lating microblog text and modest improvements in translating edited news commentary. The re- sources in described in this paper are available at http://www.cs.cmu.edu/ ～ lingwang/utopia. ",,,,ACL
19,2013,Improved Bayesian Logistic Supervised Topic Models with Data Augmentation,"Jun Zhu, Xun Zheng, Bo Zhang",Supervised topic models with a logistic likelihood have two issues that potential- ly limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-field assumptions. We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by intro- ducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restrict- ing assumptions and can be easily paral- lelized. Empirical results demonstrate sig- nificant improvements on prediction per- formance and time efficiency. ,,,,ACL
20,2013,Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning,"Miguel Almeida, André Martins","We present a dual decomposition frame- work for multi-document summarization, using a model that jointly extracts and compresses sentences. Compared with previous work based on integer linear pro- gramming, our approach does not require external solvers, is significantly faster, and is modular in the three qualities a sum- mary should have: conciseness, informa- tiveness, and grammaticality. In addition, we propose a multi-task learning frame- work to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC- 2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers. ",,,,ACL
21,2013,Unsupervised Transcription of Historical Documents,"Taylor Berg-Kirkpatrick, Greg Durrett, Dan Klein","We present a generative probabilistic model, inspired by historical printing pro- cesses, for transcribing images of docu- ments from the printing press era. By jointly modeling the text of the docu- ment and the noisy (but regular) process of rendering glyphs, our unsupervised sys- tem is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially out- performs state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading com- mercial system for historical transcription, and a 47% relative reduction over Tesser- act, Google’s open source OCR system. ",,,,ACL
22,2013,Adapting Discriminative Reranking to Grounded Language Learning,"Joohyun Kim, Raymond Mooney","We adapt discriminative reranking to im- prove the performance of grounded lan- guage acquisition, specifically the task of learning to follow navigation instructions from observation. Unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded set- ting. Instead, we show how the weak su- pervision of response feedback (e.g. suc- cessful task completion) can be used as an alternative, experimentally demonstrat- ing that its performance is comparable to training on gold-standard parse trees. ",,,,ACL
23,2013,Universal Conceptual Cognitive Annotation (UCCA),"Omri Abend, Ari Rappoport","Syntactic structures, by their nature, re- flect first and foremost the formal con- structions used for expressing meanings. This renders them sensitive to formal vari- ation both within and across languages, and limits their value to semantic ap- plications. We present UCCA, a novel multi-layered framework for semantic rep- resentation that aims to accommodate the semantic distinctions expressed through linguistic utterances. We demonstrate UCCA’s portability across domains and languages, and its relative insensitivity to meaning-preserving syntactic variation. We also show that UCCA can be ef- fectively and quickly learned by annota- tors with no linguistic background, and describe the compilation of a UCCA- annotated corpus. ",,,,ACL
24,2013,Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media,"Weiwei Guo, Hao Li, Heng Ji, Mona Diab","Many current Natural Language Process- ing [NLP] techniques work well assum- ing a large context of text as input data. However they become ineffective when applied to short texts such as Twitter feeds. To overcome the issue, we want to find a related newswire document to a given tweet to provide contextual support for NLP tasks. This requires robust model- ing and understanding of the semantics of short texts. The contribution of the paper is two-fold: 1. we introduce the Linking-Tweets-to- News task as well as a dataset of linked tweet-news pairs, which can benefit many NLP applications; 2. in contrast to previ- ous research which focuses on lexical fea- tures within the short texts (text-to-word information), we propose a graph based latent variable model that models the in- ter short text correlations (text-to-text in- formation). This is motivated by the ob- servation that a tweet usually only cov- ers one aspect of an event. We show that using tweet specific feature (hashtag) and news specific feature (named entities) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. Our experiments show significant im- provement of our new model over base- lines with three evaluation metrics in the new task. ",,,,ACL
25,2013,A computational approach to politeness with application to social factors,"Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec","We propose a computational framework for identifying linguistic aspects of polite- ness. Our starting point is a new corpus of requests annotated for politeness, which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context. These findings guide our construction of a classifier with domain-independent lexi- cal and syntactic features operationalizing key components of politeness theory, such as indirection, deference, impersonaliza- tion and modality. Our classifier achieves close to human performance and is effec- tive across domains. We use our frame- work to study the relationship between po- liteness and social power, showing that po- lite Wikipedia editors are more likely to achieve high status through elections, but, once elevated, they become less polite. We see a similar negative correlation between politeness and power on Stack Exchange, where users at the top of the reputation scale are less polite than those at the bot- tom. Finally, we apply our classifier to a preliminary analysis of politeness vari- ation by gender and community. ",,,,ACL
26,2013,Modeling Thesis Clarity in Student Essays,"Isaac Persing, Vincent Ng","Recently, researchers have begun explor- ing methods of scoring student essays with respect to particular dimensions of qual- ity such as coherence, technical errors, and relevance to prompt, but there is rel- atively little work on modeling thesis clar- ity. We present a new annotated corpus and propose a learning-based approach to scoring essays along the thesis clarity di- mension. Additionally, in order to pro- vide more valuable feedback on why an essay is scored as it is, we propose a sec- ond learning-based approach to identify- ing what kinds of errors an essay has that may lower its thesis clarity score. ",,,,ACL
27,2013,Translating Italian connectives into Italian Sign Language,"Camillo Lugaresi, Barbara Di Eugenio","We present a corpus analysis of how Ital- ian connectives are translated into LIS, the Italian Sign Language. Since corpus re- sources are scarce, we propose an align- ment method between the syntactic trees of the Italian sentence and of its LIS trans- lation. This method, and clustering ap- plied to its outputs, highlight the differ- ent ways a connective can be rendered in LIS: with a corresponding sign, by affect- ing the location or shape of other signs, or being omitted altogether. We translate these findings into a computational model that will be integrated into the pipeline of an existing Italian-LIS rendering system. Initial experiments to learn the four possi- ble translations with Decision Trees give promising results. ",,,,ACL
28,2013,Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing,"David Mareček, Milan Straka","Even though the quality of unsupervised dependency parsers grows, they often fail in recognition of very basic dependencies. In this paper, we exploit a prior knowledge of STOP-probabilities (whether a given word has any children in a given direc- tion), which is obtained from a large raw corpus using the reducibility principle. By incorporating this knowledge into Depen- dency Model with Valence, we managed to considerably outperform the state-of-the- art results in terms of average attachment score over 20 treebanks from CoNLL 2006 and 2007 shared tasks. ",,,,ACL
29,2013,Transfer Learning for Constituency-Based Grammars,"Yuan Zhang, Regina Barzilay, Amir Globerson","In this paper, we consider the problem of cross-formalism transfer in parsing. We are interested in parsing constituency- based grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quan- tity of coarse CFG annotations from the Penn Treebank. While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and se- mantic features. To handle this appar- ent discrepancy, we design a probabilistic model that jointly generates CFG and tar- get formalism parses. The model includes features of both parses, allowing trans- fer between the formalisms, while pre- serving parsing efficiency. We evaluate our approach on three constituency-based grammars — CCG, HPSG, and LFG, aug- mented with the Penn Treebank-1. Our ex- periments show that across all three for- malisms, the target parsers significantly benefit from the coarse annotations. 1 ",,,,ACL
30,2013,A Context Free TAG Variant,"Ben Swanson, Elif Yamangil, Eugene Charniak, Stuart Shieber","We propose a new variant of Tree- Adjoining Grammar that allows adjunc- tion of full wrapping trees but still bears only context-free expressivity. We provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters. This collapsed context-free form is used to implement efficient gram- mar estimation and parsing algorithms. We perform parsing experiments the Penn Treebank and draw comparisons to Tree- Substitution Grammars and between dif- ferent variations in probabilistic model de- sign. Examination of the most probable derivations reveals examples of the lin- guistically relevant structure that our vari- ant makes possible. ",,,,ACL
31,2013,Fast and Adaptive Online Training of Feature-Rich Translation Models,"Spence Green, Sida Wang, Daniel Cer, Christopher D. Manning","We present a fast and scalable online method for tuning statistical machine trans- lation models with large feature sets. The standard tuning algorithm—MERT—only scales to tens of features. Recent discrimi- native algorithms that accommodate sparse features have produced smaller than ex- pected translation quality gains in large systems. Our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs. Large-scale experiments on Arabic-English and Chinese-English show that our method produces significant trans- lation quality gains by exploiting sparse fea- tures. Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and ap- plies to other recent discriminative methods for machine translation. ",,,,ACL
32,2013,Advancements in Reordering Models for Statistical Machine Translation,"Minwei Feng, Jan-Thorsten Peter, Hermann Ney","In this paper, we propose a novel re- ordering model based on sequence label- ing techniques. Our model converts the reordering problem into a sequence label- ing problem, i.e. a tagging task. Results on five Chinese-English NIST tasks show that our model improves the baseline sys- tem by 1.32 B LEU and 1.53 T ER on av- erage. Results of comparative study with other seven widely used reordering mod- els will also be reported. ",,,,ACL
33,2013,A Markov Model of Machine Translation using Non-parametric Bayesian Inference,"Yang Feng, Trevor Cohn","Most modern machine translation systems use phrase pairs as translation units, al- lowing for accurate modelling of phrase- internal translation and reordering. How- ever phrase-based approaches are much less able to model sentence level effects between different phrase-pairs. We pro- pose a new model to address this im- balance, based on a word-based Markov model of translation which generates tar- get translations left-to-right. Our model encodes word and phrase level phenom- ena by conditioning translation decisions on previous decisions and uses a hierar- chical Pitman-Yor Process prior to pro- vide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over com- petitive baselines, of up to +3.4 BLEU. ",,,,ACL
34,2013,Scaling Semi-supervised Naive Bayes with Feature Marginals,"Michael Lucas, Doug Downey","Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL tech- niques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statis- tics calculated from the unlabeled corpus. In experiments with text topic classifica- tion and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previ- ous work. ",,,,ACL
35,2013,Learning Latent Personas of Film Characters,"David Bamman, Brendan O’Connor, Noah A. Smith","We present two latent variable models for learning character types, or personas, in film, in which a persona is defined as a set of mixtures over latent lexical classes. These lexical classes capture the stereo- typical actions of which a character is the agent and patient, as well as attributes by which they are described. As the first attempt to solve this problem explicitly, we also present a new dataset for the text-driven analysis of film, along with a benchmark testbed to help drive future work in this area. ",,,,ACL
36,2013,Scalable Decipherment for Machine Translation via Hash Sampling,Sujith Ravi,"In this paper, we propose a new Bayesian inference method to train statistical ma- chine translation systems using only non- parallel corpora. Following a probabilis- tic decipherment approach, we first intro- duce a new framework for decipherment training that is flexible enough to incorpo- rate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then de- rive a hash sampling strategy that is in- spired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex mod- els (for the first time) and large vocab- ulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving sig- nificant computational speedups (several orders faster). We also report for the first time—BLEU score results for a large- scale MT task using only non-parallel data (EMEA corpus). ",,,,ACL
37,2013,Automatic Interpretation of the English Possessive,"Stephen Tratz, Eduard Hovy","The English ’s possessive construction oc- curs frequently in text and can encode several different semantic relations; how- ever, it has received limited attention from the computational linguistics community. This paper describes the creation of a se- mantic relation inventory covering the use of ’s, an inter-annotator agreement study to calculate how well humans can agree on the relations, a large collection of pos- sessives annotated according to the rela- tions, and an accurate automatic annota- tion system for labeling new examples. Our 21,938 example dataset is by far the largest annotated possessives dataset we are aware of, and both our automatic clas- sification system, which achieves 87.4% accuracy in our classification experiment, and our annotation data are publicly avail- able. ",,,,ACL
38,2013,Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web,"Katsuma Narisawa, Yotaro Watanabe, Junta Mizuno, Naoaki Okazaki","This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e.g., three billion) is large, small, or nor- mal for a given context (e.g., number of people facing a water shortage). We first discuss the necessity of numerical com- mon sense in solving textual entailment problems. We explore two approaches for acquiring numerical common sense. Both approaches start with extracting numeri- cal expressions and their context from the Web. One approach estimates the distribu- tion of numbers co-occurring within a con- text and examines whether a given value is large, small, or normal, based on the distri- bution. Another approach utilizes textual patterns with which speakers explicitly ex- presses their judgment about the value of a numerical expression. Experimental re- sults demonstrate the effectiveness of both approaches. ",,,,ACL
39,2013,Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors,"Jackie Chi Kit Cheung, Gerald Penn","Generative probabilistic models have been used for content modelling and template induction, and are typically trained on small corpora in the target domain. In contrast, vector space models of distribu- tional semantics are trained on large cor- pora, but are typically applied to domain- general lexical disambiguation tasks. We introduce Distributional Semantic Hidden Markov Models, a novel variant of a hid- den Markov model that integrates these two approaches by incorporating contex- tualized distributional semantic vectors into a generative model as observed emis- sions. Experiments in slot induction show that our approach yields improvements in learning coherent entity clusters in a do- main. In a subsequent extrinsic evalua- tion, we show that these improvements are also reflected in multi-document summa- rization. ",,,,ACL
40,2013,Extracting bilingual terminologies from comparable corpora,"Ahmet Aker, Monica Paramita, Rob Gaizauskas","In this paper we present a method for extracting bilingual terminologies from comparable corpora. In our approach we treat bilingual term extrac- tion as a classification problem. For classification we use an SVM binary classifier and training data taken from the EUROVOC thesaurus. We test our approach on a held-out test set from EUROVOC and perform precision, recall and f-measure eval- uations for 20 European language pairs. The per- formance of our classifier reaches the 100% pre- cision level for many language pairs. We also perform manual evaluation on bilingual terms ex- tracted from English-German term-tagged compa- rable corpora. The results of this manual evalu- ation showed 60-83% of the term pairs generated are exact translations and over 90% exact or partial translations. ",,,,ACL
41,2013,The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis,"Kashyap Popat, Balamurali A.R, Pushpak Bhattacharyya, Gholamreza Haffari","Expensive feature engineering based on Word Net senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. In this paper, the problem of data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA. ",,,,ACL
42,2013,Large-scale Semantic Parsing via Schema Matching and Lexicon Extension,"Qingqing Cai, Alexander Yates","Supervised training procedures for seman- tic parsers produce high-quality semantic parsers, but they have difficulty scaling to large databases because of the sheer number of logical constants for which they must see labeled training data. We present a technique for developing seman- tic parsers for large databases based on a reduction to standard supervised train- ing algorithms, schema matching, and pat- tern learning. Leveraging techniques from each of these areas, we develop a semantic parser for Freebase that is capable of pars- ing questions with an F1 that improves by 0.42 over a purely-supervised learning al- gorithm. ",,,,ACL
43,2013,Fast and Accurate Shift-Reduce Constituent Parsing,"Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang","Shift-reduce dependency parsers give comparable accuracies to their chart- based counterparts, yet the best shift- reduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. ",,,,ACL
44,2013,Nonconvex Global Optimization for Latent-Variable Models,"Matthew R. Gormley, Jason Eisner","Many models in NLP involve latent vari- ables, such as unknown parses, tags, or alignments. Finding the optimal model pa- rameters is then usually a difficult noncon- vex optimization problem. The usual prac- tice is to settle for local optimization meth- ods such as EM or gradient ascent. We explore how one might instead search for a global optimum in parameter space, using branch-and-bound. Our method would eventually find the global maxi- mum (up to a user-specified ) if run for long enough, but at any point can return a suboptimal solution together with an up- per bound on the global maximum. As an illustrative case, we study a gener- ative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to for- mulate this as a mixed integer quadratic programming problem with nonlinear con- straints. We use the Reformulation Lin- earization Technique to produce convex relaxations during branch-and-bound. Al- though these techniques do not yet pro- vide a practical solution to our instance of this NP-hard problem, they sometimes find better solutions than Viterbi EM with random restarts, in the same time. ",,,,ACL
45,2013,Parsing with Compositional Vector Grammars,"Richard Socher, John Bauer, Christopher D. Manning, Andrew Y. Ng","Natural language parsing has typically been done with small sets of discrete cat- egories such as NP and VP, but this rep- resentation does not capture the full syn- tactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting cate- gories only partly address the problem at the cost of huge feature spaces and sparse- ness. Instead, we introduce a Compo- sitional Vector Grammar (CVG), which combines PCFGs with a syntactically un- tied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments. ",,,,ACL
46,2013,Discriminative state tracking for spoken dialog systems,"Angeliki Metallinou, Dan Bohus, Jason Williams","In spoken dialog systems, statistical state tracking aims to improve robustness to speech recognition errors by tracking a posterior distribution over hidden dialog states. Current approaches based on gener- ative or discriminative models have differ- ent but important shortcomings that limit their accuracy. In this paper we discuss these limitations and introduce a new ap- proach for discriminative state tracking that overcomes them by leveraging the problem structure. An offline evaluation with dialog data collected from real users shows improvements in both state track- ing accuracy and the quality of the pos- terior probabilities. Features that encode speech recognition error patterns are par- ticularly helpful, and training requires rel- atively few dialogs. ",,,,ACL
47,2013,Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition,"Man Lan, Yu Xu, Zhengyu Niu","To overcome the shortage of labeled data for implicit discourse relation recogni- tion, previous works attempted to auto- matically generate training data by remov- ing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learn- ing tasks, can achieve an averaged F 1 im- provement of 5.86% over baseline models. ",,,,ACL
48,2013,Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis,"Shafiq Joty, Giuseppe Carenini, Raymond Ng, Yashar Mehdad","We propose a novel approach for develop- ing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Con- ditional Random Fields: one for intra- sentential parsing and the other for multi- sentential parsing. We present two ap- proaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the state- of-the-art, often by a wide margin. ",,,,ACL
49,2013,Improving pairwise coreference models through feature space hierarchy learning,"Emmanuel Lassalle, Pascal Denis","This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we con- struct distinct classification models. In ef- fect, our approach finds an optimal fea- ture space (derived from a base feature set and indicator set) for discriminating coref- erential mention pairs. Although our ap- proach explores a very large space of pos- sible feature spaces, it remains tractable by exploiting the structure of the hierar- chies built from the indicators. Our exper- iments on the CoNLL-2012 Shared Task English datasets (gold mentions) indicate that our method is robust relative to dif- ferent clustering strategies and evaluation metrics, showing large and consistent im- provements over a single pairwise model using the same base features. Our best system obtains a competitive 67.2 of aver- age F1 over MUC, B 3 , and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets. ",,,,ACL
50,2013,Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval,"K. Tamsin Maxwell, Jon Oberlander, W. Bruce Croft","Techniques that compare short text seg- ments using dependency paths (or simply, paths) appear in a wide range of automated language processing applications including question answering (QA). However, few models in ad hoc information retrieval (IR) use paths for document ranking due to the prohibitive cost of parsing a retrieval collection. In this paper, we introduce a flexible notion of paths that describe chains of words on a dependency path. These chains, or catenae, are readily applied in standard IR models. Informative catenae are selected using supervised machine learning with linguistically informed fea- tures and compared to both non-linguistic terms and catenae selected heuristically with filters derived from work on paths. Automatically selected catenae of 1-2 words deliver significant performance gains on three TREC collections. ",,,,ACL
51,2013,Coordination Structures in Dependency Treebanks,"Martin Popel, David Mareček, Jan Štěpánek, Daniel Zeman","Paratactic syntactic structures are noto- riously difficult to represent in depen- dency formalisms. This has painful con- sequences such as high frequency of pars- ing errors related to coordination. In other words, coordination is a pending prob- lem in dependency analysis of natural lan- guages. This paper tries to shed some light on this area by bringing a system- atizing view of various formal means de- veloped for encoding coordination struc- tures. We introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages. In addition, empirical obser- vations on convertibility between selected styles of representations are shown too. ",,,,ACL
52,2013,GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web,"Flavio De Benedictis, Stefano Faralli, Roberto Navigli","We present GlossBoot, an effective minimally-supervised approach to ac- quiring wide-coverage domain glossaries for many languages. For each language of interest, given a small number of hypernymy relation seeds concerning a target domain, we bootstrap a glossary from the Web for that domain by means of iteratively acquired term/gloss extraction patterns. Our experiments show high performance in the acquisition of domain terminologies and glossaries for three different languages. ",,,,ACL
53,2013,Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model,"Ulle Endriss, Raquel Fernández","Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective anno- tation, taking inspiration from the field of social choice theory. We formulate a gen- eral formal model for collective annotation and propose several aggregation methods that go beyond the commonly used major- ity rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation. ",,,,ACL
54,2013,ParGramBank: The ParGram Parallel Treebank,"Sebastian Sulger, Miriam Butt, Tracy Holloway King, Paul Meurer","This paper discusses the construction of a parallel treebank currently involving ten languages from six language families. The treebank is based on deep LFG (Lexical- Functional Grammar) grammars that were developed within the framework of the ParGram (Parallel Grammar) effort. The grammars produce output that is maxi- mally parallelized across languages and language families. This output forms the basis of a parallel treebank covering a diverse set of phenomena. The treebank is publicly available via the INESS tree- banking environment, which also allows for the alignment of language pairs. We thus present a unique, multilayered paral- lel treebank that represents more and dif- ferent types of languages than are avail- able in other treebanks, that represents deep linguistic knowledge and that allows for the alignment of sentences at sev- eral levels: dependency structures, con- stituency structures and POS information. ",,,,ACL
55,2013,Identifying Bad Semantic Neighbors for Improving Distributional Thesauri,Olivier Ferret,"Distributional thesauri are now widely used in a large number of Natural Lan- guage Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an impor- tant issue that is mainly tackled indirectly through the improvement of semantic sim- ilarity measures. In this article, we pro- pose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification re- lies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by ap- plying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies. ",,,,ACL
56,2013,Models of Semantic Representation with Visual Attributes,"Carina Silberer, Vittorio Ferrari, Mirella Lapata",We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual at- tributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classi- fiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word as- sociation data compared to amodal models and word representations based on hand- crafted norming data. ,,,,ACL
57,2013,Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages,"Dan Garrette, Jason Mielens, Jason Baldridge","Developing natural language processing tools for low-resource languages often re- quires creating resources from scratch. While a variety of semi-supervised meth- ods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary. We dis- cuss a series of experiments designed to shed light on such questions in the con- text of part-of-speech tagging. We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy (as well as English) and eval- uate how the amounts of various kinds of data affect performance of a trained POS -tagger. Our results show that an- notation of word types is the most im- portant, provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. We also show that finite- state morphological analyzers are effective sources of type information when few la- beled examples are available. ",,,,ACL
58,2013,Using subcategorization knowledge to improve case prediction for translation to German,"Marion Weller, Alexander Fraser, Sabine Schulte im Walde","This paper demonstrates the need and im- pact of subcategorization information for SMT. We combine (i) features on source- side syntactic subcategorization and (ii) an external knowledge base with quantita- tive, dependency-based information about target-side subcategorization frames. A manual evaluation of an English-to- German translation task shows that the subcategorization information has a posi- tive impact on translation quality through better prediction of case. ",,,,ACL
59,2013,Name-aware Machine Translation,"Haibo Li, Jing Zheng, Heng Ji, Qi Li","We propose a Name-aware Machine Translation (MT) approach which can tightly integrate name processing into MT model, by jointly annotating parallel cor- pora, extracting name-aware translation grammar and rules, adding name phrase table and name translation driven decod- ing. Additionally, we also propose a new MT metric to appropriately evaluate the translation quality of informative words, by assigning different weights to differ- ent words according to their importance values in a document. Experiments on Chinese-English translation demonstrated the effectiveness of our approach on en- hancing the quality of overall translation, name translation and word alignment over a high-quality MT baseline 1 . ",,,,ACL
60,2013,Decipherment Complexity in 1:1 Substitution Ciphers,"Malte Nuhn, Hermann Ney","In this paper we show that even for the case of 1:1 substitution ciphers—which encipher plaintext symbols by exchang- ing them with a unique substitute—finding the optimal decipherment with respect to a bigram language model is NP-hard. We show that in this case the decipherment problem is equivalent to the quadratic as- signment problem (QAP). To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature be- fore. ",,,,ACL
61,2013,Non-Monotonic Sentence Alignment via Semisupervised Learning,"Xiaojun Quan, Chunyu Kit, Yan Song","This paper studies the problem of non- monotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisuper- vised learning approach based on two as- sumptions: (1) sentences with high affinity in one language tend to have their counter- parts with similar relatedness in the other; and (2) initial alignment is readily avail- able with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally op- timal solution. The evaluation with real- world legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data. ",,,,ACL
62,2013,Bootstrapping Entity Translation on Weakly Comparable Corpora,"Taesung Lee, Seung-won Hwang","This paper studies the problem of mining named entity translations from compara- ble corpora with some “asymmetry”. Un- like the previous approaches relying on the “symmetry” found in parallel corpora, the proposed method is tolerant to asymme- try often found in comparable corpora, by distinguishing different semantics of rela- tions of entity pairs to selectively prop- agate seed entity translations on weakly comparable corpora. Our experimental results on English-Chinese corpora show that our selective propagation approach outperforms the previous approaches in named entity translation in terms of the mean reciprocal rank by up to 0.16 for or- ganization names, and 0.14 in a low com- parability case. ",,,,ACL
63,2013,Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia,"Zhigang Wang, Zhixing Li, Juanzi Li, Jie Tang","Wikipedia infoboxes are a valuable source of structured knowledge for global knowl- edge sharing. However, infobox infor- mation is very incomplete and imbal- anced among the Wikipedias in differen- t languages. It is a promising but chal- lenging problem to utilize the rich struc- tured knowledge from a source language Wikipedia to help complete the missing in- foboxes for a target language. In this paper, we formulate the prob- lem of cross-lingual knowledge extraction from multilingual Wikipedia sources, and present a novel framework, called Wiki- CiKE, to solve this problem. An instance- based transfer learning method is utilized to overcome the problems of topic drift and translation errors. Our experimen- tal results demonstrate that Wiki CiKE out- performs the monolingual knowledge ex- traction method and the translation-based method. ",,,,ACL
64,2013,Bridging Languages through Etymology: The case of cross language text categorization,"Vivi Nastase, Carlo Strapparava","We propose the hypothesis that word ety- mology is useful for NLP applications as a bridge between languages. We support this hypothesis with experiments in cross- language (English-Italian) document cat- egorization. In a straightforward bag-of- words experimental set-up we add etymo- logical ancestors of the words in the docu- ments, and investigate the performance of a model built on English data, on Italian test data (and viceversa). The results show not only statistically significant, but a large improvement – a jump of almost 40 points in F1-score – over the raw (vanilla bag-of- words) representation. ",,,,ACL
65,2013,Creating Similarity: Lateral Thinking for Vertical Similarity Judgments,"Tony Veale, Guofu Li","Just as observing is more than just see- ing, comparing is far more than mere matching. It takes understanding, and even inventiveness, to discern a useful basis for judging two ideas as similar in a particular context, especially when our perspective is shaped by an act of linguis- tic creativity such as metaphor, simile or analogy. Structured resources such as WordNet offer a convenient hierarchical means for converging on a common ground for comparison, but offer little support for the divergent thinking that is needed to creatively view one concept as another. We describe such a means here, by showing how the web can be used to harvest many divergent views for many familiar ideas. These lateral views com- plement the vertical views of WordNet, and support a system for idea exploration called Thesaurus Rex. We show also how Thesaurus Rex supports a novel, genera- tive similarity measure for WordNet. ",,,,ACL
66,2013,Discovering User Interactions in Ideological Discussions,"Arjun Mukherjee, Bing Liu","Online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest. In forums where users discuss social, political, or religious issues, there are often heated debates among users or participants. Existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points. In this paper, we focus on identifying the nature of interactions among user pairs. The central questions are: How does each pair of users interact with each other? Does the pair of users mostly agree or disagree? What is the lexicon that people often use to express agreement and disagreement? We present a topic model based approach to answer these questions. Since agreement and disagreement expressions are usually multi- word phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling. After modeling, we use the modeling results to classify the nature of interaction of each user pair. Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques. ",,,,ACL
67,2013,Multilingual Affect Polarity and Valence Prediction in Metaphor-Rich Texts,Zornitsa Kozareva,"Metaphor is an important way of convey- ing the affect of people, hence understand- ing how people use metaphors to convey affect is important for the communication between individuals and increases cohe- sion if the perceived affect of the con- crete example is the same for the two in- dividuals. Therefore, building computa- tional models that can automatically iden- tify the affect in metaphor-rich texts like “The team captain is a rock.”, “Time is money.”, “My lawyer is a shark.” is an important challenging problem, which has been of great interest to the research com- munity. To solve this task, we have collected and manually annotated the affect of metaphor-rich texts for four languages. We present novel algorithms that integrate triggers for cognitive, affective, perceptual and social processes with stylistic and lex- ical information. By running evaluations on datasets in English, Spanish, Russian and Farsi, we show that the developed af- fect polarity and valence prediction tech- nology of metaphor-rich texts is portable and works equally well for different lan- guages. ",,,,ACL
68,2013,Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language,"Tiberiu Boros, Radu Ion, Dan Tufis","Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories). For this reason, a number of alternative methods have been proposed over the years. One of the most successful methods used for this task, exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. A second phase is aimed at recovering the full set of morpho-syntactic features. In this paper we present an alternative method to Tiered Tagging, based on local optimizations with Neural Networks and we show how, by properly encoding the input sequence in a general Neural Network architecture, we achieve results similar to the Tiered Tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method. ",,,,ACL
69,2013,Learning to lemmatise Polish noun phrases,Adam Radziszewski,"We present a novel approach to noun phrase lemmatisation where the main phase is cast as a tagging problem. The idea draws on the observation that the lemmatisation of almost all Polish noun phrases may be decomposed into trans- formation of singular words (tokens) that make up each phrase. We perform eval- uation, which shows results similar to those obtained earlier by a rule-based sys- tem, while our approach allows to separate chunking from lemmatisation. ",,,,ACL
70,2013,Using Conceptual Class Attributes to Characterize Social Media Users,"Shane Bergsma, Benjamin Van Durme","We describe a novel approach for automat- ically predicting the hidden demographic properties of social media users. Building on prior work in common-sense knowl- edge acquisition from third-person text, we first learn the distinguishing attributes of certain classes of people. For exam- ple, we learn that people in the Female class tend to have maiden names and en- gagement rings. We then show that this knowledge can be used in the analysis of first-person communication; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new train- ing examples. Our novel approach enables substantial improvements on the widely- studied task of user gender prediction, ob- taining a 20% relative error reduction over the current state-of-the-art. ",,,,ACL
71,2013,The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia,"Oliver Ferschke, Iryna Gurevych, Marc Rittberger","With the increasing amount of user gener- ated reference texts in the web, automatic quality assessment has become a key chal- lenge. However, only a small amount of annotated data is available for training quality assessment systems. Wikipedia contains a large amount of texts anno- tated with cleanup templates which iden- tify quality flaws. We show that the dis- tribution of these labels is topically bi- ased, since they cannot be applied freely to any arbitrary article. We argue that it is necessary to consider the topical restric- tions of each label in order to avoid a sam- pling bias that results in a skewed classifier and overly optimistic evaluation results. We factor out the topic bias by extracting reliable training instances from the revi- sion history which have a topic distribu- tion similar to the labeled articles. This ap- proach better reflects the situation a classi- fier would face in a real-life application. ",,,,ACL
72,2013,Mining Informal Language from Chinese Microtext: Joint Word Recognition and Segmentation,"Aobo Wang, Min-Yen Kan","We address the problem of informal word recognition in Chinese microblogs. A key problem is the lack of word delimiters in Chinese. We exploit this reliance as an opportunity: recognizing the relation be- tween informal word recognition and Chi- nese word segmentation, we propose to model the two tasks jointly. Our joint in- ference method significantly outperforms baseline systems that conduct the tasks in- dividually or sequentially. ",,,,ACL
73,2013,Generating Synthetic Comparable Questions for News Articles,"Oleg Rokhlenko, Idan Szpektor","We introduce the novel task of automati- cally generating questions that are relevant to a text but do not appear in it. One mo- tivating example of its application is for increasing user engagement around news articles by suggesting relevant compara- ble questions, such as “is Beyonce a bet- ter singer than Madonna?”, for the user to answer. We present the first algorithm for the task, which consists of: (a) of- fline construction of a comparable ques- tion template database; (b) ranking of rel- evant templates to a given article; and (c) instantiation of templates only with enti- ties in the article whose comparison un- der the template’s relation makes sense. We tested the suggestions generated by our algorithm via a Mechanical Turk ex- periment, which showed a significant im- provement over the strongest baseline of more than 45% in all metrics. ",,,,ACL
74,2013,Punctuation Prediction with Transition-based Parsing,"Dongdong Zhang, Shuangzhi Wu, Nan Yang, Mu Li","Punctuations are not available in automatic speech recognition outputs, which could cre- ate barriers to many subsequent text pro- cessing tasks. This paper proposes a novel method to predict punctuation symbols for the stream of words in transcribed speech texts. Our method jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features when processing words from left to right. It can exploit a global view to capture long-range dependencies for punc- tuation prediction with linear complexity. The experimental results on the test data sets of IWSLT and TDT4 show that our method can achieve high-level performance in punctuation prediction over the stream of words in tran- scribed speech text. ",,,,ACL
75,2013,Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study,"Wenbin Jiang, Meng Sun, Yajuan Lü, Yating Yang","Structural information in web text pro- vides natural annotations for NLP prob- lems such as word segmentation and pars- ing. In this paper we propose a discrim- inative learning algorithm to take advan- tage of the linguistic knowledge in large amounts of natural annotations on the In- ternet. It utilizes the Internet as an external corpus with massive (although slight and sparse) natural annotations, and enables a classifier to evolve on the large-scaled and real-time updated web text. With Chinese word segmentation as a case study, exper- iments show that the segmenter enhanced with the Chinese wikipedia achieves sig- nificant improvement on a series of testing sets from different domains, even with a single classifier and local features. ",,,,ACL
76,2013,Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging,"Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, Isabel Trancoso","This paper introduces a graph-based semi- supervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor simi- larity graph over all trigrams of labeled and unlabeled data for propagating syn- tactic information, i.e., label distribution- s. The derived label distributions are re- garded as virtual evidences to regular- ize the learning of linear conditional ran- dom fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better result- s than the supervised baselines and other competitive semi-supervised CRFs in this task. ",,,,ACL
77,2013,An Infinite Hierarchical Bayesian Model of Phrasal Translation,"Trevor Cohn, Gholamreza Haffari","Modern phrase-based machine translation systems make extensive use of word- based translation models for inducing alignments from parallel corpora. This is problematic, as the systems are inca- pable of accurately modelling many trans- lation phenomena that do not decompose into word-for-word translation. This pa- per presents a novel method for induc- ing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminat- ing at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competi- tive baseline systems. ",,,,ACL
78,2013,Additive Neural Networks for Statistical Machine Translation,"Lemao Liu, Taro Watanabe, Eiichiro Sumita, Tiejun Zhao","Most statistical machine translation (SMT) systems are modeled using a log- linear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential. A neural network is a reasonable method to address these pitfalls. However, modeling SMT with a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embed- ding features on Chinese-to-English and Japanese-to-English translation tasks. ",,,,ACL
79,2013,Hierarchical Phrase Table Combination for Machine Translation,"Conghui Zhu, Taro Watanabe, Eiichiro Sumita, Tiejun Zhao","Typical statistical machine translation sys- tems are batch trained with a given train- ing data and their performances are large- ly influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training ev- ery time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In par- ticular, we train a Bayesian phrasal inver- sion transduction grammars for each do- main separately. The learned phrase ta- bles are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the tra- ditional batch training method. Further- more, each phrase table is trained sepa- rately in each domain, and while compu- tational overhead is significantly reduced by training them in parallel. ",,,,ACL
80,2013,Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation,"Fabienne Braune, Nina Seemann, Daniel Quernheim, Andreas Maletti","We present a new translation model in- tegrating the shallow local multi bottom- up tree transducer. We perform a large- scale empirical evaluation of our obtained system, which demonstrates that we sig- nificantly beat a realistic tree-to-tree base- line on the WMT 2009 English → German translation task. As an additional contribu- tion we make the developed software and complete tool-chain publicly available for further experimentation. ",,,,ACL
81,2013,Enlisting the Ghost: Modeling Empty Categories for Machine Translation,"Bing Xiang, Xiaoqiang Luo, Bowen Zhou","Empty categories (EC) are artificial ele- ments in Penn Treebanks motivated by the government-binding (GB) theory to ex- plain certain language phenomena such as pro-drop. ECs are ubiquitous in languages like Chinese, but they are tacitly ignored in most machine translation (MT) work because of their elusive nature. In this paper we present a comprehensive treat- ment of ECs by first recovering them with a structured MaxEnt model with a rich set of syntactic and lexical features, and then incorporating the predicted ECs into a Chinese-to-English machine translation task through multiple approaches, includ- ing the extraction of EC-specific sparse features. We show that the recovered empty categories not only improve the word alignment quality, but also lead to significant improvements in a large-scale state-of-the-art syntactic MT system. ",,,,ACL
82,2013,A Multi-Domain Translation Model Framework for Statistical Machine Translation,"Rico Sennrich, Holger Schwenk, Walid Aransa","While domain adaptation techniques for SMT have proven to be effective at im- proving translation quality, their practical- ity for a multi-domain environment is of- ten limited because of the computational and human costs of developing and main- taining multiple systems adapted to differ- ent domains. We present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also de- scribe a method for unsupervised adapta- tion with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effec- tiveness of both our translation model ar- chitecture and automatic clustering, with gains of up to ",,,,ACL
83,2013,Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation,"Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Hiroya Takamura","This paper proposes a nonparametric Bayesian method for inducing Part-of- Speech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilin- gual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned tar- get word, either jointly (joint model), or independently (independent model). Eval- uations of Japanese-to-English translation on the NTCIR-9 data show that our in- duced Japanese POS tags for dependency trees improve the performance of a forest- to-string SMT system. Our independent model gains over 1 point in BLEU by re- solving the sparseness problem introduced in the joint model. ",,,,ACL
84,2013,Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization,"Guangyou Zhou, Fang Liu, Yang Liu, Shizhu He","Community question answering (CQA) has become an increasingly popular re- search topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implic- itly expanding the queried questions with additional words or phrases using mono- lingual translation models. While use- ful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they are troubled by noise issue. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advan- tage of potentially rich semantic informa- tion drawn from other languages. Our pro- posed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other lan- guages via matrix factorization. Experi- ments conducted on a real CQA data show that our proposed approach is promising. ",,,,ACL
85,2013,Improved Lexical Acquisition through DPP-based Verb Clustering,"Roi Reichart, Anna Korhonen","Subcategorization frames (SCFs), selec- tional preferences (SPs) and verb classes capture related aspects of the predicate- argument structure. We present the first unified framework for unsupervised learn- ing of these three types of information. We show how to utilize Determinantal Point Processes (DPPs), elegant proba- bilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high qual- ity and diverse subsets, for clustering. Our novel clustering algorithm constructs a joint SCF-DPP DPP kernel matrix and uti- lizes the efficient sampling algorithms of DPPs to cluster together verbs with sim- ilar SCFs and SPs. We evaluate the in- duced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1 . ",,,,ACL
86,2013,Semantic Frames to Predict Stock Price Movement,"Boyi Xie, Rebecca J. Passonneau, Leon Wu, Germán G. Creamer","Semantic frames are a rich linguistic re- source. There has been much work on semantic frame parsers, but less that applies them to general NLP problems. We address a task to predict change in stock price from financial news. Seman- tic frames help to generalize from spe- cific sentences to scenarios, and to de- tect the (positive or negative) roles of spe- cific companies. We introduce a novel tree representation, and use it to train predic- tive models with tree kernels using sup- port vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that fea- tures derived from semantic frame pars- ing have significantly better performance across years on the polarity task. ",,,,ACL
87,2013,Density Maximization in Context-Sense Metric Space for All-words WSD,"Koichi Tanigaki, Mitsuteru Shiba, Tatsuji Munaka, Yoshinori Sagisaka","This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disam- biguation from untagged corpora. By gen- eralizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity result- ing from a large variety of linguistic con- text and sense, as well as to exploit sense- interdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic ker- nels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superi- ority of the proposed method over conven- tional ones by showing the better perfor- mances beyond most-frequent-sense base- line performance where none of SemEval- 2 unsupervised systems reached. ",,,,ACL
88,2013,The Role of Syntax in Vector Space Models of Compositional Semantics,"Karl Moritz Hermann, Phil Blunsom","Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a funda- mental task of Natural Language Process- ing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Com- binatory Categorial Autoencoders. This model leverages the CCG combinatory op- erators to guide a non-linear transforma- tion of meaning within a sentence. We use this model to learn high dimensional em- beddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a con- cise model to learn representations that are both effective and general. ",,,,ACL
89,2013,Margin-based Decomposed Amortized Inference,"Gourab Kundu, Vivek Srikumar, Dan Roth","Given that structured output prediction is typically performed over entire datasets, one natural question is whether it is pos- sible to re-use computation from earlier inference instances to speed up inference for future instances. Amortized inference has been proposed as a way to accomplish this. In this paper, first, we introduce a new amortized inference algorithm called the Margin-based Amortized Inference, which uses the notion of structured margin to identify inference problems for which pre- vious solutions are provably optimal. Sec- ond, we introduce decomposed amortized inference, which is designed to address very large inference problems, where ear- lier amortization methods become less ef- fective. This approach works by decom- posing the output structure and applying amortization piece-wise, thus increasing the chance that we can re-use previous so- lutions for parts of the output structure. These parts are then combined to a global coherent solution using Lagrangian relax- ation. In our experiments, using the NLP tasks of semantic role labeling and entity- relation extraction, we demonstrate that with the margin-based algorithm, we need to call the inference engine only for a third of the test examples. Further, we show that the decomposed variant of margin-based amortized inference achieves a greater re- duction in the number of inference calls. ",,,,ACL
90,2013,Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression,"Asli Celikyilmaz, Dilek Hakkani-Tur, Gokhan Tur, Ruhi Sarikaya","Finding concepts in natural language ut- terances is a challenging task, especially given the scarcity of labeled data for learn- ing semantic ambiguity. Furthermore, data mismatch issues, which arise when the expected test (target) data does not exactly match the training data, aggra- vate this scarcity problem. To deal with these issues, we describe an efficient semi- supervised learning (SSL) approach which has two components: (i) Markov Topic Regression is a new probabilistic model to cluster words into semantic tags (con- cepts). It can efficiently handle seman- tic ambiguity by extending standard topic models with two new features. First, it en- codes word n-gram features from labeled source and unlabeled target data. Sec- ond, by going beyond a bag-of-words ap- proach, it takes into account the inherent sequential nature of utterances to learn se- mantic classes based on context. (ii) Ret- rospective Learner is a new learning tech- nique that adapts to the unlabeled target data. Our new SSL approach improves semantic tagging performance by 3% ab- solute over the baseline models, and also compares favorably on semi-supervised syntactic tagging. ",,,,ACL
91,2013,Parsing Graphs with Hyperedge Replacement Grammars,"David Chiang, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann","Hyperedge replacement grammar (HRG) is a formalism for generating and trans- forming graphs that has potential appli- cations in natural language understand- ing and generation. A recognition al- gorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimiza- tion analogous to binarization of context- free grammars, and some important im- plementation details, resulting in an algo- rithm that is practical for natural-language applications. The algorithm is part of Boli- nas, a new software toolkit for HRG pro- cessing. ",,,,ACL
92,2013,Grounded Unsupervised Semantic Parsing,Hoifung Poon,"We present the first unsupervised ap- proach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. Our GUSP system produces a semantic parse by annotat- ing the dependency-tree nodes and edges with latent states, and learns a proba- bilistic grammar using EM. To compen- sate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by su- pervised approaches. ",,,,ACL
93,2013,Automatic detection of deception in child-produced speech using syntactic complexity features,"Maria Yancheva, Frank Rudzicz","It is important that the testimony of chil- dren be admissible in court, especially given allegations of abuse. Unfortunately, children can be misled by interrogators or might offer false information, with dire consequences. In this work, we evalu- ate various parameterizations of five clas- sifiers (including support vector machines, neural networks, and random forests) in deciphering truth from lies given tran- scripts of interviews with 198 victims of abuse between the ages of 4 and 7. These evaluations are performed using a novel set of syntactic features, including mea- sures of complexity. Our results show that sentence length, the mean number of clauses per utterance, and the Stajner- Mitkov measure of complexity are highly informative syntactic features, that classi- fication accuracy varies greatly by the age of the speaker, and that accuracy up to 91.7 % can be achieved by support vec- tor machines given a sufficient amount of data. ",,,,ACL
94,2013,Sentiment Relevance,"Christian Scheible, Hinrich Schütze","A number of different notions, including subjectivity, have been proposed for dis- tinguishing parts of documents that con- vey sentiment from those that do not. We propose a new concept, sentiment rele- vance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demon- strate experimentally that sentiment rele- vance and subjectivity are related, but dif- ferent. Since no large amount of labeled training data for our new notion of sen- timent relevance is available, we investi- gate two semi-supervised methods for cre- ating sentiment relevance classifiers: a dis- tant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge trans- fer. We show that both methods learn sen- timent relevance classifiers that perform well. ",,,,ACL
95,2013,Predicting and Eliciting Addressee’s Emotion in Online Dialogue,"Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga, Masashi Toyoda","While there have been many attempts to estimate the emotion of an addresser from her/his utterance, few studies have ex- plored how her/his utterance affects the emotion of the addressee. This has mo- tivated us to investigate two novel tasks: predicting the emotion of the addressee and generating a response that elicits a specific emotion in the addressee’s mind. We target Japanese Twitter posts as a source of dialogue data and automatically build training data for learning the pre- dictors and generators. The feasibility of our approaches is assessed by using 1099 utterance-response pairs that are built by five human workers. ",,,,ACL
96,2013,Utterance-Level Multimodal Sentiment Analysis,"Verónica Pérez-Rosas, Rada Mihalcea, Louis-Philippe Morency","During real-life interactions, people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions. With the recent growth of social websites such as YouTube, Facebook, and Amazon, video reviews are emerging as a new source of multimodal and natural opinions that has been left al- most untapped by automatic opinion anal- ysis techniques. This paper presents a method for multimodal sentiment classi- fication, which can identify the sentiment expressed in utterance-level visual datas- treams. Using a new multimodal dataset consisting of sentiment annotated utter- ances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality. ",,,,ACL
97,2013,Probabilistic Sense Sentiment Similarity through Hidden Emotions,"Mitra Mohtarami, Man Lan, Chew Lim Tan","Sentiment Similarity of word pairs reflects the distance between the words regarding their underlying sentiments. This paper aims to in- fer the sentiment similarity between word pairs with respect to their senses. To achieve this aim, we propose a probabilistic emotion- based approach that is built on a hidden emo- tional model. The model aims to predict a vec- tor of basic human emotions for each sense of the words. The resultant emotional vectors are then employed to infer the sentiment similarity of word pairs. We apply the proposed ap- proach to address two main NLP tasks, name- ly, Indirect yes/no Question Answer Pairs in- ference and Sentiment Orientation prediction. Extensive experiments demonstrate the effec- tiveness of the proposed approach. ",,,,ACL
98,2013,A user-centric model of voting intention from Social Media,"Vasileios Lampos, Daniel Preoţiuc-Pietro, Trevor Cohn","Social Media contain a multitude of user opinions which can be used to predict real- world phenomena in many domains in- cluding politics, finance and health. Most existing methods treat these problems as linear regression, learning to relate word frequencies and other simple features to a known response variable (e.g., voting intention polls or financial indicators). These techniques require very careful fil- tering of the input texts, as most Social Media posts are irrelevant to the task. In this paper, we present a novel approach which performs high quality filtering au- tomatically, through modelling not just words but also users, framed as a bilin- ear model with a sparse regulariser. We also consider the problem of modelling groups of related output variables, us- ing a structured multi-task regularisation method. Our experiments on voting inten- tion prediction demonstrate strong perfor- mance over large-scale input from Twitter on two distinct case studies, outperform- ing competitive baselines. ",,,,ACL
99,2013,Using Supervised Bigram-based ILP for Extractive Summarization,"Chen Li, Xian Qian, Yang Liu","In this paper, we propose a bigram based supervised method for extractive docu- ment summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to es- timate its frequency in the reference sum- mary. The regression model uses a vari- ety of indicative features and is trained dis- criminatively to minimize the distance be- tween the estimated and the ground truth bigram frequency in the reference sum- mary. During testing, the sentence selec- tion problem is formulated as an ILP prob- lem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also con- ducted various analysis to show the im- pact of bigram selection, weight estima- tion, and ILP setup. ",,,,ACL
100,2013,Summarization Through Submodularity and Dispersion,"Anirban Dasgupta, Ravi Kumar, Sujith Ravi","We propose a new optimization frame- work for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the sum- marization desideratum is expressed as a sum of a submodular function and a non- submodular function, which we call dis- persion; the latter uses inter-sentence dis- similarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion func- tions and show that a greedy algorithm can obtain an approximately optimal sum- mary in all three cases. We conduct ex- periments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algo- rithm outperforms those that rely only on submodularity. ",,,,ACL
101,2013,Subtree Extractive Summarization via Submodular Maximization,"Hajime Morita, Ryohei Sasano, Hiroya Takamura, Manabu Okumura","This study proposes a text summarization model that simultaneously performs sen- tence extraction and compression. We translate the text summarization task into a problem of extracting a set of depen- dency subtrees in the document cluster. We also encode obligatory case constraints as must-link dependency constraints in or- der to guarantee the readability of the gen- erated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 1 2 (1 ? e ? 1 ) . Our experiments with the NTCIR ACLIA test collections show that our approach outper- forms a state-of-the-art algorithm. ",,,,ACL
102,2013,The effect of non-tightness on Bayesian estimation of PCFGs,"Shay B. Cohen, Mark Johnson","Probabilistic context-free grammars have the unusual property of not always defin- ing tight distributions (i.e., the sum of the “probabilities” of the trees the grammar generates can be less than one). This paper reviews how this non-tightness can arise and discusses its impact on Bayesian es- timation of PCFGs. We begin by present- ing the notion of “almost everywhere tight grammars” and show that linear CFGs fol- low it. We then propose three different ways of reinterpreting non-tight PCFGs to make them tight, show that the Bayesian estimators in Johnson et al. (2007) are correct under one of them, and provide MCMC samplers for the other two. We conclude with a discussion of the impact of tightness empirically. ",,,,ACL
103,2013,Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources,"Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka, Yusuke Miyao","This paper describes a method of in- ducing wide-coverage CCG resources for Japanese. While deep parsers with corpus- induced grammars have been emerging for some languages, those for Japanese have not been widely studied, mainly be- cause most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based cor- pora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexi- con and the accuracy of parsing. ",,,,ACL
104,2013,Transition-based Dependency Parsing with Selectional Branching,"Jinho D. Choi, Andrew McCallum","We present a novel approach, called selec- tional branching, which uses confidence es- timates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected lin- ear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transition- based parser that uses beam search. ",,,,ACL
105,2013,Bilingually-Guided Monolingual Dependency Grammar Induction,"Kai Liu, Yajuan Lü, Wenbin Jiang, Qun Liu","This paper describes a novel strategy for automatic induction of a monolingual de- pendency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency in- formation projected from the parsed coun- terpart language, and simultaneously min- ing the underlying syntactic structure of the language considered, it effectively in- tegrates the advantages of bilingual pro- jection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised in- duction. We induced dependency gram- mar for five different languages under the guidance of dependency information pro- jected from the parsed English translation, experiments show that the bilingually- guided method achieves a significant improvement of 28.5% over the unsuper- vised baseline and 3.0% over the best pro- jection baseline on average. ",,,,ACL
106,2013,Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition,"Mengqiu Wang, Wanxiang Che, Christopher D. Manning","Translated bi-texts contain complemen- tary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in perfor- mance over monolingual taggers by pro- moting agreement of tagging decisions be- tween the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mis- takes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two uni- directional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tag- ging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the com- bined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields signif- icant improvements in both NER and word alignment over state-of-the-art monolin- gual baselines. ",,,,ACL
107,2013,Resolving Entity Morphs in Censored Data,"Hongzhao Huang, Zhen Wen, Dian Yu, Heng Ji","In some societies, internet users have to create information morphs (e.g. “Peace West King” to refer to “Bo Xilai”) to avoid active censorship or achieve other com- munication goals. In this paper we aim to solve a new problem of resolving en- tity morphs to their real targets. We ex- ploit temporal constraints to collect cross- source comparable corpora relevant to any given morph query and identify target can- didates. Then we propose various novel similarity measurements including surface features, meta-path based semantic fea- tures and social correlation features and combine them in a learning-to-rank frame- work. Experimental results on Chinese Sina Weibo data demonstrate that our ap- proach is promising and significantly out- performs baseline methods 1 . ",,,,ACL
108,2013,Learning to Extract International Relations from Political Context,"Brendan O’Connor, Brandon M. Stewart, Noah A. Smith","We describe a new probabilistic model for extracting events between major polit- ical actors from news corpora. Our un- supervised model brings together famil- iar components in natural language pro- cessing (like parsers and topic models) with contextual political information— temporal and dyad dependence—to in- fer latent event classes. We quantita- tively evaluate the model’s performance on political science benchmarks: recover- ing expert-assigned event class valences, and detecting real-world conflict. We also conduct a small case study based on our model’s inferences. A supplementary appendix, and replica- tion software/data are available online, at: http://brenocon.com/irevents ",,,,ACL
109,2013,Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation,"Majid Razmara, Maryam Siahbani, Reza Haffari, Anoop Sarkar","Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language transla- tions. Experimental results show that our graph propagation method significantly improves per- formance over two strong baselines under intrin- sic and extrinsic evaluation metrics. ",,,,ACL
110,2013,Online Relative Margin Maximization for Statistical Machine Translation,"Vladimir Eidelman, Yuval Marton, Philip Resnik","Recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization, such as the spread of the data. However, these so- lutions are impractical in complex struc- tured prediction problems such as statis- tical machine translation. We present an online gradient-based algorithm for rela- tive margin maximization, which bounds the spread of the projected data while max- imizing the margin. We evaluate our op- timizer on Chinese-English and Arabic- English translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant im- provements of 1.2-2 B LEU and 1.7-4.3 TER on average over state-of-the-art opti- mizers with the large feature set. ",,,,ACL
111,2013,Handling Ambiguities of Bilingual Predicate-Argument Structures for Statistical Machine Translation,"Feifei Zhai, Jiajun Zhang, Yu Zhou, Chengqing Zong","Predicate-argument structure (PAS) has been demonstrated to be very effective in improving SMT performance. However, since a source- side PAS might correspond to multiple differ- ent target-side PASs, there usually exist many PAS ambiguities during translation. In this pa- per, we group PAS ambiguities into two types: role ambiguity and gap ambiguity. Then we propose two novel methods to handle the two PAS ambiguities for SMT accordingly: 1) in- side context integration; 2) a novel maximum entropy PAS disambiguation (MEPD) model. In this way, we incorporate rich context in- formation of PAS for disambiguation. Then we integrate the two methods into a PAS- based translation framework. Experiments show that our approach helps to achieve sig- nificant improvements on translation quality. ",,,,ACL
112,2013,Reconstructing an Indo-European Family Tree from Non-native English Texts,"Ryo Nagata, Edward Whittaker","Mother tongue interference is the phe- nomenon where linguistic systems of a mother tongue are transferred to another language. Although there has been plenty of work on mother tongue interference, very little is known about how strongly it is transferred to another language and about what relation there is across mother tongues. To address these questions, this paper explores and visualizes mother tongue interference preserved in English texts written by Indo-European language speakers. This paper further explores lin- guistic features that explain why certain relations are preserved in English writing, and which contribute to related tasks such as native language identification. ",,,,ACL
113,2013,Word Association Profiles and their Use for Automated Scoring of Essays,"Beata Beigman Klebanov, Michael Flor","We describe a new representation of the content vocabulary of a text we call word association profile that captures the pro- portions of highly associated, mildly asso- ciated, unassociated, and dis-associated pairs of words that co-exist in the given text. We illustrate the shape of the dis- tirbution and observe variation with genre and target audience. We present a study of the relationship between quality of writ- ing and word association profiles. For a set of essays written by college graduates on a number of general topics, we show that the higher scoring essays tend to have higher percentages of both highly asso- ciated and dis-associated pairs, and lower percentages of mildly associated pairs of words. Finally, we use word association profiles to improve a system for automated scoring of essays. ",,,,ACL
114,2013,Adaptive Parser-Centric Text Normalization,"Congle Zhang, Tyler Baldwin, Howard Ho, Benny Kimelfeld","Text normalization is an important first step towards enabling many Natural Lan- guage Processing (NLP) tasks over infor- mal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly define the task in the word-to-word sense; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of nor- malization on the parser, we tie normal- ization performance directly to parser per- formance. Additionally, we design a cus- tomizable framework to address the often overlooked concept of domain adaptabil- ity, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimen- tal study over datasets from three domains demonstrates that our approach outper- forms not only the state-of-the-art word- to-word normalization techniques, but also manual word-to-word annotations. ",,,,ACL
115,2013,A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation,"Zhenhua Tian, Hengheng Xiang, Ziqi Liu, Qinghua Zheng","This paper presents an unsupervised ran- dom walk approach to alleviate data spar- sity for selectional preferences. Based on the measure of preferences between predi- cates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predi- cate’s smoothed preferences. Experimen- tal results show that this approach out- performs several state-of-the-art method- s on the pseudo-disambiguation task, and it better correlates with human plausibility judgements. ",,,,ACL
116,2013,ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling,"Egoitz Laparra, German Rigau","This paper presents a novel deterministic algorithm for implicit Semantic Role La- beling. The system exploits a very sim- ple but relevant discursive property, the ar- gument coherence over different instances of a predicate. The algorithm solves the implicit arguments sequentially, exploit- ing not only explicit but also the implicit arguments previously solved. In addition, we empirically demonstrate that the algo- rithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data. ",,,,ACL
117,2013,Cross-lingual Transfer of Semantic Role Labeling Models,"Mikhail Kozhevnikov, Ivan Titov","Semantic Role Labeling (SRL) has be- come one of the standard tasks of natural language processing and proven useful as a source of information for a number of other applications. We address the prob- lem of transferring an SRL model from one language to another using a shared feature representation. This approach is then evaluated on three language pairs, demonstrating competitive performance as compared to a state-of-the-art unsuper- vised SRL system and a cross-lingual an- notation projection baseline. We also con- sider the contribution of different aspects of the feature representation to the perfor- mance of the model and discuss practical applicability of this method. ",,,,ACL
118,2013,DErivBase: Inducing and Evaluating a Derivational Morphology Resource for German,"Britta Zeller, Jan Šnajder, Sebastian Padó","Derivational models are still an under- researched area in computational morphol- ogy. Even for German, a rather resource- rich language, there is a lack of large- coverage derivational knowledge. This pa- per describes a rule-based framework for inducing derivational families (i.e., clus- ters of lemmas in derivational relation- ships) and its application to create a high- coverage German resource, DE RIV B ASE , mapping over 280k lemmas into more than 17k non-singleton clusters. We focus on the rule component and a qualitative and quan- titative evaluation. Our approach achieves up to 93% precision and 71% recall. We attribute the high precision to the fact that our rules are based on information from grammar books. ",,,,ACL
119,2013,Crowdsourcing Interaction Logs to Understand Text Reuse from the Web,"Martin Potthast, Matthias Hagen, Michael Völske, Benno Stein","We report on the construction of the Webis text reuse corpus 2012 for advanced re- search on text reuse. The corpus compiles manually written documents obtained from a completely controlled, yet representative environment that emulates the web. Each of the 297 documents in the corpus is about one of the 150 topics used at the TREC Web Tracks 2009–2011, thus forming a strong connection with existing evaluation efforts. Writers, hired at the crowdsourc- ing platform oDesk, had to retrieve sources for a given topic and to reuse text from what they found. Part of the corpus are detailed interaction logs that consistently cover the search for sources as well as the creation of documents. This will allow for in-depth analyses of how text is composed if a writer is at liberty to reuse texts from a third party—a setting which has not been studied so far. In addition, the corpus pro- vides an original resource for the evalua- tion of text reuse and plagiarism detectors, where currently only less realistic resources are employed. ",,,,ACL
120,2013,SPred: Large-scale Harvesting of Semantic Predicates,"Tiziano Flati, Roberto Navigli","We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing colloca- tions to form lexical predicates (e.g., break ? ) and learn the semantic classes that best fit the ? argument. To do this, we extract all the occurrences in Wikipedia which match the predicate and abstract its argu- ments to general semantic classes (e.g., break B ODY P ART , break A GREEMENT , etc.). Our experiments show that we are able to create a large collection of seman- tic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach. ",,,,ACL
121,2013,Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain,"Jackie Chi Kit Cheung, Gerald Penn","In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Cur- rent systems use centrality, along with re- dundancy avoidance and some sentence compression, to produce mostly extrac- tive summaries. In this paper, we investi- gate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the se- mantic level of caseframes. We show that model summaries (1) are more abstrac- tive and make use of more sentence aggre- gation, (2) do not contain as many topi- cal caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improve- ments are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed. ",,,,ACL
122,2013,HEADY: News headline abstraction through event pattern clustering,"Enrique Alfonseca, Daniele Pighin, Guillermo Garrido","This paper presents H EADY : a novel, ab- stractive approach for headline generation from news collections. From a web-scale corpus of English news, we mine syntac- tic patterns that a Noisy-OR model gener- alizes into event descriptions. At inference time, we query the model with the patterns observed in an unseen news collection, identify the event that better captures the gist of the collection and retrieve the most appropriate pattern to generate a head- line. H EADY improves over a state-of-the- art open-domain title abstraction method, bridging half of the gap that separates it from extractive methods using human- generated titles in manual evaluations, and performs comparably to human-generated headlines as evaluated with ROUGE. ",,,,ACL
123,2013,Conditional Random Fields for Responsive Surface Realisation using Global Features,"Nina Dethlefs, Helen Hastie, Heriberto Cuayáhuitl, Oliver Lemon","Surface realisers in spoken dialogue sys- tems need to be more responsive than con- ventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisa- tion. It also allows generation from partial and modified inputs and is therefore ap- plicable to incremental surface realisation. Results from a human rating study confirm that users are sensitive to this extended no- tion of context and assign ratings that are significantly higher (up to 14% ) than those for taking only local context into account. ",,,,ACL
124,2013,Two-Neighbor Orientation Model with Cross-Boundary Global Contexts,"Hendra Setiawan, Bowen Zhou, Bing Xiang, Libin Shen","Long distance reordering remains one of the greatest challenges in statistical ma- chine translation research as the key con- textual information may well be beyond the confine of translation units. In this paper, we propose Two-Neighbor Orien- tation (TNO) model that jointly models the orientation decisions between anchors and two neighboring multi-unit chunks which may cross phrase or rule bound- aries. We explicitly model the longest span of such chunks, referred to as Max- imal Orientation Span, to serve as a global parameter that constrains under- lying local decisions. We integrate our proposed model into a state-of-the-art string-to-dependency translation system and demonstrate the efficacy of our pro- posal in a large-scale Chinese-to-English translation task. On NIST MT08 set, our most advanced model brings around +2.0 BLEU and -1.0 TER improvement. ",,,,ACL
125,2013,Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation,"Karthik Visweswariah, Mitesh M. Khapra, Ananthakrishnan Ramanathan","Preordering of a source language sentence to match target word order has proved to be useful for improving machine transla- tion systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation perfor- mance. In this paper, we focus on further improving the performance of the reorder- ing model (and thereby machine transla- tion) by using a larger corpus of sentence aligned data for which manual word align- ments are not available but automatic ma- chine generated alignments are available. The main challenge we tackle is to gen- erate quality data for training the reorder- ing model in spite of the machine align- ments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorder- ings produced given noisy alignments and also improves word alignments using in- formation from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline su- pervised aligner. The data generated al- lows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English eval- uation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline. ",,,,ACL
126,2013,Vector Space Model for Adaptation in Statistical Machine Translation,"Boxing Chen, Roland Kuhn, George Foster","This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a di- mensionality equal to the number of train- ing subcorpora; each entry in the vector re- flects the contribution of a particular sub- corpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the train- ing data, we create a vector with features defined in the same way, and calculate its similarity score with the vector represent- ing the dev set. Thus, we obtain a de- coding feature whose value represents the phrase pair’s closeness to the dev. This is a simple, computationally cheap form of instance weighting for phrase pairs. Ex- periments on large scale NIST evaluation data show improvements over strong base- lines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and signifi- cant improvements in most circumstances over baselines with linear mixture model adaptation. An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre. ",,,,ACL
127,2013,From Natural Language Specifications to Program Input Parsers,"Tao Lei, Fan Long, Regina Barzilay, Martin Rinard","We present a method for automatically generating input parsers from English specifications of input file formats. We use a Bayesian generative model to cap- ture relevant natural language phenomena and translate the English specification into a specification tree, which is then trans- lated into a C++ input parser. We model the problem as a joint dependency pars- ing and semantic role labeling task. Our method is based on two sources of infor- mation: (1) the correlation between the text and the specification tree and (2) noisy supervision as determined by the success of the generated C++ parser in reading in- put examples. Our results show that our approach achieves 80.0% F-Score accu- racy compared to an F-Score of 66.7% produced by a state-of-the-art semantic parser on a dataset of input format speci- fications from the ACM International Col- legiate Programming Contest (which were written in English for humans with no in- tention of providing support for automated processing). 1 ",,,,ACL
128,2013,Entity Linking for Tweets,"Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou","We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. Particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method. ",,,,ACL
129,2013,Identification of Speakers in Novels,"Hua He, Denilson Barbosa, Grzegorz Kondrak","Speaker identification is the task of at- tributing utterances to characters in a lit- erary narrative. It is challenging to auto- mate because the speakers of the majority of utterances are not explicitly identified in novels. In this paper, we present a super- vised machine learning approach for the task that incorporates several novel fea- tures. The experimental results show that our method is more accurate and general than previous approaches to the problem. ",,,,ACL
130,2013,Language Acquisition and Probabilistic Models: keeping it simple,"Aline Villavicencio, Marco Idiart, Robert Berwick, Igor Malioutov","Hierarchical Bayesian Models (HBMs) have been used with some success to capture empirically observed pat- terns of under- and overgeneralization in child language acquisition. How- ever, as is well known, HBMs are “ideal” learning systems, assuming ac- cess to unlimited computational re- sources that may not be available to child language learners. Conse- quently, it remains crucial to carefully assess the use of HBMs along with al- ternative, possibly simpler, candidate models. This paper presents such an evaluation for a language acquisi- tion domain where explicit HBMs have been proposed: the acquisition of En- glish dative constructions. In particu- lar, we present a detailed, empirically- grounded model-selection compari- son of HBMs vs. a simpler alternative based on clustering along with max- imum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance with- out incurring on the high computa- tional costs associated with HBMs. ",,,,ACL
131,2013,A Two Level Model for Context Sensitive Inference Rules,"Oren Melamud, Jonathan Berant, Ido Dagan, Jacob Goldberger","Automatic acquisition of inference rules for predicates has been commonly ad- dressed by computing distributional simi- larity between vectors of argument words, operating at the word space level. A re- cent line of work, which addresses context sensitivity of rules, represented contexts in a latent topic space and computed similar- ity over topic vectors. We propose a novel two-level model, which computes simi- larities between word-level vectors that are biased by topic-level context repre- sentations. Evaluations on a naturally- distributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set. ",,,,ACL
132,2013,"Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity","Mohammad Taher Pilehvar, David Jurgens, Roberto Navigli","Semantic similarity is an essential com- ponent of many Natural Language Pro- cessing applications. However, prior meth- ods for computing semantic similarity of- ten operate at different levels, e.g., sin- gle words or entire documents, which re- quires adapting the method for each data type. We present a unified approach to se- mantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-of- the-art performance on three tasks: seman- tic textual similarity, word similarity, and word sense coarsening. ",,,,ACL
133,2013,Linking and Extending an Open Multilingual Wordnet,"Francis Bond, Ryan Foster","We create an open multilingual wordnet with large wordnets for over 26 languages and smaller ones for 57 languages. It is made by combining wordnets with open li- cences, data from Wiktionary and the Uni- code Common Locale Data Repository. Overall there are over 2 million senses for over 100 thousand concepts, linking over 1.4 million words in hundreds of lan- guages. ",,,,ACL
134,2013,FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection,"Silvana Hartmann, Iryna Gurevych","We present a new bilingual FrameNet lex- icon for English and German. It is cre- ated through a simple, but powerful ap- proach to construct a FrameNet in any language using Wiktionary as an inter- lingual representation. Our approach is based on a sense alignment of FrameNet and Wiktionary, and subsequent transla- tion disambiguation into the target lan- guage. We perform a detailed evaluation of the created resource and a discussion of Wiktionary as an interlingual connection for the cross-language transfer of lexical- semantic resources. The created resource is publicly available at http://www. ukp.tu-darmstadt.de/fnwkde/ . ",,,,ACL
135,2013,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,"Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn","Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting par- allel text, but crawling the entire web is impossible for all but the largest compa- nies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon’s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the result- ing data boosts translation performance across the board for five different language pairs in the news domain, and on open do- main test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource. 1 ",,,,ACL
136,2013,A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization,"Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Florian","We consider the problem of using sentence compression techniques to facilitate query- focused multi-document summarization. We present a sentence-compression-based frame- work for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search de- coder is proposed to efficiently find highly probable compressions. Under this frame- work, we show how to integrate various in- dicative metrics such as linguistic motivation and query relevance into the compression pro- cess by deriving a novel formulation of a com- pression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ",,,,ACL
137,2013,Domain-Independent Abstract Generation for Focused Meeting Summarization,"Lu Wang, Claire Cardie","We address the challenge of generating natu- ral language abstractive summaries for spoken meetings in a domain-independent fashion. We apply Multiple-Sequence Alignment to in- duce abstract generation templates that can be used for different domains. An Overgenerate- and-Rank strategy is utilized to produce and rank candidate abstracts. Experiments us- ing in-domain and out-of-domain training on disparate corpora show that our system uni- formly outperforms state-of-the-art supervised extract-based approaches. In addition, human judges rate our system summaries significantly higher than compared systems in fluency and overall quality. ",,,,ACL
138,2013,A Statistical NLG Framework for Aggregated Planning and Realization,"Ravi Kondadadi, Blake Howald, Frank Schilder","We present a hybrid natural language gen- eration (NLG) system that consolidates macro and micro planning and surface re- alization tasks into one statistical learn- ing process. Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target do- main. First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis. Each sentence is then organized into semanti- cally similar groups (representing a do- main specific concept) by k-means cluster- ing. After this semi-automatic processing (human review of cluster assignments), a number of corpus–level statistics are com- piled and used as features by a ranking SVM to develop model weights from a training corpus. At generation time, a set of input data, the collection of semanti- cally organized templates, and the model weights are used to select optimal tem- plates. Our system is evaluated with au- tomatic, non–expert crowdsourced and ex- pert evaluation metrics. We also introduce a novel automatic metric – syntactic vari- ability – that represents linguistic variation as a measure of unique template sequences across a collection of automatically gener- ated documents. The metrics for generated weather and biography texts fall within ac- ceptable ranges. In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based ar- chitectures and readily adapts to different domains with reduced development time. ?*Ravi Kondadadi is now affiliated with Nuance Commu- nications, Inc. ",,,,ACL
139,2013,Models of Translation Competitions,"Mark Hopkins, Jonathan May","What do we want to learn from a trans- lation competition and how do we learn it with confidence? We argue that a dis- proportionate focus on ranking competi- tion participants has led to lots of differ- ent rankings, but little insight about which rankings we should trust. In response, we provide the first framework that allows an empirical comparison of different analy- ses of competition results. We then use this framework to compare several analyt- ical models on data from the Workshop on Machine Translation (WMT). ",,,,ACL
140,2013,Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation,"Jiajun Zhang, Chengqing Zong","Currently, almost all of the statistical ma- chine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a lan- guage pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for in- ducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effec- tive method to induce a phrase-based model from the monolingual corpora given an au- tomatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. ",,,,ACL
141,2013,SenseSpotting: Never let your parallel data tie you to an old domain,"Marine Carpuat, Hal Daumé III, Katharine Henry, Ann Irvine","Words often gain new senses in new do- mains. Being able to automatically iden- tify, from a corpus of monolingual text, which word tokens are being used in a pre- viously unseen sense has applications to machine translation and other tasks sensi- tive to lexical semantics. We define a task, S ENSE S POTTING , in which we build sys- tems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a gold- standard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for ma- chine translation. Our system is able to achieve F-measures of as much as 80% , when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains. ",,,,ACL
142,2013,BRAINSUP: Brainstorming Support for Creative Sentence Generation,"Gözde Özbal, Daniele Pighin, Carlo Strapparava","We present B RAIN S UP , an extensible framework for the generation of creative sentences in which users are able to force several words to appear in the sen- tences and to control the generation pro- cess across several semantic dimensions, namely emotions, colors, domain related- ness and phonetic properties. We evalu- ate its performance on a creative sentence generation task, showing its capability of generating well-formed, catchy and effec- tive sentences that have all the good qual- ities of slogans produced by human copy- writers. ",,,,ACL
143,2013,Grammatical Error Correction Using Integer Linear Programming,"Yuanbin Wu, Hwee Tou Ng","We propose a joint inference algorithm for grammatical error correction. Different from most previous work where different error types are corrected independently, our proposed inference process considers all possible errors in a unied framework. We use integer linear programming (ILP) to model the inference process, which can easily incorporate both the power of exist- ing error classiers and prior knowledge on grammatical error correction. Exper- imental results on the Helping Our Own shared task show that our method is com- petitive with state-of-the-art systems. ",,,,ACL
144,2013,Text-Driven Toponym Resolution using Indirect Supervision,"Michael Speriosu, Jason Baldridge","Toponym resolvers identify the specific lo- cations referred to by ambiguous place- names in text. Most resolvers are based on heuristics using spatial relationships be- tween multiple toponyms in a document, or metadata such as population. This pa- per shows that text-driven disambiguation for toponyms is far more effective. We ex- ploit document-level geotags to indirectly generate training instances for text classi- fiers for toponym resolution, and show that textual cues can be straightforwardly in- tegrated with other commonly used ones. Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles. ",,,,ACL
145,2013,Argument Inference from Relevant Event Mentions in Chinese Argument Extraction,"Peifeng Li, Qiaoming Zhu, Guodong Zhou","As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those inter- sentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. ",,,,ACL
146,2013,Fine-grained Semantic Typing of Emerging Entities,"Ndapandula Nakashole, Tomasz Tylenda, Gerhard Weikum","Methods for information extraction (IE) and knowledge base (KB) construction have been intensively studied. However, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging. In this paper, we present a method for discovering and se- mantically typing newly emerging out-of- KB entities, thus improving the freshness and recall of ontology-based IE and im- proving the precision and semantic rigor of open IE. Our method is based on a prob- abilistic model that feeds weights into in- teger linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints. Our experimental evaluation, based on crowd- sourced user studies, show our method performing significantly better than prior work. ",,,,ACL
147,2013,Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction,"Barbara Plank, Alessandro Moschitti","Relation Extraction (RE) is the task of extracting semantic relationships between entities in text. Recent studies on rela- tion extraction are mostly supervised. The clear drawback of supervised methods is the need of training data: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. This is the problem of domain adapta- tion. In this paper, we propose to combine (i) term generalization approaches such as word clustering and latent semantic anal- ysis (LSA) and (ii) structured kernels to improve the adaptability of relation ex- tractors to new text genres/domains. The empirical evaluation on ACE 2005 do- mains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation. ",,,,ACL
148,2013,A joint model of word segmentation and phonological variation for English word-final /t/-deletion,"Benjamin Börschinger, Mark Johnson, Katherine Demuth","Word-final /t/-deletion refers to a common phenomenon in spoken English where words such as / wEst / “west” are pro- nounced as [ wEs ] “wes” in certain con- texts. Phonological variation like this is common in naturally occurring speech. Current computational models of unsu- pervised word segmentation usually as- sume idealized input that is devoid of these kinds of variation. We extend a non-parametric model of word segmenta- tion by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-defined joint model as a first step towards han- dling variation and segmentation in a sin- gle model. We analyse how our model handles /t/-deletion on a large corpus of transcribed speech, and show that the joint model can perform word segmentation and recover underlying /t/s. We find that Bi- gram dependencies are important for per- forming well on real data and for learning appropriate deletion probabilities for dif- ferent contexts. 1 ",,,,ACL
149,2013,Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics,"Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli, Marco Baroni","Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional meth- ods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations con- structed in this way beat a strong baseline and can be of higher quality than represen- tations directly constructed from corpus data. Our results constitute a novel evalua- tion of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics. ",,,,ACL
150,2013,Unsupervised Consonant-Vowel Prediction over Hundreds of Languages,"Young-Bum Kim, Benjamin Snyder","In this paper, we present a solution to one aspect of the decipherment task: the pre- diction of consonants and vowels for an unknown language and alphabet. Adopt- ing a classical Bayesian perspective, we performs posterior inference over hun- dreds of languages, leveraging knowledge of known languages and alphabets to un- cover general linguistic patterns of typo- logically coherent language clusters. We achieve average accuracy in the unsuper- vised consonant/vowel prediction task of 99% across 503 languages. We further show that our methodology can be used to predict more fine-grained phonetic dis- tinctions. On a three-way classification task between vowels, nasals, and non- nasal consonants, our model yields unsu- pervised accuracy of 89% across the same set of languages. ",,,,ACL
151,2013,Improving Text Simplification Language Modeling Using Unsimplified Text Data,David Kauchak,"In this paper we examine language mod- eling for text simplification. Unlike some text-to-text translation tasks, text simplifi- cation is a monolingual translation task al- lowing for text in both the input and out- put domain to be used for training the lan- guage model. We explore the relation- ship between normal English and simpli- fied English and compare language mod- els trained on varying amounts of text from each. We evaluate the models intrin- sically with perplexity and extrinsically on the lexical simplification task from Se- mEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc anal- ysis shows that the additional unsimplified data provides better coverage for unseen and rare n -grams. ",,,,ACL
152,2013,Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures,"Sina Zarrieß, Jonas Kuhn","We suggest a generation task that inte- grates discourse-level referring expression generation and sentence-level surface re- alization. We present a data set of Ger- man articles annotated with deep syntax and referents, including some types of im- plicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results sug- gest that a revision-based pipeline, with in- termediate linearization, significantly out- performs standard pipelines or a parallel architecture. ",,,,ACL
153,2013,Named Entity Recognition using Cross-lingual Resources: Arabic as an Example,Kareem Darwish,"Some languages lack large knowledge bases and good discriminative features for Name Entity Recognition (NER) that can general- ize to previously unseen named entities. One such language is Arabic, which: a) lacks a capitalization feature; and b) has relatively small knowledge bases, such as Wikipedia. In this work we address both problems by in- corporating cross-lingual features and knowl- edge bases from English using cross-lingual links. We show that such features have a dramatic positive effect on recall. We show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in F- measure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and mi- croblogs test sets respectively. ",,,,ACL
154,2013,Beam Search for Solving Substitution Ciphers,"Malte Nuhn, Julian Schamper, Hermann Ney","In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a concep- tually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ci- phers and is able to use high order n -gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3 -gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6 -gram language model. We also apply our approach to the famous Zodiac- 408 cipher and obtain slightly bet- ter (and near to optimal) results than pre- viously published. Unlike the previous state-of-the-art approach that uses addi- tional word lists to evaluate possible deci- pherments, our approach only uses a letter- based 6 -gram language model. Further- more we use our algorithm to solve large vocabulary substitution ciphers and im- prove the best published decipherment er- ror rate based on the Gigaword corpus of 7.8% to 6.0% error rate. ",,,,ACL
155,2013,Social Text Normalization using Contextual Graph Random Walks,"Hany Hassan, Arul Menezes","We introduce a social media text normal- ization system that can be deployed as a preprocessing step for Machine Transla- tion and various NLP applications to han- dle social media text. The proposed sys- tem is based on unsupervised learning of the normalization equivalences from unla- beled text. The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. We show that the proposed approach has a very high precision of (92.43) and a rea- sonable recall of (56.4). When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP appli- cation to handle social media text. ",,,,ACL
156,2013,Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation,"ThuyLinh Nguyen, Stephan Vogel","Hiero translation models have two lim- itations compared to phrase-based mod- els: 1) Limited hypothesis space; 2) No lexicalized reordering model. We pro- pose an extension of Hiero called Phrasal- Hiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypoth- esis space as the original Hiero but in- corporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero transla- tion derivation, find its corresponding dis- continuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for Arabic- English, Chinese-English and German- English translation. ",,,,ACL
157,2013,Machine Translation Detection from Monolingual Web-Text,"Yuki Arase, Ming Zhou","We propose a method for automatically detecting low-quality Web-text translated by statistical machine translation (SMT) systems. We focus on the phrase salad phenomenon that is observed in existing SMT results and propose a set of computa- tionally inexpensive features to effectively detect such machine-translated sentences from a large-scale Web-mined text. Un- like previous approaches that require bilin- gual data, our method uses only monolin- gual text as input; therefore it is applicable for refining data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. ",,,,ACL
158,2013,Paraphrase-Driven Learning for Open Question Answering,"Anthony Fader, Luke Zettlemoyer, Oren Etzioni","We study question answering as a ma- chine learning problem, and induce a func- tion that maps open-domain questions to queries over a database of web extrac- tions. Given a large, community-authored, question-paraphrase corpus, we demon- strate that it is possible to learn a se- mantic lexicon and linear ranking func- tion without manually annotating ques- tions. Our approach automatically gener- alizes a seed lexicon and includes a scal- able, parallelized perceptron parameter es- timation scheme. Experiments show that our approach more than quadruples the re- call of the seed lexicon, with only an 8% loss in precision. ",,,,ACL
159,2013,Aid is Out There: Looking for Help from Tweets during a Large Scale Disaster,"István Varga, Motoki Sano, Kentaro Torisawa, Chikara Hashimoto","The 2011 Great East Japan Earthquake caused a wide range of problems, and as countermeasures, many aid activities were carried out. Many of these problems and aid activities were reported via Twitter. However, most problem reports and corre- sponding aid messages were not success- fully exchanged between victims and lo- cal governments or humanitarian organi- zations, overwhelmed by the vast amount of information. As a result, victims could not receive necessary aid and humanitar- ian organizations wasted resources on re- dundant efforts. In this paper, we propose a method for discovering matches between problem reports and aid messages. Our system contributes to problem-solving in a large scale disaster situation by facilitat- ing communication between victims and humanitarian organizations. ",,,,ACL
160,2013,"A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations","Angeliki Lazaridou, Ivan Titov, Caroline Sporleder","We propose a joint model for unsuper- vised induction of sentiment, aspect and discourse information and show that by in- corporating a notion of latent discourse re- lations in the model, we improve the pre- diction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of dis- course, as we induce types of discourse re- lations and associated discourse cues rel- evant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and as- pect shifters. The quantitative analysis that we conducted indicated that the integra- tion of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the in- duced representations encode a meaning- ful discourse structure. ",,,,ACL
161,2013,Joint Inference for Fine-grained Opinion Extraction,"Bishan Yang, Claire Cardie","This paper addresses the task of fine- grained opinion extraction – the identi- fication of opinion-related entities: the opinion expressions, the opinion hold- ers, and the targets of the opinions, and the relations between opinion expressions and their targets and holders. Most ex- isting approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner, where the inter- dependencies among different extraction stages are not captured. We propose a joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction, and seeks a glob- ally optimal solution. Experimental re- sults demonstrate that our joint inference approach significantly outperforms tradi- tional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction. ",,,,ACL
162,2013,Linguistic Models for Analyzing and Detecting Biased Language,"Marta Recasens, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky","Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiq- uitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits de- signed to remove bias from Wikipedia ar- ticles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and episte- mological bias, related to whether propo- sitions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective inten- sifiers. These insights help us develop fea- tures for a model to solve a new prediction task of practical importance: given a bi- ased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task. ",,,,ACL
163,2013,Evaluating a City Exploration Dialogue System with Integrated Question-Answering and Pedestrian Navigation,"Srinivasan Janarthanam, Oliver Lemon, Phil Bartie, Tiphaine Dalmas","We present a city navigation and tourist information mobile dialogue app with in- tegrated question-answering (QA) and ge- ographic information system (GIS) mod- ules that helps pedestrian users to nav- igate in and learn about urban environ- ments. In contrast to existing mobile apps which treat these problems independently, our Android app addresses the prob- lem of navigation and touristic question- answering in an integrated fashion using a shared dialogue context. We evaluated our system in comparison with Samsung S-Voice (which interfaces to Google nav- igation and Google search) with 17 users and found that users judged our system to be significantly more interesting to inter- act with and learn from. They also rated our system above Google search (with the Samsung S-Voice interface) for tourist in- formation tasks. ",,,,ACL
164,2013,Lightly Supervised Learning of Procedural Dialog Systems,"Svitlana Volkova, Pallavi Choudhury, Chris Quirk, Bill Dolan","Procedural dialog systems can help users achieve a wide range of goals. However, such systems are challenging to build, currently requiring manual engineering of substantial domain-specific task knowl- edge and dialog management strategies. In this paper, we demonstrate that it is pos- sible to learn procedural dialog systems given only light supervision, of the type that can be provided by non-experts. We consider domains where the required task knowledge exists in textual form (e.g., in- structional web pages) and where system builders have access to statements of user intent (e.g., search query logs or dialog interactions). To learn from such tex- tual resources, we describe a novel ap- proach that first automatically extracts task knowledge from instructions, then learns a dialog manager over this task knowledge to provide assistance. Evaluation in a Mi- crosoft Office domain shows that the indi- vidual components are highly accurate and can be integrated into a dialog system that provides effective help to users. ",,,,ACL
165,2013,Public Dialogue: Analysis of Tolerance in Online Discussions,"Arjun Mukherjee, Vivek Venkataraman, Bing Liu, Sharon Meraz","Social media platforms have enabled people to freely express their views and discuss issues of interest with others. While it is important to dis- cover the topics in discussions, it is equally use- ful to mine the nature of such discussions or de- bates and the behavior of the participants. There are many questions that can be asked. One key question is whether the participants give rea- soned arguments with justifiable claims via constructive debates or exhibit dogmatism and egotistic clashes of ideologies. The central idea of this question is tolerance, which is a key concept in the field of communications. In this work, we perform a computational study of tol- erance in the context of online discussions. We aim to identify tolerant vs. intolerant partici- pants and investigate how disagreement affects tolerance in discussions in a quantitative framework. To the best of our knowledge, this is the first such study. Our experiments using real-life discussions demonstrate the effective- ness of the proposed technique and also provide some key insights into the psycholinguistic phenomenon of tolerance in online discussions. ",,,,ACL
166,2013,Offspring from Reproduction Problems: What Replication Failure Teaches Us,"Antske Fokkens, Marieke van Erp, Marten Postma, Ted Pedersen","Repeating experiments is an important in- strument in the scientific toolbox to vali- date previous work and build upon exist- ing work. We present two concrete use cases involving key techniques in the NLP domain for which we show that reproduc- ing results is still difficult. We show that the deviation that can be found in repro- duction efforts leads to questions about how our results should be interpreted. Moreover, investigating these deviations provides new insights and a deeper under- standing of the examined techniques. We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers. Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field. ",,,,ACL
167,2013,Evaluating Text Segmentation using Boundary Edit Distance,Chris Fournier,"This work proposes a new segmentation evaluation metric, named boundary simi- larity (B), an inter-coder agreement coef- ficient adaptation, and a confusion-matrix for segmentation that are all based upon an adaptation of the boundary edit distance in Fournier and Inkpen (2012). Existing seg- mentation metrics such as P k , WindowD- iff, and Segmentation Similarity (S) are all able to award partial credit for near misses between boundaries, but are biased towards segmentations containing few or tightly clustered boundaries. Despite S’s improvements, its normalization also pro- duces cosmetically high values that over- estimate agreement & performance, lead- ing this work to propose a solution. ",,,,ACL
168,2013,Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation,"Rohan Ramanath, Monojit Choudhury, Kalika Bali, Rishiraj Saha Roy","Query segmentation, like text chunking, is the first step towards query understand- ing. In this study, we explore the effec- tiveness of crowdsourcing for this task. Through carefully designed control ex- periments and Inter Annotator Agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal (often only two) parts. Sim- ilarly, in the case of hierarchical or nested segmentation, turkers have a strong prefer- ence towards balanced binary trees. ",,,,ACL
169,2013,Deceptive Answer Prediction with User Preference Graph,"Fangtao Li, Yang Gao, Shuchang Zhou, Xiance Si","In Community question answering (QA) sites, malicious users may provide decep- tive answers to promote their products or services. It is important to identify and fil- ter out these deceptive answers. In this paper, we first solve this problem with the traditional supervised learning meth- ods. Two kinds of features, including tex- tual and contextual features, are investi- gated for this task. We further propose to exploit the user relationships to identify the deceptive answers, based on the hy- pothesis that similar users will have simi- lar behaviors to post deceptive or authentic answers. To measure the user similarity, we propose a new user preference graph based on the answer preference expressed by users, such as “helpful” voting and “best answer” selection. The user prefer- ence graph is incorporated into traditional supervised learning framework with the graph regularization technique. The ex- periment results demonstrate that the user preference graph can indeed help improve the performance of deceptive answer pre- diction. ",,,,ACL
170,2013,Why-Question Answering using Intra- and Inter-Sentential Causal Relations,"Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Motoki Sano","In this paper, we explore the utility of intra- and inter-sentential causal relations between terms or clauses as evidence for answering why-questions. To the best of our knowledge, this is the first work that uses both intra- and inter-sentential causal relations for why-QA. We also propose a method for assessing the appropriate- ness of causal relations as answers to a given question using the semantic orienta- tion of excitation proposed by Hashimoto et al. (2012). By applying these ideas to Japanese why-QA, we improved preci- sion by 4.4% against all the questions in our test set over the current state-of-the- art system for Japanese why-QA. In addi- tion, unlike the state-of-the-art system, our system could achieve very high precision (83.2%) for 25% of all the questions in the test set by restricting its output to the con- fident answers only. ",,,,ACL
171,2013,Question Answering Using Enhanced Lexical Semantic Models,"Wen-tau Yih, Ming-Wei Chang, Christopher Meek, Andrzej Pastusiak","In this paper, we study the answer sentence selection problem for ques- tion answering. Unlike previous work, which primarily leverages syntactic analy- sis through dependency tree matching, we focus on improving the performance us- ing models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, re- gardless of the choice of learning algo- rithms. When evaluated on a bench- mark dataset, the MAP and MRR scores are increased by 8 to 10 points, com- pared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin. ",,,,ACL
172,2013,Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews,"Kang Liu, Liheng Xu, Jun Zhao","Mining opinion targets is a fundamen- tal and important task for opinion min- ing from online reviews. To this end, there are usually two kinds of methods: syntax based and alignment based meth- ods. Syntax based methods usually ex- ploited syntactic patterns to extract opin- ion targets, which were however prone to suffer from parsing errors when dealing with online informal texts. In contrast, alignment based methods used word align- ment model to fulfill this task, which could avoid parsing errors without using pars- ing. However, there is no research fo- cusing on which kind of method is more better when given a certain amount of re- views. To fill this gap, this paper empiri- cally studies how the performance of these two kinds of methods vary when chang- ing the size, domain and language of the corpus. We further combine syntactic pat- terns with alignment model by using a par- tially supervised framework and investi- gate whether this combination is useful or not. In our experiments, we verify that our combination is effective on the corpus with small and medium size. ",,,,ACL
173,2013,Mining Opinion Words and Opinion Targets in a Two-Stage Framework,"Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen","This paper proposes a novel two-stage method for mining opinion words and opinion targets. In the first stage, we propose a Sentiment Graph Walking algo- rithm, which naturally incorporates syn- tactic patterns in a Sentiment Graph to ex- tract opinion word/target candidates. Then random walking is employed to estimate confidence of candidates, which improves extraction accuracy by considering confi- dence of patterns. In the second stage, we adopt a self-learning strategy to refine the results from the first stage, especially for filtering out high-frequency noise terms and capturing the long-tail terms, which are not investigated by previous meth- ods. The experimental results on three real world datasets demonstrate the effective- ness of our approach compared with state- of-the-art unsupervised methods. ",,,,ACL
174,2013,Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning,"Song Feng, Jun Seok Kang, Polina Kuznetsova, Yejin Choi","Understanding the connotation of words plays an important role in interpreting sub- tle shades of sentiment beyond denotative or surface meaning of text, as seemingly objective statements often allude nuanced sentiment of the writer, and even purpose- fully conjure emotion from the readers’ minds. The focus of this paper is draw- ing nuanced, connotative sentiments from even those words that are objective on the surface, such as “intelligence”, “human”, and “cheesecake”. We propose induction algorithms encoding a diverse set of lin- guistic insights (semantic prosody, distri- butional similarity, semantic parallelism of coordination) and prior knowledge drawn from lexical resources, resulting in the first broad-coverage connotation lexicon. ",,,,ACL
1,2014,Learning Ensembles of Structured Prediction Rules,"Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri","We present a series of algorithms with the- oretical guarantees for learning accurate ensembles of several structured prediction rules for which no prior knowledge is as- sumed. This includes a number of ran- domized and deterministic algorithms de- vised by converting on-line learning al- gorithms to batch ones, and a boosting- style algorithm applicable in the context of structured prediction with a large number of labels. We also report the results of ex- tensive experiments with these algorithms. ",,,,ACL
2,2014,Representation Learning for Text-level Discourse Parsing,"Yangfeng Ji, Jacob Eisenstein","Text-level discourse parsing is notoriously difficult, as distinctions between discourse relations require subtle semantic judg- ments that are not easily captured using standard features. In this paper, we present a representation learning approach, in which we transform surface features into a latent space that facilitates RST dis- course parsing. By combining the machin- ery of large-margin transition-based struc- tured prediction with representation learn- ing, our method jointly learns to parse dis- course while at the same time learning a discourse-driven projection of surface fea- tures. The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art in pre- dicting relations and nuclearity on the RST Treebank. ",,,,ACL
3,2014,Text-level Discourse Dependency Parsing,"Sujian Li, Liang Wang, Ziqiang Cao, Wenjie Li","Previous researches on Text-level discourse parsing mainly made use of constituency structure to parse the whole document into one discourse tree. In this paper, we present the limitations of constituency based dis- course parsing and first propose to use de- pendency structure to directly represent the relations between elementary discourse units (EDUs). The state-of-the-art depend- ency parsing techniques, the Eisner algo- rithm and maximum spanning tree (MST) algorithm, are adopted to parse an optimal discourse dependency tree based on the arc- factored model and the large-margin learn- ing techniques. Experiments show that our discourse dependency parsers achieve a competitive performance on text-level dis- course parsing. ",,,,ACL
4,2014,Discovering Latent Structure in Task-Oriented Dialogues,"Ke Zhai, Jason D. Williams","A key challenge for computational conver- sation models is to discover latent struc- ture in task-oriented dialogue, since it pro- vides a basis for analysing, evaluating, and building conversational systems. We pro- pose three new unsupervised models to discover latent structures in task-oriented dialogues. Our methods synthesize hidden Markov models (for underlying state) and topic models (to connect words to states). We apply them to two real, non-trivial datasets: human-computer spoken dia- logues in bus query service, and human- human text-based chats from a live tech- nical support service. We show that our models extract meaningful state represen- tations and dialogue structures consistent with human annotations. Quantitatively, we show our models achieve superior per- formance on held-out log likelihood eval- uation and an ordering task. ",,,,ACL
5,2014,Learning Structured Perceptrons for Coreference Resolution with Latent Antecedents and Non-local Features,"Anders Björkelund, Jonas Kuhn","We investigate different ways of learning structured perceptron models for coref- erence resolution when using non-local features and beam search. Our experi- mental results indicate that standard tech- niques such as early updates or Learning as Search Optimization (LaSO) perform worse than a greedy baseline that only uses local features. By modifying LaSO to de- lay updates until the end of each instance we obtain significant improvements over the baseline. Our model obtains the best results to date on recent shared task data for Arabic, Chinese, and English. ",,,,ACL
6,2014,Multilingual Models for Compositional Distributed Semantics,"Karl Moritz Hermann, Phil Blunsom","We present a novel technique for learn- ing semantic representations, which ex- tends the distributional hypothesis to mul- tilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The mod- els do not rely on word alignments or any syntactic information and are success- fully applied to a number of diverse lan- guages. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic rela- tionships across languages without paral- lel data. ",,,,ACL
7,2014,Simple Negation Scope Resolution through Deep Parsing: A Semantic Solution to a Semantic Problem,"Woodley Packard, Emily M. Bender, Jonathon Read, Stephan Oepen","In this work, we revisit Shared Task 1 from the 2012 *SEM Conference: the au- tomated analysis of negation. Unlike the vast majority of participating systems in 2012, our approach works over explicit and formal representations of proposi- tional semantics, i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning rep- resentations. We relate the task-specific interpretation of (negation) scope to the concept of (quantifier and operator) scope in mainstream underspecified semantics. With reference to an explicit encoding of semantic predicate-argument structure, we can operationalize the annotation deci- sions made for the 2012 *SEM task, and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system. In a system combination setting, our approach improves over the best pub- lished results on this task to date. ",,,,ACL
8,2014,Logical Inference on Dependency-based Compositional Semantics,"Ran Tian, Yusuke Miyao, Takuya Matsuzaki","Dependency-based Compositional Se- mantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics. In this paper, we equip the DCS framework with logical inference, by defining ab- stract denotations as an abstraction of the computing process of denotations in original DCS. An inference engine is built to achieve inference on abstract denota- tions. Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation. Experiments on FraCaS and PASCAL RTE datasets show promising results. ",,,,ACL
9,2014,A practical and linguistically-motivated approach to compositional distributional semantics,"Denis Paperno, Nghia The Pham, Marco Baroni","Distributional semantic methods to ap- proximate word meaning with context vectors have been very successful empir- ically, and the last years have seen a surge of interest in their compositional exten- sion to phrases and sentences. We present here a new model that, like those of Co- ecke et al. (2010) and Baroni and Zam- parelli (2010), closely mimics the standard Montagovian semantic treatment of com- position in distributional terms. However, our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged, real-life sentences. We test the model on a variety of empirical tasks, showing that it consistently outperforms a set of competitive rivals. ",,,,ACL
10,2014,Lattice Desegmentation for Statistical Machine Translation,"Mohammad Salameh, Colin Cherry, Grzegorz Kondrak","Morphological segmentation is an effec- tive sparsity reduction strategy for statis- tical machine translation (SMT) involv- ing morphologically complex languages. When translating into a segmented lan- guage, an extra step is required to deseg- ment the output; previous studies have de- segmented the 1-best output from the de- coder. In this paper, we expand our trans- lation options by desegmenting n-best lists or lattices. Our novel lattice desegmenta- tion algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs, which allows for inclusion of features related to the de- segmentation process, as well as an un- segmented language model (LM). We in- vestigate this technique in the context of English-to-Arabic and English-to-Finnish translation, showing significant improve- ments in translation quality over deseg- mentation of 1-best decoder outputs. ",,,,ACL
11,2014,Bilingually-constrained Phrase Embeddings for Machine Translation,"Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou","We propose Bilingually-constrained Re- cursive Auto-encoders (BRAE) to learn semantic phrase embeddings (compact vector representations for phrases), which can distinguish the phrases with differ- ent semantic meanings. The BRAE is trained in a way that minimizes the seman- tic distance of translation equivalents and maximizes the semantic distance of non- translation pairs simultaneously. After training, the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other. We evaluate our proposed method on two end-to-end SMT tasks (phrase ta- ble pruning and decoding with phrasal se- mantic similarities) which need to mea- sure semantic similarity between a source phrase and its translation candidates. Ex- tensive experiments show that the BRAE is remarkably effective in these two tasks. ",,,,ACL
12,2014,Learning New Semi-Supervised Deep Auto-encoder Features for Statistical Machine Translation,"Shixiang Lu, Zhenbiao Chen, Bo Xu","In this paper, instead of designing new fea- tures based on intuition, linguistic knowl- edge and domain, we learn some new and effective features using the deep auto- encoder (DAE) paradigm for phrase-based translation model. Using the unsupervised pre-trained deep belief net (DBN) to ini- tialize DAE’s parameters and using the in- put original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsuper- vised DBN features. Moreover, to learn high dimensional feature representation, we introduce a natural horizontal compo- sition of more DAEs for large hidden lay- ers feature learning. On two Chinese- English tasks, our semi-supervised DAE features obtain statistically significant im- provements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the unsupervised DBN features and the base- line features, respectively. ",,,,ACL
13,2014,Learning Topic Representation for SMT with Neural Networks,"Lei Cui, Dongdong Zhang, Shujie Liu, Qiming Chen","Statistical Machine Translation (SMT) usually utilizes contextual information to disambiguate translation candidates. However, it is often limited to contexts within sentence boundaries, hence broader topical information cannot be leveraged. In this paper, we propose a novel approach to learning topic representation for paral- lel data using a neural network architec- ture, where abundant topical contexts are embedded via topic relevant monolingual data. By associating each translation rule with the topic representation, topic rele- vant rules are selected according to the dis- tributional similarity with the source text during SMT decoding. Experimental re- sults show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task com- pared to a state-of-the-art baseline. ",,,,ACL
14,2014,Tagging The Web: Building A Robust Web Tagger with Neural Network,"Ji Ma, Yue Zhang, Jingbo Zhu","In this paper, we address the problem of web-domain POS tagging using a two- phase approach. The first phase learns rep- resentations that capture regularities un- derlying web text. The representation is integrated as features into a neural network that serves as a scorer for an easy-first POS tagger. Parameters of the neural network are trained using guided learning in the second phase. Experiment on the SANCL 2012 shared task show that our approach achieves 93.15% average tagging accu- racy, which is the best accuracy reported so far on this data set, higher than those given by ensembled syntactic parsers. ",,,,ACL
15,2014,Unsupervised Solution Post Identification from Discussion Forums,"Deepak P, Karthik Visweswariah","Discussion forums have evolved into a de- pendable source of knowledge to solve common problems. However, only a mi- nority of the posts in discussion forums are solution posts. Identifying solution posts from discussion forums, hence, is an important research problem. In this pa- per, we present a technique for unsuper- vised solution post identification leverag- ing a so far unexplored textual feature, that of lexical correlations between problems and solutions. We use translation mod- els and language models to exploit lex- ical correlations and solution post char- acter respectively. Our technique is de- signed to not rely much on structural fea- tures such as post metadata since such features are often not uniformly available across forums. Our clustering-based itera- tive solution identification approach based on the EM-formulation performs favor- ably in an empirical evaluation, beating the only unsupervised solution identifica- tion technique from literature by a very large margin. We also show that our unsu- pervised technique is competitive against methods that require supervision, outper- forming one such technique comfortably. ",,,,ACL
16,2014,Weakly Supervised User Profile Extraction from Twitter,"Jiwei Li, Alan Ritter, Eduard Hovy","While user attribute extraction on social media has received considerable attention, existing approaches, mostly supervised, encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates (e.g., gen- der). In this paper, we present a weakly- supervised approach to user profile extrac- tion from Twitter. Users’ profiles from so- cial media websites such as Facebook or Google Plus are used as a distant source of supervision for extraction of their at- tributes from user-generated text. In addi- tion to traditional linguistic features used in distant supervision for information ex- traction, our approach also takes into ac- count network information, a unique op- portunity offered by social media. We test our algorithm on three attribute domains: spouse, education and job; experimental results demonstrate our approach is able to make accurate predictions for users’ at- tributes based on their tweets. 1 ",,,,ACL
17,2014,The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter,"Chenhao Tan, Lillian Lee, Bo Pang","Consider a person trying to spread an important message on a social network. He/she can spend hours trying to craft the message. Does it actually matter? While there has been extensive prior work look- ing into predicting popularity of social- media content, the effect of wording per se has rarely been studied since it is of- ten confounded with the popularity of the author and the topic. To control for these confounding factors, we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording. Given such pairs, we ask: which version attracts more retweets? This turns out to be a more difficult task than predicting popular topics. Still, hu- mans can answer this question better than chance (but far from perfectly), and the computational methods we develop can do better than both an average human and a strong competing method trained on non- controlled data. ",,,,ACL
18,2014,Inferring User Political Preferences from Streaming Communications,"Svitlana Volkova, Glen Coppersmith, Benjamin Van Durme","Existing models for social media per- sonal analytics assume access to thou- sands of messages per user, even though most users author content only sporadi- cally over time. Given this sparsity, we: (i) leverage content from the local neigh- borhood of a user; (ii) evaluate batch mod- els as a function of size and the amount of messages in various types of neighbor- hoods; and (iii) estimate the amount of time and tweets required for a dynamic model to predict user preferences. We show that even when limited or no self- authored data is available, language from friend, retweet and user mention commu- nications provide sufficient evidence for prediction. When updating models over time based on Twitter, we find that polit- ical preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author’s tweeting frequency. ",,,,ACL
19,2014,Steps to Excellence: Simple Inference with Refined Scoring of Dependency Trees,"Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola","Much of the recent work on depen- dency parsing has been focused on solv- ing inherent combinatorial problems as- sociated with rich scoring functions. In contrast, we demonstrate that highly ex- pressive scoring functions can be used with substantially simpler inference pro- cedures. Specifically, we introduce a sampling-based parser that can easily han- dle arbitrary global features. Inspired by SampleRank, we learn to take guided stochastic steps towards a high scoring parse. We introduce two samplers for traversing the space of trees, Gibbs and Metropolis-Hastings with Random Walk. The model outperforms state-of-the-art re- sults when evaluated on 14 languages of non-projective CoNLL datasets. Our sampling-based approach naturally ex- tends to joint prediction scenarios, such as joint parsing and POS correction. The resulting method outperforms the best re- ported results on the CATiB dataset, ap- proaching performance of parsing with gold tags. 1 ",,,,ACL
20,2014,"Sparser, Better, Faster GPU Parsing","David Hall, Taylor Berg-Kirkpatrick, Dan Klein","Due to their origin in computer graph- ics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points. Natural lan- guage processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, Canny et al. (2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computa- tional power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per sec- ond on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning ap- proach to the constraints of a GPU. The resulting system is capable of computing over 404 Viterbi parses per second—more than a 2x speedup—on the same hard- ware. Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improv- ing throughput for this more accurate algo- rithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning—nearly a 6x speedup. ",,,,ACL
21,2014,Shift-Reduce CCG Parsing with a Dependency Model,"Wenduan Xu, Stephen Clark, Yue Zhang","This paper presents the first dependency model for a shift-reduce CCG parser. Mod- elling dependencies is desirable for a num- ber of reasons, including handling the “spurious” ambiguity of CCG ; fitting well with the theory of CCG ; and optimizing for structures which are evaluated at test time. We develop a novel training tech- nique using a dependency oracle, in which all derivations are hidden. A challenge arises from the fact that the oracle needs to keep track of exponentially many gold- standard derivations, which is solved by integrating a packed parse forest with the beam-search decoder. Standard CCGBank tests show the model achieves up to 1.05 labeled F-score improvements over three existing, competitive CCG parsing models. ",,,,ACL
22,2014,"Less Grammar, More Features","David Hall, Greg Durrett, Dan Klein","We present a parser that relies primar- ily on extracting information directly from surface spans rather than on propagat- ing information through enriched gram- mar structure. For example, instead of cre- ating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP. Moving context out of the grammar and onto surface fea- tures can greatly simplify the structural component of the parser: because so many deep syntactic cues have surface reflexes, our system can still parse accurately with context-free backbones as minimal as X- bar grammars. Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks. On the SPMRL 2013 multilingual con- stituency parsing shared task (Seddah et al., 2013), our system outperforms the top single parser system of Bj¨orkelund et al. (2013) on a range of languages. In addi- tion, despite being designed for syntactic analysis, our system also achieves state- of-the-art numbers on the structural senti- ment task of Socher et al. (2013). Finally, we show that, in both syntactic parsing and sentiment analysis, many broad linguistic trends can be captured via surface features. ",,,,ACL
23,2014,"Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors","Marco Baroni, Georgiana Dinu, Germán Kruszewski","Context-predicting models (more com- monly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the litera- ture is still lacking a systematic compari- son of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counter- parts. ",,,,ACL
24,2014,Metaphor Detection with Cross-Lingual Model Transfer,"Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric Nyberg","We show that it is possible to reliably dis- criminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by piv- oting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages. We pro- vide results on three new test sets in Span- ish, Farsi, and Russian. The results sup- port the hypothesis that metaphors are conceptual, rather than lexical, in nature. ",,,,ACL
25,2014,"Learning Word Sense Distributions, Detecting Unattested Senses and Identifying Novel Senses Using Topic Models","Jey Han Lau, Paul Cook, Diana McCarthy, Spandana Gella","Unsupervised word sense disambiguation ( WSD ) methods are an attractive approach to all-words WSD due to their non-reliance on expensive annotated data. Unsuper- vised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distri- butions. This paper presents a fully unsu- pervised topic modelling-based approach to sense frequency estimation, which is highly portable to different corpora and sense inventories, in being applicable to any part of speech, and not requiring a hi- erarchical sense inventory, parsing or par- allel text. We demonstrate the effective- ness of the method over the tasks of pre- dominant sense learning and sense distri- bution acquisition, and also the novel tasks of detecting senses which aren’t attested in the corpus, and identifying novel senses in the corpus which aren’t captured in the sense inventory. ",,,,ACL
26,2014,Learning to Automatically Solve Algebra Word Problems,"Nate Kushman, Yoav Artzi, Luke Zettlemoyer, Regina Barzilay","We present an approach for automatically learning to solve algebra word problems. Our algorithm reasons across sentence boundaries to construct and solve a sys- tem of linear equations, while simultane- ously recovering an alignment of the vari- ables and numbers in these equations to the problem text. The learning algorithm uses varied supervision, including either full equations or just the final answers. We evaluate performance on a newly gathered corpus of algebra word problems, demon- strating that the system can correctly an- swer almost 70% of the questions in the dataset. This is, to our knowledge, the first learning result for this task. ",,,,ACL
27,2014,Modelling function words improves unsupervised word segmentation,"Mark Johnson, Anne Christophe, Emmanuel Dupoux, Katherine Demuth","Inspired by experimental psychological findings suggesting that function words play a special role in word learning, we make a simple modification to an Adaptor Grammar based Bayesian word segmenta- tion model to allow it to learn sequences of monosyllabic “function words” at the beginnings and endings of collocations of (possibly multi-syllabic) words. This modification improves unsupervised word segmentation on the standard Bernstein- Ratner (1987) corpus of child-directed En- glish by more than 4% token f-score com- pared to a model identical except that it does not special-case “function words”, setting a new state-of-the-art of 92.4% to- ken f-score. Our function word model as- sumes that function words appear at the left periphery, and while this is true of languages such as English, it is not true universally. We show that a learner can use Bayesian model selection to determine the location of function words in their lan- guage, even though the input to the model only consists of unsegmented sequences of phones. Thus our computational models support the hypothesis that function words play a special role in word learning. ",,,,ACL
28,2014,Max-Margin Tensor Neural Network for Chinese Word Segmentation,"Wenzhe Pei, Tao Ge, Baobao Chang","Recently, neural network models for nat- ural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering. In this paper, we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network (MMTNN). By exploiting tag embeddings and tensor- based transformation, MMTNN has the ability to model complicated interactions between tags and context characters. Fur- thermore, a new tensor factorization ap- proach is proposed to speed up the model and avoid overfitting. Experiments on the benchmark dataset show that our model achieves better performances than previ- ous neural network models and that our model can achieve a competitive perfor- mance with minimal feature engineering. Despite Chinese word segmentation being a specific case, MMTNN can be easily generalized and applied to other sequence labeling tasks. ",,,,ACL
29,2014,An Empirical Study on the Effect of Negation Words on Sentiment,"Xiaodan Zhu, Hongyu Guo, Saif Mohammad, Svetlana Kiritchenko","Negation words, such as no and not, play a fundamental role in modifying sentiment of textual expressions. We will refer to a negation word as the negator and the text span within the scope of the negator as the argument. Commonly used heuristics to estimate the sentiment of negated expres- sions rely simply on the sentiment of ar- gument (and not on the negator or the ar- gument itself). We use a sentiment tree- bank to show that these existing heuristics are poor estimators of sentiment. We then modify these heuristics to be dependent on the negators and show that this improves prediction. Next, we evaluate a recently proposed composition model (Socher et al., 2013) that relies on both the negator and the argument. This model learns the syntax and semantics of the negator’s ar- gument with a recursive neural network. We show that this approach performs bet- ter than those mentioned above. In ad- dition, we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting er- rors. ",,,,ACL
30,2014,Extracting Opinion Targets and Opinion Words from Online Reviews with Graph Co-ranking,"Kang Liu, Liheng Xu, Jun Zhao","Extracting opinion targets and opinion words from online reviews are two fun- damental tasks in opinion mining. This paper proposes a novel approach to col- lectively extract them with graph co- ranking. First, compared to previous methods which solely employed opinion relations among words, our method con- structs a heterogeneous graph to model two types of relations, including seman- tic relations and opinion relations. Next, a co-ranking algorithm is proposed to es- timate the confidence of each candidate, and the candidates with higher confidence will be extracted as opinion targets/words. In this way, different relations make coop- erative effects on candidates’ confidence estimation. Moreover, word preference is captured and incorporated into our co- ranking algorithm. In this way, our co- ranking is personalized and each candi- date’s confidence is only determined by its preferred collocations. It helps to improve the extraction precision. The experimen- tal results on three data sets with differ- ent sizes and languages show that our ap- proach achieves better performance than state-of-the-art methods. ",,,,ACL
31,2014,Context-aware Learning for Sentence-level Sentiment Analysis with Posterior Regularization,"Bishan Yang, Claire Cardie","This paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences. Most ex- isting machine learning approaches suf- fer from limitations in the modeling of complex linguistic structures across sen- tences and often fail to capture non- local contextual cues that are important for sentiment interpretation. In contrast, our approach allows structured modeling of sentiment while taking into account both local and global contextual infor- mation. Specifically, we encode intu- itive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization. The context-aware constraints provide ad- ditional power to the CRF model and can guide semi-supervised learning when la- beled data is limited. Experiments on standard product review datasets show that our method outperforms the state-of-the- art methods in both the supervised and semi-supervised settings. ",,,,ACL
32,2014,Product Feature Mining: Semantic Clues versus Syntactic Constituents,"Liheng Xu, Kang Liu, Siwei Lai, Jun Zhao","Product feature mining is a key subtask in fine-grained opinion mining. Previ- ous works often use syntax constituents in this task. However, syntax-based methods can only use discrete contextual informa- tion, which may suffer from data sparsity. This paper proposes a novel product fea- ture mining method which leverages lexi- cal and contextual semantic clues. Lexical semantic clue verifies whether a candidate term is related to the target product, and contextual semantic clue serves as a soft pattern miner to find candidates, which ex- ploits semantics of each word in context so as to alleviate the data sparsity prob- lem. We build a semantic similarity graph to encode lexical semantic clue, and em- ploy a convolutional neural model to cap- ture contextual semantic clue. Then Label Propagation is applied to combine both se- mantic clues. Experimental results show that our semantics-based method signif- icantly outperforms conventional syntax- based approaches, which not only mines product features more accurately, but also extracts more infrequent product features. ",,,,ACL
33,2014,Aspect Extraction with Automated Prior Knowledge Learning,"Zhiyuan Chen, Arjun Mukherjee, Bing Liu","Aspect extraction is an important task in sentiment analysis. Topic modeling is a popular method for the task. However, unsupervised topic models often generate incoherent aspects. To address the is- sue, several knowledge-based models have been proposed to incorporate prior knowl- edge provided by the user to guide mod- eling. In this paper, we take a major step forward and show that in the big data era, without any user input, it is possi- ble to learn prior knowledge automatically from a large amount of review data avail- able on the Web. Such knowledge can then be used by a topic model to discover more coherent aspects. There are two key challenges: (1) learning quality knowl- edge from reviews of diverse domains, and (2) making the model fault-tolerant to handle possibly wrong knowledge. A novel approach is proposed to solve these problems. Experimental results using re- views from 36 domains show that the pro- posed approach achieves significant im- provements over state-of-the-art baselines. ",,,,ACL
34,2014,Anchors Regularized: Adding Robustness and Extensibility to Scalable Topic-Modeling Algorithms,"Thang Nguyen, Yuening Hu, Jordan Boyd-Graber","Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expec- tation maximization. However, these new methods lack the rich priors associated with probabilistic models. We examine Arora et al.’s anchor words algorithm for topic mod- eling and develop new, regularized algo- rithms that not only mathematically resem- ble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be com- bined with informed priors. ",,,,ACL
35,2014,A Bayesian Mixed Effects Model of Literary Character,"David Bamman, Ted Underwood, Noah A. Smith","We consider the problem of automatically inferring latent character types in a collec- tion of 15,099 English novels published between 1700 and 1899. Unlike prior work in which character types are assumed responsible for probabilistically generat- ing all text associated with a character, we introduce a model that employs mul- tiple effects to account for the influence of extra-linguistic information (such as au- thor). In an empirical evaluation, we find that this method leads to improved agree- ment with the preregistered judgments of a literary scholar, complementing the results of alternative models. ",,,,ACL
36,2014,Collective Tweet Wikification based on Semi-supervised Graph Regularization,"Hongzhao Huang, Yunbo Cao, Xiaojiang Huang, Heng Ji","Wikification for tweets aims to automat- ically identify each concept mention in a tweet and link it to a concept referent in a knowledge base (e.g., Wikipedia). Due to the shortness of a tweet, a collective inference model incorporating global ev- idence from multiple mentions and con- cepts is more appropriate than a non- collecitve approach which links each men- tion at a time. In addition, it is chal- lenging to generate sufficient high quality labeled data for supervised models with low cost. To tackle these challenges, we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multi- ple tweets through three fine-grained re- lations. In order to identify semantically- related mentions for collective inference, we detect meta path-based semantic rela- tions through social networks. Compared to the state-of-the-art supervised model trained from 100% labeled data, our pro- posed approach achieves comparable per- formance with 31% labeled data and ob- tains 5% absolute F1 gain with 50% la- beled data. ",,,,ACL
37,2014,Zero-shot Entity Extraction from Web Pages,"Panupong Pasupat, Percy Liang","In order to extract entities of a fine-grained category from semi-structured data in web pages, existing information extraction sys- tems rely on seed examples or redundancy across multiple web pages. In this paper, we consider a new zero-shot learning task of extracting entities specified by a natural language query (in place of seeds) given only a single web page. Our approach de- fines a log-linear model over latent extrac- tion predicates, which select lists of enti- ties from the web page. The main chal- lenge is to define features on widely vary- ing candidate entity lists. We tackle this by abstracting list elements and using aggre- gate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline. ",,,,ACL
38,2014,Incremental Joint Extraction of Entity Mentions and Relations,"Qi Li, Heng Ji","We present an incremental joint frame- work to simultaneously extract entity men- tions and relations using structured per- ceptron with efficient beam-search. A segment-based decoder based on the idea of semi-Markov chain is adopted to the new framework as opposed to traditional token-based tagging. In addition, by virtue of the inexact search, we developed a num- ber of new and effective global features as soft constraints to capture the inter- dependency among entity mentions and relations. Experiments on Automatic Con- tent Extraction (ACE) 1 corpora demon- strate that our joint model significantly outperforms a strong pipelined baseline, which attains better performance than the best-reported end-to-end system. ",,,,ACL
39,2014,That’s Not What I Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text,"Manjuan Duan, Michael White","We investigate whether parsers can be used for self-monitoring in surface real- ization in order to avoid egregious errors involving “vicious” ambiguities, namely those where the intended interpretation fails to be considerably more likely than alternative ones. Using parse accuracy in a simple reranking strategy for self- monitoring, we find that with a state- of-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make. How- ever, by using an SVM ranker to combine the realizer’s model score together with features from multiple parsers, including ones designed to make the ranker more ro- bust to parsing mistakes, we show that sig- nificant increases in BLEU scores can be achieved. Moreover, via a targeted man- ual analysis, we demonstrate that the SVM reranker frequently manages to avoid vi- cious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy. ",,,,ACL
40,2014,Surface Realisation from Knowledge-Bases,"Bikash Gyawali, Claire Gardent","We present a simple, data-driven approach to generation from knowledge bases (KB). A key feature of this approach is that grammar induction is driven by the ex- tended domain of locality principle of TAG (Tree Adjoining Grammar); and that it takes into account both syntactic and semantic information. The resulting ex- tracted TAG includes a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data. Experimental evaluation on the KBGen data shows that our model outper- forms a data-driven generate-and-rank ap- proach based on an automatically induced probabilistic grammar; and is comparable with a handcrafted symbolic approach. ",,,,ACL
41,2014,Hybrid Simplification using Deep Semantics and Machine Translation,"Shashi Narayan, Claire Gardent","We present a hybrid approach to sentence simplification which combines deep se- mantics and monolingual machine transla- tion to derive simple sentences from com- plex ones. The approach differs from pre- vious work in two main ways. First, it is semantic based in that it takes as in- put a deep semantic representation rather than e.g., a sentence or a parse tree. Sec- ond, it combines a simplification model for splitting and deletion with a monolin- gual translation model for phrase substi- tution and reordering. When compared against current state of the art methods, our model yields significantly simpler out- put that is both grammatical and meaning preserving. ",,,,ACL
42,2014,Grammatical Relations in Chinese: GB-Ground Extraction and Data-Driven Parsing,"Weiwei Sun, Yantao Du, Xin Kou, Shuoyang Ding","This paper is concerned with building linguistic re- sources and statistical parsers for deep grammatical relation (GR) analysis of Chinese texts. A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general di- rected dependency graphs. The reliability of this linguistically-motivated GR extraction procedure is highlighted by manual evaluation. Based on the converted corpus, we study transition-based, data- driven models for GR parsing. We present a novel transition system which suits GR graphs better than existing systems. The key idea is to introduce a new type of transition that reorders top k elements in the memory module. Evaluation gauges how successful GR parsing for Chinese can be by applying data- driven models. ",,,,ACL
43,2014,Ambiguity-aware Ensemble Training for Semi-supervised Dependency Parsing,"Zhenghua Li, Min Zhang, Wenliang Chen","This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1- best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. This framework offers two promising advantages. 1) ambiguity encoded in parse forests compromises noise in 1-best parse trees. During training, the parser is aware of these ambiguous structures, and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves. 2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. ",,,,ACL
44,2014,A Robust Approach to Aligning Heterogeneous Lexical Resources,"Mohammad Taher Pilehvar, Roberto Navigli","Lexical resource alignment has been an active field of research over the last decade. However, prior methods for align- ing lexical resources have been either spe- cific to a particular pair of resources, or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned. Here we present a unified approach that can be applied to an arbitrary pair of lexical resources, includ- ing machine-readable dictionaries with no network structure. Our approach leverages a similarity measure that enables the struc- tural comparison of senses across lexical resources, achieving state-of-the-art per- formance on the task of aligning WordNet to three different collaborative resources: Wikipedia, Wiktionary and OmegaWiki. ",,,,ACL
45,2014,Predicting the relevance of distributional semantic similarity with contextual information,"Philippe Muller, Cécile Fabre, Clémentine Adam","Using distributional analysis methods to compute semantic proximity links be- tween words has become commonplace in NLP. The resulting relations are often noisy or difficult to interpret in general. This paper focuses on the issues of eval- uating a distributional resource and filter- ing the relations it contains, but instead of considering it in abstracto, we focus on pairs of words in context. In a dis- course, we are interested in knowing if the semantic link between two items is a by- product of textual coherence or is irrele- vant. We first set up a human annotation of semantic links with or without contex- tual information to show the importance of the textual context in evaluating the rele- vance of semantic similarity, and to assess the prevalence of actual semantic relations between word tokens. We then built an ex- periment to automatically predict this rel- evance, evaluated on the reliable reference data set which was the outcome of the first annotation. We show that in-document in- formation greatly improve the prediction made by the similarity level alone. ",,,,ACL
46,2014,Interpretable Semantic Vectors from a Joint Model of Brain- and Text- Based Meaning,"Alona Fyshe, Partha P. Talukdar, Brian Murphy, Tom M. Mitchell","Vector space models (VSMs) represent word meanings as points in a high dimen- sional space. VSMs are typically created using a large text corpora, and so repre- sent word semantics as observed in text. We present a new algorithm (JNNSE) that can incorporate a measure of semantics not previously used to create VSMs: brain activation data recorded while people read words. The resulting model takes advan- tage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representa- tion of semantics. Evaluations show that the model 1) matches a behavioral mea- sure of semantics more closely, 2) can be used to predict corpus data for unseen words and 3) has predictive power that generalizes across brain imaging technolo- gies and across subjects. We believe that the model is thus a more faithful represen- tation of mental vocabularies. ",,,,ACL
47,2014,Single-Agent vs. Multi-Agent Techniques for Concurrent Reinforcement Learning of Negotiation Dialogue Policies,"Kallirroi Georgila, Claire Nelson, David Traum","We use single-agent and multi-agent Rein- forcement Learning (RL) for learning dia- logue policies in a resource allocation ne- gotiation scenario. Two agents learn con- currently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from. In particular, we compare the Q- learning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the sce- nario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate. Our re- sults show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly. We also show that very high gradually decreasing exploration rates are required for convergence. We conclude that multi- agent RL of dialogue policies is a promis- ing alternative to using single-agent RL and SUs or learning directly from corpora. ",,,,ACL
48,2014,A Linear-Time Bottom-Up Discourse Parser with Constraints and Post-Editing,"Vanessa Wei Feng, Graeme Hirst","Text-level discourse parsing remains a challenge. The current state-of-the-art overall accuracy in relation assignment is 55.73%, achieved by Joty et al. (2013). However, their model has a high order of time complexity, and thus cannot be ap- plied in practice. In this work, we develop a much faster model whose time complex- ity is linear in the number of sentences. Our model adopts a greedy bottom-up ap- proach, with two linear-chain CRFs ap- plied in cascade as local classifiers. To en- hance the accuracy of the pipeline, we add additional constraints in the Viterbi decod- ing of the first CRF. In addition to effi- ciency, our parser also significantly out- performs the state of the art. Moreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy. ",,,,ACL
49,2014,Negation Focus Identification with Contextual Discourse Information,"Bowei Zou, Guodong Zhou, Qiaoming Zhu","Negative expressions are common in natural language text and play a critical role in in- formation extraction. However, the perfor- mances of current systems are far from satis- faction, largely due to its focus on intra- sentence information and its failure to con- sider inter-sentence information. In this paper, we propose a graph model to enrich intra- sentence features with inter-sentence features from both lexical and topic perspectives. Evaluation on the *SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus iden- tification and justifies the effectiveness of our graph model in capturing such global infor- mation. * ",,,,ACL
50,2014,New Word Detection for Sentiment Analysis,"Minlie Huang, Borui Ye, Yichen Wang, Haiqiang Chen","Automatic extraction of new words is an indispensable precursor to many NLP tasks such as Chinese word segmentation, named entity extraction, and sentimen- t analysis. This paper aims at extract- ing new sentiment words from large-scale user-generated content. We propose a ful- ly unsupervised, purely data-driven frame- work for this purpose. We design statisti- cal measures respectively to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word. The method is almost free of linguistic re- sources (except POS tags), and requires no elaborated linguistic rules. We also demonstrate how new sentiment word will benefit sentiment analysis. Experiment re- sults demonstrate the effectiveness of the proposed method. ",,,,ACL
51,2014,ReNew: A Semi-Supervised Framework for Generating Domain-Specific Lexicons and Sentiment Analysis,"Zhe Zhang, Munindar P. Singh","The sentiment captured in opinionated text provides interesting and valuable informa- tion for social media services. However, due to the complexity and diversity of linguistic representations, it is challeng- ing to build a framework that accurately extracts such sentiment. We propose a semi-supervised framework for generat- ing a domain-specific sentiment lexicon and inferring sentiments at the segment level. Our framework can greatly reduce the human effort for building a domain- specific sentiment lexicon with high qual- ity. Specifically, in our evaluation, work- ing with just 20 manually labeled reviews, it generates a domain-specific sentiment lexicon that yields weighted average F- Measure gains of 3%. Our sentiment clas- sification model achieves approximately 1% greater accuracy than a state-of-the-art approach based on elementary discourse units. ",,,,ACL
52,2014,A Decision-Theoretic Approach to Natural Language Generation,"Nathan McKinley, Soumya Ray","We study the problem of generating an En- glish sentence given an underlying prob- abilistic grammar, a world and a com- municative goal. We model the genera- tion problem as a Markov decision process with a suitably defined reward function that reflects the communicative goal. We then use probabilistic planning to solve the MDP and generate a sentence that, with high probability, accomplishes the com- municative goal. We show empirically that our approach can generate complex sen- tences with a speed that generally matches or surpasses the state of the art. Further, we show that our approach is anytime and can handle complex communicative goals, including negated goals. ",,,,ACL
53,2014,Generating Code-switched Text for Lexical Learning,"Igor Labutov, Hod Lipson","A vast majority of L1 vocabulary acqui- sition occurs through incidental learning during reading (Nation, 2001; Schmitt et al., 2001). We propose a probabilistic ap- proach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading. Our model that takes as input a bilingual dictionary and an English text, and gener- ates a code-switched text that optimizes a defined “learnability” metric by construct- ing a factor graph over lexical mentions. Using an artificial language vocabulary, we evaluate a set of algorithms for gener- ating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence com- pletion task. ",,,,ACL
54,2014,Omni-word Feature and Soft Constraint for Chinese Relation Extraction,"Yanping Chen, Qinghua Zheng, Wei Zhang","Chinese is an ancient hieroglyphic. It is inat- tentive to structure. Therefore, segmenting and parsing Chinese are more difficult and less accurate. In this paper, we propose an Omni- word feature and a soft constraint method for Chinese relation extraction. The Omni-word feature uses every potential word in a sentence as lexicon feature, reducing errors caused by word segmentation. In order to utilize the structure information of a relation instance, we discuss how soft constraint can be used to cap- ture the local dependency. Both Omni-word feature and soft constraint make a better use of sentence information and minimize the in- fluences caused by Chinese word segmenta- tion and parsing. We test these methods on the ACE 2005 RDC Chinese corpus. The re- sults show a significant improvement in Chi- nese relation extraction, outperforming other methods in F-score by 10% in 6 relation types and 15% in 18 relation subtypes. ",,,,ACL
55,2014,Bilingual Active Learning for Relation Classification via Pseudo Parallel Corpora,"Longhua Qian, Haotian Hui, Ya’nan Hu, Guodong Zhou","Active learning (AL) has been proven ef- fective to reduce human annotation ef- forts in NLP. However, previous studies on AL are limited to applications in a single language. This paper proposes a bilingual active learning paradigm for re- lation classification, where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle. Instead of using a parallel corpus, labeled and unlabeled instances in one language are translated into ones in the other language and all instances in both languages are then fed into a bilin- gual active learning engine as pseudo parallel corpora. Experimental results on the ACE RDC 2005 Chinese and English corpora show that bilingual active learn- ing for relation classification signifi- cantly outperforms monolingual active learning. ",,,,ACL
56,2014,Learning Soft Linear Constraints with Application to Citation Field Extraction,"Sam Anzaroot, Alexandre Passos, David Belanger, Andrew McCallum","Accurately segmenting a citation string into fields for authors, titles, etc. is a chal- lenging task because the output typically obeys various global constraints. Previous work has shown that modeling soft con- straints, where the model is encouraged, but not require to obey the constraints, can substantially improve segmentation per- formance. On the other hand, for impos- ing hard constraints, dual decomposition is a popular technique for efficient predic- tion given existing algorithms for uncon- strained inference. We extend dual decom- position to perform prediction subject to soft constraints. Moreover, with a tech- nique for performing inference given soft constraints, it is easy to automatically gen- erate large families of constraints and learn their costs with a simple convex optimiza- tion problem during training. This allows us to obtain substantial gains in accuracy on a new, challenging citation extraction dataset. ",,,,ACL
57,2014,A Study of Concept-based Weighting Regularization for Medical Records Search,"Yue Wang, Xitong Liu, Hui Fang","An important search task in the biomedical domain is to find medical records of pa- tients who are qualified for a clinical trial. One commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept- based representation. However, the map- ping results are not perfect, and none of previous work studied how to deal with them in the retrieval process. In this pa- per, we focus on addressing the limitations caused by the imperfect mapping results and study how to further improve the re- trieval performance of the concept-based ranking methods. In particular, we ap- ply axiomatic approaches and propose two weighting regularization methods that ad- just the weighting based on the relations among the concepts. Experimental results show that the proposed methods are effec- tive to improve the retrieval performance, and their performances are comparable to other top-performing systems in the TREC Medical Records Track. ",,,,ACL
58,2014,Learning to Predict Distributions of Words Across Domains,"Danushka Bollegala, David Weir, John Carroll","Although the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been lim- ited to a single domain because the dis- tribution of a word can vary between do- mains as the word’s predominant mean- ing changes. However, if it were pos- sible to predict how the distribution of a word changes from one domain to an- other, the predictions could be used to adapt a system trained in one domain to work in another. We propose an unsuper- vised method to predict the distribution of a word in one domain, given its distribu- tion in another domain. We evaluate our method on two tasks: cross-domain part- of-speech tagging and cross-domain sen- timent classification. In both tasks, our method significantly outperforms compet- itive baselines and returns results that are statistically comparable to current state- of-the-art methods, while requiring no task-specific customisations. ",,,,ACL
59,2014,How to make words with vectors: Phrase generation in distributional semantics,"Georgiana Dinu, Marco Baroni","We introduce the problem of generation in distributional semantics: Given a distri- butional vector representing some mean- ing, how can we generate the phrase that best expresses that meaning? We mo- tivate this novel challenge on theoretical and practical grounds and propose a sim- ple data-driven approach to the estimation of generation functions. We test this in a monolingual scenario (paraphrase gen- eration) as well as in a cross-lingual set- ting (translation by synthesizing adjective- noun phrase vectors in English and gener- ating the equivalent expressions in Italian). ",,,,ACL
60,2014,Vector space semantics with frequency-driven motifs,"Shashank Srivastava, Eduard Hovy","Traditional models of distributional se- mantics suffer from computational issues such as data sparsity for individual lex- emes and complexities of modeling se- mantic composition when dealing with structures larger than single lexical items. In this work, we present a frequency- driven paradigm for robust distributional semantics in terms of semantically cohe- sive lineal constituents, or motifs. The framework subsumes issues such as dif- ferential compositional as well as non- compositional behavior of phrasal con- situents, and circumvents some problems of data sparsity by design. We design a segmentation model to optimally par- tition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically dis- ambiguated. Hellinger PCA embeddings learnt using the framework show competi- tive results on empirical tasks. ",,,,ACL
61,2014,Lexical Inference over Multi-Word Predicates: A Distributional Approach,"Omri Abend, Shay B. Cohen, Mark Steedman","Representing predicates in terms of their argument distribution is common practice in NLP. Multi-word predicates (MWPs) in this context are often either disregarded or considered as fixed expressions. The lat- ter treatment is unsatisfactory in two ways: (1) identifying MWPs is notoriously diffi- cult, (2) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts. We propose a novel approach that integrates the distributional representation of multiple sub-sets of the MWP’s words. We assume a latent distri- bution over sub-sets of the MWP, and esti- mate it relative to a downstream prediction task. Focusing on the supervised identi- fication of lexical inference relations, we compare against state-of-the-art baselines that consider a single sub-set of an MWP, obtaining substantial improvements. To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics. ",,,,ACL
62,2014,A Convolutional Neural Network for Modelling Sentences,"Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom","The ability to accurately represent sen- tences is central to language understand- ing. We describe a convolutional architec- ture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pool- ing, a global pooling operation over lin- ear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily ap- plicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment predic- tion, six-way question classification and Twitter sentiment prediction by distant su- pervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline. ",,,,ACL
63,2014,Online Learning in Tensor Space,"Yuan Cao, Sanjeev Khudanpur","We propose an online learning algorithm based on tensor-space models. A tensor- space model represents data in a compact way, and via rank-1 approximation the weight tensor can be made highly struc- tured, resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models. This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is de- fined but very limited resources are avail- able for training. We apply with the pro- posed algorithm to a parsing task, and show that even with very little training data the learning algorithm based on a ten- sor model performs well, and gives signif- icantly better results than standard learn- ing algorithms based on traditional vector- space models. ",,,,ACL
64,2014,Graph-based Semi-Supervised Learning of Translation Models from Monolingual Data,"Avneesh Saluja, Hany Hassan, Kristina Toutanova, Chris Quirk","Statistical phrase-based translation learns translation rules from bilingual corpora, and has traditionally only used monolin- gual evidence to construct features that rescore existing translation candidates. In this work, we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data. The proposed tech- nique first constructs phrase graphs using both source and target language mono- lingual corpora. Next, graph propaga- tion identifies translations of phrases that were not observed in the bilingual cor- pus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrase- based systems, leading to consistent im- provements between 1 and 4 BLEU points on standard evaluation sets. ",,,,ACL
65,2014,Using Discourse Structure Improves Machine Translation Evaluation,"Francisco Guzmán, Shafiq Joty, Lluís Màrquez, Preslav Nakov","We present experiments in using dis- course structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level. Rather than proposing a single new metric, we show that discourse information is com- plementary to the state-of-the-art evalu- ation metrics, and thus should be taken into account in the development of future richer evaluation metrics. ",,,,ACL
66,2014,Learning Continuous Phrase Representations for Translation Modeling,"Jianfeng Gao, Xiaodong He, Wen-tau Yih, Li Deng","This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representa- tions, whose distributed nature enables the sharing of related phrases in their represen- tations. A pair of source and target phrases are projected into continuous-valued vec- tor representations in a low-dimensional latent space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a neural network whose weights are learned on parallel training data. Experimental evaluation has been performed on two WMT translation tasks. Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points. ",,,,ACL
67,2014,Adaptive Quality Estimation for Machine Translation,"Marco Turchi, Antonios Anastasopoulos, José G. C. de Souza, Matteo Negri","The automatic estimation of machine translation (MT) output quality is a hard task in which the selection of the appro- priate algorithm and the most predictive features over reasonably sized training sets plays a crucial role. When moving from controlled lab evaluations to real-life sce- narios the task becomes even harder. For current MT quality estimation (QE) sys- tems, additional complexity comes from the difficulty to model user and domain changes. Indeed, the instability of the sys- tems with respect to data coming from dif- ferent distributions calls for adaptive so- lutions that react to new operating con- ditions. To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes. Contrastive exper- iments in different testing conditions in- volving user and domain changes demon- strate the effectiveness of our approach. ",,,,ACL
68,2014,Learning Grounded Meaning Representations with Autoencoders,"Carina Silberer, Mirella Lapata","In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from tex- tual and visual input. The two modali- ties are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similar- ity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models. ",,,,ACL
69,2014,Joint POS Tagging and Transition-based Constituent Parsing in Chinese with Non-local Features,"Zhiguo Wang, Nianwen Xue","We propose three improvements to ad- dress the drawbacks of state-of-the-art transition-based constituent parsers. First, to resolve the error propagation problem of the traditional pipeline approach, we incorporate POS tagging into the syntac- tic parsing process. Second, to allevi- ate the negative influence of size differ- ences among competing action sequences, we align parser states during beam-search decoding. Third, to enhance the pow- er of parsing models, we enlarge the fea- ture set with non-local features and semi- supervised word cluster features. Exper- imental results show that these modifica- tions improve parsing performance signif- icantly. Evaluated on the Chinese Tree- Bank (CTB), our final performance reach- es 86.3% (F1) when trained on CTB 5.1, and 87.1% when trained on CTB 6.0, and these results outperform all state-of-the-art parsers. ",,,,ACL
70,2014,Strategies for Contiguous Multiword Expression Analysis and Dependency Parsing,"Marie Candito, Matthieu Constant","In this paper, we investigate various strate- gies to predict both syntactic dependency parsing and contiguous multiword expres- sion (MWE) recognition, testing them on the dependency version of French Tree- bank (Abeill′e and Barrier, 2004), as in- stantiated in the SPMRL Shared Task (Seddah et al., 2013). Our work focuses on using an alternative representation of syntactically regular MWEs, which cap- tures their syntactic internal structure. We obtain a system with comparable perfor- mance to that of previous works on this dataset, but which predicts both syntactic dependencies and the internal structure of MWEs. This can be useful for capturing the various degrees of semantic composi- tionality of MWEs. ",,,,ACL
71,2014,Correcting Preposition Errors in Learner English Using Error Case Frames and Feedback Messages,"Ryo Nagata, Mikko Vilenius, Edward Whittaker","This paper presents a novel framework called error case frames for correcting preposition errors. They are case frames specially designed for describing and cor- recting preposition errors. Their most dis- tinct advantage is that they can correct er- rors with feedback messages explaining why the preposition is erroneous. This pa- per proposes a method for automatically generating them by comparing learner and native corpora. Experiments show (i) au- tomatically generated error case frames achieve a performance comparable to con- ventional methods; (ii) error case frames are intuitively interpretable and manually modifiable to improve them; (iii) feedback messages provided by error case frames are effective in language learning assis- tance. Considering these advantages and the fact that it has been difficult to provide feedback messages by automatically gen- erated rules, error case frames will likely be one of the major approaches for prepo- sition error correction. ",,,,ACL
72,2014,Kneser-Ney Smoothing on Expected Counts,"Hui Zhang, David Chiang","Widely used in speech and language pro- cessing, Kneser-Ney (KN) smoothing has consistently been shown to be one of the best-performing smoothing methods. However, KN smoothing assumes integer counts, limiting its potential uses—for ex- ample, inside Expectation-Maximization. In this paper, we propose a generaliza- tion of KN smoothing that operates on fractional counts, or, more precisely, on distributions over counts. We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts, and apply it to two tasks where KN smoothing was not applicable before: one in language model adaptation, and the other in word alignment. In both cases, our method improves performance signifi- cantly. ",,,,ACL
73,2014,Robust Entity Clustering via Phylogenetic Inference,"Nicholas Andrews, Jason Eisner, Mark Dredze","Entity clustering must determine when two named-entity mentions refer to the same entity. Typical approaches use a pipeline ar- chitecture that clusters the mentions using fixed or learned measures of name and con- text similarity. In this paper, we propose a model for cross-document coreference res- olution that achieves robustness by learn- ing similarity from unlabeled data. The generative process assumes that each entity mention arises from copying and option- ally mutating an earlier name from a sim- ilar context. Clustering the mentions into entities depends on recovering this copying tree jointly with estimating models of the mutation process and parent selection pro- cess. We present a block Gibbs sampler for posterior inference and an empirical evalu- ation on several datasets. ",,,,ACL
74,2014,Linguistic Structured Sparsity in Text Categorization,"Dani Yogatama, Noah A. Smith","We introduce three linguistically moti- vated structured regularizers based on parse trees, topics, and hierarchical word clusters for text categorization. These regularizers impose linguistic bias in fea- ture weights, enabling us to incorporate prior knowledge into conventional bag- of-words models. We show that our structured regularizers consistently im- prove classification accuracies compared to standard regularizers that penalize fea- tures in isolation (such as lasso, ridge, and elastic net regularizers) on a range of datasets for various text prediction prob- lems: topic classification, sentiment anal- ysis, and forecasting. ",,,,ACL
75,2014,Perplexity on Reduced Corpora,Hayato Kobayashi,"This paper studies the idea of remov- ing low-frequency words from a corpus, which is a common practice to reduce computational costs, from a theoretical standpoint. Based on the assumption that a corpus follows Zipf’s law, we derive trade- off formulae of the perplexity of k -gram models and topic models with respect to the size of the reduced vocabulary. In ad- dition, we show an approximate behavior of each formula under certain conditions. We verify the correctness of our theory on synthetic corpora and examine the gap be- tween theory and practice on real corpora. ",,,,ACL
76,2014,Robust Domain Adaptation for Relation Extraction via Clustering Consistency,"Minh Luan Nguyen, Ivor W. Tsang, Kian Ming A. Chai, Hai Leong Chieu","We propose a two-phase framework to adapt existing relation extraction classi- fiers to extract relations for new target do- mains. We address two challenges: neg- ative transfer when knowledge in source domains is used without considering the differences in relation distributions; and lack of adequate labeled samples for rarer relations in the new domain, due to a small labeled data set and imbalance rela- tion distributions. Our framework lever- ages on both labeled and unlabeled data in the target domain. First, we determine the relevance of each source domain to the target domain for each relation type, using the consistency between the clus- tering given by the target domain labels and the clustering given by the predic- tors trained for the source domain. To overcome the lack of labeled samples for rarer relations, these clusterings operate on both the labeled and unlabeled data in the target domain. Second, we trade-off between using relevance-weighted source- domain predictors and the labeled target data. Again, to overcome the imbalance distribution, the source-domain predictors operate on the unlabeled target data. Our method outperforms numerous baselines and a weakly-supervised relation extrac- tion method on ACE 2004 and YAGO. ",,,,ACL
77,2014,Encoding Relation Requirements for Relation Extraction via Joint Inference,"Liwei Chen, Yansong Feng, Songfang Huang, Yong Qin","Most existing relation extraction models make predictions for each entity pair lo- cally and individually, while ignoring im- plicit global clues available in the knowl- edge base, sometimes leading to conflicts among local predictions from different en- tity pairs. In this paper, we propose a joint inference framework that utilizes these global clues to resolve disagree- ments among local predictions. We ex- ploit two kinds of clues to generate con- straints which can capture the implicit type and cardinality requirements of a relation. Experimental results on three datasets, in both English and Chinese, show that our framework outperforms the state-of-the- art relation extraction models when such clues are applicable to the datasets. And, we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human. ",,,,ACL
78,2014,Medical Relation Extraction with Manifold Models,"Chang Wang, James Fan","In this paper, we present a manifold model for medical relation extraction. Our model is built upon a medical corpus containing 80M sentences (11 gigabyte text) and de- signed to accurately and ef?ciently detect the key medical relations that can facilitate clinical decision making. Our approach integrates domain speci?c parsing and typ- ing systems, and can utilize labeled as well as unlabeled examples. To provide users with more ?exibility, we also take label weight into consideration. Effectiveness of our model is demonstrated both theo- retically with a proof to show that the so- lution is a closed-form solution and exper- imentally with positive results in experi- ments. ",,,,ACL
79,2014,Distant Supervision for Relation Extraction with Matrix Completion,"Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu","The essence of distantly supervised rela- tion extraction is that it is an incomplete multi-label classification problem with s- parse and noisy features. To tackle the s- parsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a s- parse matrix that concatenates training and testing textual features with training label- s. Our algorithmic framework is based on the assumption that the rank of item-by- feature and item-by-label joint matrix is low. We apply two optimization model- s to recover the underlying low-rank ma- trix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the glob- al optimum. Experiments on two wide- ly used datasets with different dimension- s of textual features demonstrate that our low-rank matrix completion approach sig- nificantly outperforms the baseline and the state-of-the-art methods. ",,,,ACL
80,2014,Enhancing Grammatical Cohesion: Generating Transitional Expressions for SMT,"Mei Tu, Yu Zhou, Chengqing Zong","Transitional expressions provide glue that holds ideas together in a text and enhance the logical organization, which together help im- prove readability of a text. However, in most current statistical machine translation (SMT) systems, the outputs of compound-complex sentences still lack proper transitional expres- sions. As a result, the translations are often hard to read and understand. To address this issue, we propose two novel models to en- courage generating such transitional expres- sions by introducing the source compound- complex sentence structure (CSS). Our models include a CSS-based translation model, which generates new CSS-based translation rules, and a generative transfer model, which en- courages producing transitional expressions during decoding. The two models are integrat- ed into a hierarchical phrase-based translation system to evaluate their effectiveness. The ex- perimental results show that significant im- provements are achieved on various test data meanwhile the translations are more cohesive and smooth. ",,,,ACL
81,2014,Adaptive HTER Estimation for Document-Specific MT Post-Editing,"Fei Huang, Jian-Ming Xu, Abraham Ittycheriah, Salim Roukos","We present an adaptive translation qual- ity estimation (QE) method to predict the human-targeted translation error rate (HTER) for a document-specific machine translation model. We first introduce fea- tures derived internal to the translation de- coding process as well as externally from the source sentence analysis. We show the effectiveness of such features in both classification and regression of MT qual- ity. By dynamically training the QE model for the document-specific MT model, we are able to achieve consistency and pre- diction quality across multiple documents, demonstrated by the higher correlation co- efficient and F-scores in finding Good sen- tences. Additionally, the proposed method is applied to IBM English-to-Japanese MT post editing field study and we observe strong correlation with human preference, with a 10% increase in human translators’ productivity. ",,,,ACL
82,2014,Translation Assistance by Translation of L1 Fragments in an L2 Context,"Maarten van Gompel, Antal van den Bosch",In this paper we present new research in translation assistance. We describe a sys- tem capable of translating native language (L1) fragments to foreign language (L2) fragments in an L2 context. Practical ap- plications of this research can be framed in the context of second language learning. The type of translation assistance system under investigation here encourages lan- guage learners to write in their target lan- guage while allowing them to fall back to their native language in case the correct word or expression is not known. These code switches are subsequently translated to L2 given the L2 context. We study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense dis- ambiguation baselines. A classification- based approach is presented that is in- deed found to improve significantly over these baselines by making use of a contex- tual window spanning a small number of neighbouring words. ,,,,ACL
83,2014,Response-based Learning for Grounded Machine Translation,"Stefan Riezler, Patrick Simianer, Carolin Haas","We propose a novel learning approach for statistical machine translation (SMT) that allows to extract supervision signals for structured learning from an extrinsic re- sponse to a translation input. We show how to generate responses by grounding SMT in the task of executing a seman- tic parse of a translated query against a database. Experiments on the G EO - QUERY database show an improvement of about 6 points in F1-score for response- based learning over learning from refer- ences only on returning the correct an- swer from a semantic parse of a translated query. In general, our approach alleviates the dependency on human reference trans- lations and solves the reachability problem in structured learning for SMT. ",,,,ACL
84,2014,"Modelling Events through Memory-based, Open-IE Patterns for Abstractive Summarization","Daniele Pighin, Marco Cornolti, Enrique Alfonseca, Katja Filippova","Abstractive text summarization of news requires a way of representing events, such as a collection of pattern clusters in which every cluster represents an event (e.g., marriage) and every pattern in the clus- ter is a way of expressing the event (e.g., X married Y, X and Y tied the knot). We compare three ways of extracting event patterns: heuristics-based, compression- based and memory-based. While the for- mer has been used previously in multi- document abstraction, the latter two have never been used for this task. Compared with the first two techniques, the memory- based method allows for generating sig- nificantly more grammatical and informa- tive sentences, at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances. To this end, we introduce a data structure and a search method that make it possible to efficiently extrapolate from every sentence the parse sub-trees that match against any of the stored utterances. ",,,,ACL
85,2014,Hierarchical Summarization: Scaling Up Multi-Document Summarization,"Janara Christensen, Stephen Soderland, Gagan Bansal, Mausam","Multi-document summarization (MDS) systems have been designed for short, un- structured summaries of 10-15 documents, and are inadequate for larger document collections. We propose a new approach to scaling up summarization called hierar- chical summarization, and present the first implemented system, S UMMA . S UMMA produces a hierarchy of relatively short summaries, in which the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest. S UMMA optimizes for coherence as well as cover- age of salient information. In an Amazon Mechanical Turk evaluation, users pref- ered S UMMA ten times as often as flat MDS and three times as often as timelines. ",,,,ACL
86,2014,Query-Chain Focused Summarization,"Tal Baumel, Raphael Cohen, Michael Elhadad","Update summarization is a form of multi- document summarization where a document set must be summarized in the context of other documents assumed to be known. Efficient update summarization must focus on identify- ing new information and avoiding repetition of known information. In Query-focused summa- rization, the task is to produce a summary as an answer to a given query. We introduce a new task, Query-Chain Summarization, which combines aspects of the two previous tasks: starting from a given document set, increas- ingly specific queries are considered, and a new summary is produced at each step. This process models exploratory search: a user ex- plores a new topic by submitting a sequence of queries, inspecting a summary of the result set and phrasing a new query at each step. We present a novel dataset comprising 22 query- chains sessions of length up to 3 with 3 match- ing human summaries each in the consumer- health domain. Our analysis demonstrates that summaries produced in the context of such exploratory process are different from in- formative summaries. We present an algorithm for Query-Chain Summarization based on a new LDA topic model variant. Evaluation in- dicates the algorithm improves on strong base- lines. ",,,,ACL
87,2014,Exploiting Timelines to Enhance Multi-document Summarization,"Jun-Ping Ng, Yan Chen, Min-Yen Kan, Zhoujun Li","We study the use of temporal information in the form of timelines to enhance multi- document summarization. We employ a fully automated temporal processing sys- tem to generate a timeline for each in- put document. We derive three features from these timelines, and show that their use in supervised summarization lead to a significant 4.1% improvement in ROUGE performance over a state-of-the-art base- line. In addition, we propose T IME MMR, a modification to Maximal Marginal Rel- evance that promotes temporal diversity by way of computing time span similar- ity, and show its utility in summarizing certain document sets. We also propose a filtering metric to discard noisy timelines generated by our automatic processes, to purify the timeline input for summariza- tion. By selectively using timelines guided by filtering, overall summarization perfor- mance is increased by a significant 5.9%. ",,,,ACL
88,2014,A chance-corrected measure of inter-annotator agreement for syntax,Arne Skjærholt,"Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used. With this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncor- rected parser evaluation metrics such as bracket F 1 (for phrase structure) and ac- curacy scores (for dependencies). In this work we present a chance-corrected metric based on Krippendorff’s α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications. To evaluate our metric we first present a number of synthetic experi- ments to better control the sources of noise and gauge the metric’s responses, before finally contrasting the behaviour of our chance-corrected metric with that of un- corrected parser evaluation metrics on real corpora. 1 ",,,,ACL
89,2014,Two Is Bigger (and Better) Than One: the Wikipedia Bitaxonomy Project,"Tiziano Flati, Daniele Vannella, Tommaso Pasini, Roberto Navigli","We present WiBi, an approach to the automatic creation of a bitaxonomy for Wikipedia, that is, an integrated taxon- omy of Wikipage pages and categories. We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show higher quality and cov- erage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy. WiBi is available at http://wibitaxonomy.org. ",,,,ACL
90,2014,Information Extraction over Structured Data: Question Answering with Freebase,"Xuchen Yao, Benjamin Van Durme","Answering natural language questions us- ing the Freebase knowledge base has re- cently been explored as a platform for ad- vancing the state of the art in open do- main semantic parsing. Those efforts map questions to sophisticated meaning repre- sentations that are then attempted to be matched against viable answer candidates in the knowledge base. Here we show that relatively modest information extrac- tion techniques, when paired with a web- scale corpus, can outperform these sophis- ticated approaches by roughly 34% rela- tive gain. ",,,,ACL
91,2014,Knowledge-Based Question Answering as Machine Translation,"Junwei Bao, Nan Duan, Ming Zhou, Tiejun Zhao","A typical knowledge-based question an- swering (KB-QA) system faces two chal- lenges: one is to transform natural lan- guage questions into their meaning repre- sentations (MRs); the other is to retrieve answers from knowledge bases (KBs) us- ing generated MRs. Unlike previous meth- ods which treat them in a cascaded man- ner, we present a translation-based ap- proach to solve these two tasks in one u- nified framework. We translate questions to answers based on CYK parsing. An- swers as translations of the span covered by each CYK cell are obtained by a ques- tion translation method, which first gener- ates formal triple queries as MRs for the span based on question patterns and re- lation expressions, and then retrieves an- swers from a given KB based on triple queries generated. A linear model is de- fined over derivations, and minimum er- ror rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system us- ing a state-of-the-art semantic parser, our method achieves better results. ",,,,ACL
92,2014,Discourse Complements Lexical Semantics for Non-factoid Answer Reranking,"Peter Jansen, Mihai Surdeanu, Peter Clark","We propose a robust answer reranking model for non-factoid questions that inte- grates lexical semantics with discourse in- formation, driven by two representations of discourse: a shallow representation cen- tered around discourse markers, and a deep one based on Rhetorical Structure Theory. We evaluate the proposed model on two corpora from different genres and domains: one from Yahoo! Answers and one from the biology domain, and two types of non-factoid questions: manner and reason. We experimentally demon- strate that the discourse structure of non- factoid answers provides information that is complementary to lexical semantic sim- ilarity between question and answer, im- proving performance up to 24% (relative) over a state-of-the-art model that exploits lexical semantic similarity alone. We fur- ther demonstrate excellent domain transfer of discourse information, suggesting these discourse features have general utility to non-factoid question answering. ",,,,ACL
93,2014,"Toward Future Scenario Generation: Extracting Event Causality Exploiting Semantic Relation, Context, and Association Features","Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer, Motoki Sano","We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture → exacerbate desertification from the web using se- mantic relation (between nouns), context, and association features. Experiments show that our method outperforms base- lines that are based on state-of-the-art methods. We also propose methods of generating future scenarios like conduct slash-and-burn agriculture → exacerbate desertification → increase Asian dust (from China) → asthma gets worse. Experi- ments show that we can generate 50,000 scenarios with 68% precision. We also generated a scenario deforestation con- tinues → global warming worsens → sea temperatures rise → vibrio parahaemolyti- cus fouls (water), which is written in no document in our input web corpus crawled in 2007. But the vibrio risk due to global warming was observed in Baker-Austin et al. (2013). Thus, we “predicted” the future event sequence in a sense. ",,,,ACL
94,2014,Cross-narrative Temporal Ordering of Medical Events,"Preethi Raghavan, Eric Fosler-Lussier, Noémie Elhadad, Albert M. Lai","Cross-narrative temporal ordering of med- ical events is essential to the task of gen- erating a comprehensive timeline over a patient’s history. We address the prob- lem of aligning multiple medical event se- quences, corresponding to different clin- ical narratives, comparing the following approaches: (1) A novel weighted finite state transducer representation of medi- cal event sequences that enables compo- sition and search for decoding, and (2) Dynamic programming with iterative pair- wise alignment of multiple sequences us- ing global and local alignment algorithms. The cross-narrative coreference and tem- poral relation weights used in both these approaches are learned from a corpus of clinical narratives. We present results us- ing both approaches and observe that the finite state transducer approach performs performs significantly better than the dy- namic programming one by 6.8% for the problem of multiple-sequence alignment. ",,,,ACL
95,2014,Language-Aware Truth Assessment of Fact Candidates,"Ndapandula Nakashole, Tom M. Mitchell","This paper introduces FactChecker, language-aware approach to truth-finding. FactChecker differs from prior approaches in that it does not rely on iterative peer voting, instead it leverages language to infer believability of fact candidates. In particular, FactChecker makes use of lin- guistic features to detect if a given source objectively states facts or is speculative and opinionated. To ensure that fact candidates mentioned in similar sources have similar believability, FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate. Our experiments on various datasets show that FactChecker yields higher accuracy than existing approaches. ",,,,ACL
96,2014,That’s sick dude!: Automatic identification of word sense change across different timescales,"Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Biemann","In this paper, we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books. We construct distribu- tional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the differ- ent time points. Subsequently, we com- pare these sense clusters of two different time points to find if (i) there is birth of a new sense or (ii) if an older sense has got split into more than one sense or (iii) if a newer sense has been formed from the joining of older senses or (iv) if a partic- ular sense has died. We conduct a thor- ough evaluation of the proposed method- ology both manually as well as through comparison with WordNet. Manual eval- uation indicates that the algorithm could correctly identify 60.4% birth cases from a set of 48 randomly picked samples and 57% split/join cases from a set of 21 ran- domly picked samples. Remarkably, in 44% cases the birth of a novel sense is attested by WordNet, while in 46% cases and 43% cases split and join are respec- tively confirmed by WordNet. Our ap- proach can be applied for lexicography, as well as for applications like word sense disambiguation or semantic search. ",,,,ACL
97,2014,A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes,"Daisuke Kawahara, Daniel W. Peterson, Martha Palmer","We present an unsupervised method for in- ducing verb classes from verb uses in giga- word corpora. Our method consists of two clustering steps: verb-specific seman- tic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only generate verb classes based on a massive amount of verb uses in a scalable manner, but also deal with verb polysemy, which is bypassed by most of the previous studies on verb clustering. In our exper- iments, we acquire semantic frames and verb classes from two giga-word corpora, the larger comprising 20 billion words. The effectiveness of our approach is veri- fied through quantitative evaluations based on polysemy-aware gold-standard data. ",,,,ACL
98,2014,Structured Learning for Taxonomy Induction with Belief Propagation,"Mohit Bansal, David Burkett, Gerard de Melo, Dan Klein","We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation. Our model incorporates heterogeneous re- lational evidence about both hypernymy and siblinghood, captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia ab- stracts. For efficient inference over tax- onomy structures, we use loopy belief propagation along with a directed span- ning tree algorithm for the core hyper- nymy factor. To train the system, we ex- tract sub-structures of WordNet and dis- criminatively learn to reproduce them, us- ing adaptive subgradient stochastic opti- mization. On the task of reproducing sub-hierarchies of WordNet, our approach achieves a 51% error reduction over a chance baseline, including a 15% error re- duction due to the non-hypernym-factored sibling features. On a comparison setup, we find up to 29% relative error reduction over previous work on ancestor F1. ",,,,ACL
99,2014,A Provably Correct Learning Algorithm for Latent-Variable PCFGs,"Shay B. Cohen, Michael Collins","We introduce a provably correct learning algorithm for latent-variable PCFGs. The algorithm relies on two steps: first, the use of a matrix-decomposition algorithm ap- plied to a co-occurrence matrix estimated from the parse trees in a training sample; second, the use of EM applied to a convex objective derived from the training sam- ples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and ef- fective in practice. ",,,,ACL
100,2014,Spectral Unsupervised Parsing with Additive Tree Metrics,"Ankur P. Parikh, Shay B. Cohen, Eric P. Xing","We propose a spectral approach for un- supervised constituent parsing that comes with theoretical guarantees on latent struc- ture recovery. Our approach is grammar- less – we directly learn the bracketing structure of a given sentence without us- ing a grammar model. The main algorithm is based on lifting the concept of additive tree metrics for structure learning of la- tent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples. Although finding the “minimal” latent tree is NP-hard in general, for the case of pro- jective trees we find that it can be found using bilexical parsing algorithms. Empir- ically, our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization. ",,,,ACL
101,2014,Weak semantic context helps phonetic learning in a model of infant language acquisition,"Stella Frank, Naomi H. Feldman, Sharon Goldwater","Learning phonetic categories is one of the first steps to learning a language, yet is hard to do using only distributional phonetic in- formation. Semantics could potentially be useful, since words with different mean- ings have distinct phonetics, but it is un- clear how many word meanings are known to infants learning phonetic categories. We show that attending to a weaker source of semantics, in the form of a distribution over topics in the current context, can lead to improvements in phonetic category learn- ing. In our model, an extension of a pre- vious model of joint word-form and pho- netic category inference, the probability of word-forms is topic-dependent, enabling the model to find significantly better pho- netic vowel categories and word-forms than a model with no semantic knowledge. ",,,,ACL
102,2014,Bootstrapping into Filler-Gap: An Acquisition Story,"Marten van Schijndel, Micha Elsner","Analyses of filler-gap dependencies usu- ally involve complex syntactic rules or heuristics; however recent results suggest that filler-gap comprehension begins ear- lier than seemingly simpler constructions such as ditransitives or passives. Therefore, this work models filler-gap acquisition as a byproduct of learning word orderings (e.g. SVO vs OSV), which must be done at a very young age anyway in order to extract meaning from language. Specifically, this model, trained on part-of-speech tags, rep- resents the preferred locations of semantic roles relative to a verb as Gaussian mix- tures over real numbers. This approach learns role assignment in filler-gap constructions in a manner con- sistent with current developmental findings and is extremely robust to initialization variance. Additionally, this model is shown to be able to account for a characteristic er- ror made by learners during this period (A and B gorped interpreted as A gorped B ). ",,,,ACL
103,2014,Nonparametric Learning of Phonological Constraints in Optimality Theory,"Gabriel Doyle, Klinton Bicknell, Roger Levy","We present a method to jointly learn fea- tures and weights directly from distri- butional data in a log-linear framework. Specifically, we propose a non-parametric Bayesian model for learning phonologi- cal markedness constraints directly from the distribution of input-output mappings in an Optimality Theory (OT) setting. The model uses an Indian Buffet Process prior to learn the feature values used in the log- linear method, and is the first algorithm for learning phonological constraints with- out presupposing constraint structure. The model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis, with a violation struc- ture corresponding to the standard con- straints. These results suggest an alterna- tive data-driven source for constraints in- stead of a fully innate constraint set. ",,,,ACL
104,2014,Active Learning with Efficient Feature Weighting Methods for Improving Data Quality and Classification Accuracy,"Justin Martineau, Lu Chen, Doreen Cheng, Amit Sheth","Many machine learning datasets are noisy with a substantial number of mislabeled instances. This noise yields sub-optimal classification performance. In this paper we study a large, low quality annotated dataset, created quickly and cheaply us- ing Amazon Mechanical Turk to crowd- source annotations. We describe compu- tationally cheap feature weighting tech- niques and a novel non-linear distribution spreading algorithm that can be used to it- eratively and interactively correcting mis- labeled instances to significantly improve annotation quality at low cost. Eight dif- ferent emotion extraction experiments on Twitter data demonstrate that our approach is just as effective as more computation- ally expensive techniques. Our techniques save a considerable amount of time. ",,,,ACL
105,2014,Political Ideology Detection Using Recursive Neural Networks,"Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, Philip Resnik","An individual’s words often reveal their po- litical ideology. Existing automated tech- niques to identify ideology from text focus on bags of words or wordlists, ignoring syn- tax. Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language, we apply a recursive neural network (RNN) framework to the task of identifying the po- litical position evinced by a sentence. To show the importance of modeling subsen- tential elements, we crowdsource political annotations at a phrase and sentence level. Our model outperforms existing models on our newly annotated dataset and an existing dataset. ",,,,ACL
106,2014,A Unified Model for Soft Linguistic Reordering Constraints in Statistical Machine Translation,"Junhui Li, Yuval Marton, Philip Resnik, Hal Daumé III","This paper explores a simple and effec- tive unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation sys- tem: 1) a syntactic reordering model that explores reorderings for context free grammar rules; and 2) a semantic re- ordering model that focuses on the re- ordering of predicate-argument structures. We develop novel features based on both models and use them as soft constraints to guide the translation process. Ex- periments on Chinese-English translation show that the reordering approach can sig- nificantly improve a state-of-the-art hier- archical phrase-based translation system. However, the gain achieved by the seman- tic reordering model is limited in the pres- ence of the syntactic reordering model, and we therefore provide a detailed analy- sis of the behavior differences between the two. ",,,,ACL
107,2014,Are Two Heads Better than One? Crowdsourced Translation via a Two-Step Collaboration of Non-Professional Translators and Editors,"Rui Yan, Mingkun Gao, Ellie Pavlick, Chris Callison-Burch","Crowdsourcing is a viable mechanism for creating training data for machine trans- lation. It provides a low cost, fast turn- around way of processing large volumes of data. However, when compared to pro- fessional translation, naive collection of translations from non-professionals yields low-quality results. Careful quality con- trol is necessary for crowdsourcing to work well. In this paper, we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals. We develop graph- based ranking models that automatically select the best output from multiple redun- dant versions of translations and edits, and improves translation quality closer to pro- fessionals. ",,,,ACL
108,2014,A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser Ney Smoothing,"Rene Pickhardt, Thomas Gottron, Martin Körner, Paul Georg Wagner","We introduce a novel approach for build- ing language models based on a system- atic, recursive exploration of skip n-gram models which are interpolated using modi- fied Kneser-Ney smoothing. Our approach generalizes language models as it contains the classical interpolation with lower or- der models as a special case. In this pa- per we motivate, formalize and present our approach. In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1% and 12.7% in comparison to traditional language mod- els using modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consis- tent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small train- ing data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity. ",,,,ACL
109,2014,A Semiparametric Gaussian Copula Regression Model for Predicting Financial Risks from Earnings Calls,"William Yang Wang, Zhenhao Hua","Earnings call summarizes the financial performance of a company, and it is an important indicator of the future financial risks of the company. We quantitatively study how earnings calls are correlated with the financial risks, with a special fo- cus on the financial crisis of 2009. In par- ticular, we perform a text regression task: given the transcript of an earnings call, we predict the volatility of stock prices from the week after the call is made. We pro- pose the use of copula: a powerful statis- tical framework that separately models the uniform marginals and their complex mul- tivariate stochastic dependencies, while not requiring any prior assumptions on the distributions of the covariate and the de- pendent variable. By performing probabil- ity integral transform, our approach moves beyond the standard count-based bag-of- words models in NLP, and improves pre- vious work on text regression by incor- porating the correlation among local fea- tures in the form of semiparametric Gaus- sian copula. In experiments, we show that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings. ",,,,ACL
110,2014,Polylingual Tree-Based Topic Models for Translation Domain Adaptation,"Yuening Hu, Ke Zhai, Vladimir Eidelman, Jordan Boyd-Graber","Topic models, an unsupervised technique for inferring translation domains improve machine translation quality. However, pre- vious work uses only the source language and completely ignores the target language, which can disambiguate domains. We pro- pose new polylingual tree-based topic mod- els to extract domain knowledge that con- siders both source and target languages and derive three different inference schemes. We evaluate our model on a Chinese to En- glish translation task and obtain up to 1.2 BLEU improvement over strong baselines. ",,,,ACL
111,2014,Low-Resource Semantic Role Labeling,"Matthew R. Gormley, Margaret Mitchell, Benjamin Van Durme, Mark Dredze","We explore the extent to which high- resource manual annotations such as tree- banks are necessary for the task of se- mantic role labeling (SRL). We examine how performance changes without syntac- tic supervision, comparing both joint and pipelined methods to induce latent syn- tax. This work highlights a new applica- tion of unsupervised grammar induction and demonstrates several approaches to SRL in the absence of supervised syntax. Our best models obtain competitive results in the high-resource setting and state-of- the-art results in the low resource setting, reaching 72.48% F1 averaged across lan- guages. We release our code for this work along with a larger toolkit for specifying arbitrary graphical structure. 1 ",,,,ACL
112,2014,Joint Syntactic and Semantic Parsing with Combinatory Categorial Grammar,"Jayant Krishnamurthy, Tom M. Mitchell","We present an approach to training a joint syntactic and semantic parser that com- bines syntactic training information from CCGbank with semantic training informa- tion from a knowledge base via distant su- pervision. The trained parser produces a full syntactic parse of any sentence, while simultaneously producing logical forms for portions of the sentence that have a se- mantic representation within the parser’s predicate vocabulary. We demonstrate our approach by training a parser whose se- mantic representation contains 130 pred- icates from the NELL ontology. A seman- tic evaluation demonstrates that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach. A syn- tactic evaluation on CCGbank demon- strates that the parser’s dependency F- score is within 2.5% of state-of-the-art. ",,,,ACL
113,2014,Learning Semantic Hierarchies via Word Embeddings,"Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che","Semantic hierarchy construction aims to build structures of concepts linked by hypernym–hyponym (“is-a”) relations. A major challenge for this task is the automatic discovery of such relations. This paper proposes a novel and effec- tive method for the construction of se- mantic hierarchies based on word em- beddings, which can be used to mea- sure the semantic relationship between words. We identify whether a candidate word pair has hypernym–hyponym rela- tion by using the word-embedding-based semantic projections between words and their hypernyms. Our result, an F-score of 73.74%, outperforms the state-of-the- art methods on a manually labeled test dataset. Moreover, combining our method with a previous manually-built hierarchy extension method can further improve F- score to 80.29%. ",,,,ACL
114,2014,Probabilistic Soft Logic for Semantic Textual Similarity,"Islam Beltagy, Katrin Erk, Raymond Mooney","Probabilistic Soft Logic (PSL) is a re- cently developed framework for proba- bilistic logic. We use PSL to combine logical and distributional representations of natural-language meaning, where distri- butional information is represented in the form of weighted inference rules. We ap- ply this framework to the task of Seman- tic Textual Similarity (STS) (i.e. judg- ing the semantic similarity of natural- language sentences), and show that PSL gives improved results compared to a pre- vious approach based on Markov Logic Networks (MLNs) and a purely distribu- tional approach. ",,,,ACL
115,2014,Abstractive Summarization of Spoken and Written Conversations Based on Phrasal Queries,"Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng","We propose a novel abstractive query- based summarization system for conversa- tions, where queries are defined as phrases reflecting a user information needs. We rank and extract the utterances in a con- versation based on the overall content and the phrasal query information. We clus- ter the selected sentences based on their lexical similarity and aggregate the sen- tences in each cluster by means of a word graph model. We propose a ranking strat- egy to select the best path in the con- structed graph as a query-based abstract sentence for each cluster. A resulting sum- mary consists of abstractive sentences rep- resenting the phrasal query information and the overall content of the conversa- tion. Automatic and manual evaluation results over meeting, chat and email con- versations show that our approach signifi- cantly outperforms baselines and previous extractive models. ",,,,ACL
116,2014,Comparing Multi-label Classification with Reinforcement Learning for Summarisation of Time-series Data,"Dimitra Gkatzia, Helen Hastie, Oliver Lemon","We present a novel approach for automatic report generation from time-series data, in the context of student feedback genera- tion. Our proposed methodology treats content selection as a multi-label (ML) classification problem, which takes as in- put time-series data and outputs a set of templates, while capturing the dependen- cies between selected templates. We show that this method generates output closer to the feedback that lecturers actually gener- ated, achieving 3.5% higher accuracy and 15% higher F-score than multiple simple classifiers that keep a history of selected templates. Furthermore, we compare a ML classifier with a Reinforcement Learn- ing (RL) approach in simulation and using ratings from real student users. We show that the different methods have different benefits, with ML being more accurate for predicting what was seen in the training data, whereas RL is more exploratory and slightly preferred by the students. ",,,,ACL
117,2014,Approximation Strategies for Multi-Structure Sentence Compression,Kapil Thadani,"Sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objec- tives but this typically requires expensive integer programming. We explore instead the use of Lagrangian relaxation to decou- ple the two subproblems and solve them separately. While dynamic programming is viable for bigram-based sentence com- pression, finding optimal compressed trees within graphs is NP-hard. We recover ap- proximate solutions to this problem us- ing LP relaxation and maximum spanning tree algorithms, yielding techniques that can be combined with the efficient bigram- based inference approach using Lagrange multipliers. Experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a sig- nificant improvement in runtime. ",,,,ACL
118,2014,Opinion Mining on YouTube,"Aliaksei Severyn, Alessandro Moschitti, Olga Uryupina, Barbara Plank",This paper defines a systematic approach to Opinion Mining (OM) on YouTube comments by (i) modeling classifiers for predicting the opinion polarity and the type of comment and (ii) proposing ro- bust shallow syntactic structures for im- proving model adaptability. We rely on the tree kernel technology to automatically ex- tract and learn features with better gener- alization power than bag-of-words. An ex- tensive empirical evaluation on our manu- ally annotated YouTube comments corpus shows a high classification accuracy and highlights the benefits of structural mod- els in a cross-domain setting. ,,,,ACL
119,2014,Automatic Keyphrase Extraction: A Survey of the State of the Art,"Kazi Saidul Hasan, Vincent Ng","While automatic keyphrase extraction has been examined extensively, state-of-the- art performance on this task is still much lower than that on many core natural lan- guage processing tasks. We present a sur- vey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. ",,,,ACL
120,2014,Pattern Dictionary of English Prepositions,Ken Litkowski,"We present a new lexical resource for the study of preposition behavior, the Pattern Dictionary of English Prepositions (PDEP). This dictionary, which follows principles laid out in Hanks’ theory of norms and exploit a- tions, is linked to 81,509 sentences for 304 prepositions, which have been made available under The Preposition Project (TPP). Nota- bly, 47,285 sentences, initially untagged, provide a representative sample of preposi- tion use, unlike the tagged sentences used in previous studies. Each sentence has been parsed with a dependency parser and our sys- tem has near-instantaneous access to features developed with this parser to explore and an- notate properties of individual senses. The features make extensive use of WordNet. We have extended feature exploration to include lookup of FrameNet lexical units and VerbNet classes for use in characterizing preposition behavior. We have designed our system to allow public access to any of the data available in the system. ",,,,ACL
121,2014,Looking at Unbalanced Specialized Comparable Corpora for Bilingual Lexicon Extraction,"Emmanuel Morin, Amir Hazem","The main work in bilingual lexicon ex- traction from comparable corpora is based on the implicit hypothesis that corpora are balanced. However, the historical context- based projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus. Within this context, we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments. Moreover, we have introduced a regression model that boosts the observations of word co- occurrences used in the context-based pro- jection method. Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons. ",,,,ACL
122,2014,Validating and Extending Semantic Knowledge Bases using Video Games with a Purpose,"Daniele Vannella, David Jurgens, Daniele Scarfini, Domenico Toscani","Large-scale knowledge bases are impor- tant assets in NLP. Frequently, such re- sources are constructed through automatic mergers of complementary resources, such as WordNet and Wikipedia. However, manually validating these resources is pro- hibitively expensive, even when using methods such as crowdsourcing. We pro- pose a cost-effective method of validat- ing and extending knowledge bases using video games with a purpose. Two video games were created to validate concept- concept and concept-image relations. In experiments comparing with crowdsourc- ing, we show that video game-based vali- dation consistently leads to higher-quality annotations, even when players are not compensated. ",,,,ACL
123,2014,Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring,"Suma Bhat, Huichao Xue, Su-Youn Yoon","Designing measures that capture various aspects of language ability is a central task in the design of systems for auto- matic scoring of spontaneous speech. In this study, we address a key aspect of lan- guage proficiency assessment – syntactic complexity. We propose a novel measure of syntactic complexity for spontaneous speech that shows optimum empirical per- formance on real world data in multiple ways. First, it is both robust and reliable, producing automatic scores that agree well with human rating compared to the state- of-the-art. Second, the measure makes sense theoretically, both from algorithmic and native language acquisition points of view. ",,,,ACL
124,2014,Can You Repeat That? Using Word Repetition to Improve Spoken Term Detection,"Jonathan Wintrode, Sanjeev Khudanpur","We aim to improve spoken term detec- tion performance by incorporating con- textual information beyond traditional N- gram language models. Instead of taking a broad view of topic context in spoken doc- uments, variability of word co-occurrence statistics across corpora leads us to fo- cus instead the on phenomenon of word repetition within single documents. We show that given the detection of one in- stance of a term we are more likely to find additional instances of that term in the same document. We leverage this bursti- ness of keywords by taking the most con- fident keyword hypothesis in each docu- ment and interpolating with lower scor- ing hits. We then develop a principled approach to select interpolation weights using only the ASR training data. Us- ing this re-weighting approach we demon- strate consistent improvement in the term detection performance across all five lan- guages in the BABEL program. ",,,,ACL
125,2014,Character-Level Chinese Dependency Parsing,"Meishan Zhang, Yue Zhang, Wanxiang Che, Ting Liu","Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words, enabling character- level analysis of Chinese syntactic struc- tures. In this paper, we investigate the problem of character-level Chinese depen- dency parsing, building dependency trees over characters. Character-level infor- mation can benefit downstream applica- tions by offering flexible granularities for word segmentation while improving word- level dependency parsing accuracies. We present novel adaptations of two ma- jor shift-reduce dependency parsing algo- rithms to character-level parsing. Exper- imental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. ",,,,ACL
126,2014,Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization,"Xuezhe Ma, Fei Xia","We present a novel approach for induc- ing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resource- rich language. We train probabilistic pars- ing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy reg- ularization. Our method can be used as a purely monolingual dependency parser, requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages. We perform experiments on three Data sets — Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages. We obtain state- of-the art performance of all the three data sets when compared with previously studied unsupervised and projected pars- ing systems. ",,,,ACL
127,2014,Unsupervised Morphology-Based Vocabulary Expansion,"Mohammad Sadegh Rasooli, Thomas Lippincott, Nizar Habash, Owen Rambow","We present a novel way of generating un- seen words, which is useful for certain ap- plications such as automatic speech recog- nition or optical character recognition in low-resource languages. We test our vo- cabulary generator on seven low-resource languages by measuring the decrease in out-of-vocabulary word rate on a held-out test set. The languages we study have very different morphological properties; we show how our results differ depend- ing on the morphological complexity of the language. In our best result (on As- samese), our approach can predict 29% of the token-based out-of-vocabulary with a small amount of unlabeled training data. ",,,,ACL
128,2014,Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints,"Xiaodong Zeng, Lidia S. Chao, Derek F. Wong, Isabel Trancoso","This study investigates on building a better Chinese word segmentation mod- el for statistical machine translation. It aims at leveraging word boundary infor- mation, automatically learned by bilin- gual character-based alignments, to induce a preferable segmentation model. We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRF- s model, trained by the treebank data (la- beled), on the bilingual data (unlabeled). The induced word boundary information is encoded as a graph propagation con- straint. The constrained model induction is accomplished by using posterior reg- ularization algorithm. The experiments on a Chinese-to-English machine transla- tion task reveal that the proposed model can bring positive segmentation effects to translation quality. ",,,,ACL
129,2014,Fast and Robust Neural Network Joint Models for Statistical Machine Translation,"Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar","Recent work has shown success in us- ing neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexi- calized and can be integrated into any MT decoder. We also present several varia- tions of the NNJM which provide signif- icant additive improvements. Although the model is quite simple, it yields strong empirical results. On the NIST OpenMT12 Arabic-English condi- tion, the NNJM features produce a gain of +3.0 BLEU on top of a powerful, feature- rich baseline which already includes a target-only NNLM. The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chi- ang’s (2007) original Hiero implementa- tion. Additionally, we describe two novel tech- niques for overcoming the historically high cost of using NNLM-style models in MT decoding. These techniques speed up NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM. This work was supported by DARPA/I2O Contract No. HR0011-12-C-0014 under the BOLT program (Approved for Public Release, Distribution Unlimited). The views, opin- ions, and/or findings contained in this article are those of the author and should not be interpreted as representing the of- ficial views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Depart- ment of Defense. ",,,,ACL
130,2014,Low-Rank Tensors for Scoring Dependency Structures,"Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay","Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich, high- dimensional feature representations. A small subset of such features is often se- lected manually. This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features. In this paper, we use tensors to map high-dimensional fea- ture vectors into low dimensional repre- sentations. We explicitly maintain the pa- rameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage mod- ularity in the tensor for easy training with online algorithms. Our parser consistently outperforms the Turbo and MST parsers across 14 different languages. We also ob- tain the best published UAS results on 5 languages. 1 ",,,,ACL
131,2014,CoSimRank: A Flexible & Efficient Graph-Theoretic Similarity Measure,"Sascha Rothe, Hinrich Schütze","We present CoSimRank, a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph. We present equivalent formalizations that show CoSimRank’s close relationship to Personalized Page- Rank and SimRank and also show how we can take advantage of fast matrix mul- tiplication algorithms to compute CoSim- Rank. Another advantage of CoSimRank is that it can be flexibly extended from ba- sic node-node similarity to several other graph-theoretic similarity measures. In an experimental evaluation on the tasks of synonym extraction and bilingual lexicon extraction, CoSimRank is faster or more accurate than previous approaches. ",,,,ACL
132,2014,Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world,"Angeliki Lazaridou, Elia Bruni, Marco Baroni","Following up on recent work on estab- lishing a mapping between vector-based semantic embeddings of words and the visual representations of the correspond- ing objects from natural images, we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning, in which an image of a previously unseen object is mapped to a linguistic representation denoting its word. We then introduce fast mapping, a challenging and more cognitively plausi- ble variant of the zero-shot task, in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts. By combining prior linguistic and visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence avail- able, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simula- tions of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts. ",,,,ACL
133,2014,Semantic Parsing via Paraphrasing,"Jonathan Berant, Percy Liang","A central challenge in semantic parsing is handling the myriad ways in which knowl- edge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this pa- per, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically gener- ate a set of candidate logical forms with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best para- phrases the input, and output the corre- sponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system P ARA S EMPRE improves state- of-the-art accuracies on two recently re- leased question-answering datasets. ",,,,ACL
134,2014,A Discriminative Graph-Based Parser for the Abstract Meaning Representation,"Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer","Meaning Representation (AMR) is a semantic formalism for which a grow- ing set of annotated examples is avail- able. We introduce the first approach to parse sentences into this representa- tion, providing a strong baseline for fu- ture improvement. The method is based on a novel algorithm for finding a maxi- mum spanning, connected subgraph, em- bedded within a Lagrangian relaxation of an optimization problem that imposes lin- guistically inspired constraints. Our ap- proach is described in the general frame- work of structured prediction, allowing fu- ture incorporation of additional features and constraints, and may extend to other formalisms as well. Our open-source sys- tem, JAMR, is available at: http://github.com/jflanigan/jamr ",,,,ACL
135,2014,Context-dependent Semantic Parsing for Time Expressions,"Kenton Lee, Yoav Artzi, Jesse Dodge, Luke Zettlemoyer","We present an approach for learning context-dependent semantic parsers to identify and interpret time expressions. We use a Combinatory Categorial Gram- mar to construct compositional meaning representations, while considering contex- tual cues, such as the document creation time and the tense of the governing verb, to compute the final time values. Exper- iments on benchmark datasets show that our approach outperforms previous state- of-the-art systems, with error reductions of 13% to 21% in end-to-end performance. ",,,,ACL
136,2014,Semantic Frame Identification with Distributed Word Representations,"Karl Moritz Hermann, Dipanjan Das, Jason Weston, Kuzman Ganchev","We present a novel technique for semantic frame identification using distributed rep- resentations of predicates and their syntac- tic context; this technique leverages auto- matic syntactic parses and a generic set of word embeddings. Given labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. The latter is used for se- mantic frame identification; with a stan- dard argument identification method in- spired by prior work, we achieve state-of- the-art results on FrameNet-style frame- semantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. ",,,,ACL
137,2014,A Sense-Based Translation Model for Statistical Machine Translation,"Deyi Xiong, Min Zhang","The sense in which a word is used deter- mines the translation of the word. In this paper, we propose a sense-based transla- tion model to integrate word senses into statistical machine translation. We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language. The pro- posed sense-based translation model en- ables the decoder to select appropriate translations for source words according to the inferred senses for these words us- ing maximum entropy classifiers. Our method is significantly different from pre- vious word sense disambiguation reformu- lated for machine translation in that the lat- ter neglects word senses in nature. We test the effectiveness of the proposed sense- based translation model on a large-scale Chinese-to-English translation task. Re- sults show that the proposed model sub- stantially outperforms not only the base- line but also the previous reformulated word sense disambiguation. ",,,,ACL
138,2014,Recurrent Neural Networks for Word Alignment Model,"Akihiro Tamura, Taro Watanabe, Eiichiro Sumita","This study proposes a word alignment model based on a recurrent neural net- work (RNN), in which an unlimited alignment history is represented by re- currently connected hidden layers. We perform unsupervised learning using noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which utilizes artificially generated negative samples. Our alignment model is directional, similar to the generative IBM models (Brown et al., 1993). To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that en- sures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable transla- tion performance to those baselines for Japanese-English and Chinese-English translation tasks. ",,,,ACL
139,2014,A Constrained Viterbi Relaxation for Bidirectional Word Alignment,"Yin-Wen Chang, Alexander M. Rush, John DeNero, Michael Collins","Bidirectional models of word alignment are an appealing alternative to post-hoc combinations of directional word align- ers. Unfortunately, most bidirectional formulations are NP-Hard to solve, and a previous attempt to use a relaxation- based decoder yielded few exact solu- tions (6%). We present a novel relax- ation for decoding the bidirectional model of DeNero and Macherey (2011). The relaxation can be solved with a mod- ified version of the Viterbi algorithm. To find optimal solutions on difficult instances, we alternate between incre- mentally adding constraints and applying optimality-preserving coarse-to-fine prun- ing. The algorithm finds provably ex- act solutions on 86% of sentence pairs and shows improvements over directional models. ",,,,ACL
140,2014,A Recursive Recurrent Neural Network for Statistical Machine Translation,"Shujie Liu, Nan Yang, Mu Li, Ming Zhou","In this paper, we propose a novel recursive recurrent neural network (R 2 NN) to mod- el the end-to-end decoding process for s- tatistical machine translation. R 2 NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neu- ral networks, so that language model and translation model can be integrated natu- rally; (2) a tree structure can be built, as recursive neural networks, so as to gener- ate the translation candidates in a bottom up manner. A semi-supervised training ap- proach is proposed to train the parameter- s, and the phrase pair embedding is ex- plored to model translation confidence di- rectly. Experiments on a Chinese to En- glish translation task show that our pro- posed R 2 NN can outperform the state- of-the-art baseline by about 1.5 points in BLEU. ",,,,ACL
141,2014,Predicting Instructor’s Intervention in MOOC forums,"Snigdha Chaturvedi, Dan Goldwasser, Hal Daumé III","Instructor intervention in student discus- sion forums is a vital component in Massive Open Online Courses (MOOCs), where personalized interaction is limited. This paper introduces the problem of pre- dicting instructor interventions in MOOC forums. We propose several prediction models designed to capture unique aspects of MOOCs, combining course informa- tion, forum structure and posts content. Our models abstract contents of individ- ual posts of threads using latent categories, learned jointly with the binary interven- tion prediction problem. Experiments over data from two Coursera MOOCs demon- strate that incorporating the structure of threads into the learning problem leads to better predictive performance. ",,,,ACL
142,2014,A Joint Graph Model for Pinyin-to-Chinese Conversion with Typo Correction,"Zhongye Jia, Hai Zhao","It is very import for Chinese language pro- cessing with the aid of an efficient input method engine (IME), of which pinyin- to-Chinese (PTC) conversion is the core part. Meanwhile, though typos are in- evitable during user pinyin inputting, ex- isting IMEs paid little attention to such big inconvenience. In this paper, motivated by a key equivalence of two decoding algo- rithms, we propose a joint graph model to globally optimize PTC and typo correction for IME. The evaluation results show that the proposed method outperforms both ex- isting academic and commercial IMEs. ",,,,ACL
143,2014,Smart Selection,"Patrick Pantel, Michael Gamon, Ariel Fuxman","Natural touch interfaces, common now in devices such as tablets and smartphones, make it cumbersome for users to select text. There is a need for a new text selec- tion paradigm that goes beyond the high acuity selection-by-mouse that we have re- lied on for decades. In this paper, we in- troduce such a paradigm, called Smart Se- lection, which aims to recover a user’s in- tended text selection from her touch input. We model the problem using an ensemble learning approach, which leverages mul- tiple linguistic analysis techniques com- bined with information from a knowledge base and a Web graph. We collect a dataset of true intended user selections and simu- lated user touches via a large-scale crowd- sourcing task, which we release to the academic community. We show that our model effectively addresses the smart se- lection task and significantly outperforms various baselines and standalone linguistic analysis techniques. ",,,,ACL
144,2014,Modeling Prompt Adherence in Student Essays,"Isaac Persing, Vincent Ng","Recently, researchers have begun explor- ing methods of scoring student essays with respect to particular dimensions of qual- ity such as coherence, technical errors, and prompt adherence. The work on modeling prompt adherence, however, has been focused mainly on whether individ- ual sentences adhere to the prompt. We present a new annotated corpus of essay- level prompt adherence scores and pro- pose a feature-rich approach to scoring es- says along the prompt adherence dimen- sion. Our approach significantly outper- forms a knowledge-lean baseline prompt adherence scoring system yielding im- provements of up to 16.6%. ",,,,ACL
145,2014,ConnotationWordNet: Learning Connotation over the Word+Sense Network,"Jun Seok Kang, Song Feng, Leman Akoglu, Yejin Choi","We introduce ConnotationWordNet, a con- notation lexicon over the network of words in conjunction with senses. We formulate the lexicon induction problem as collec- tive inference over pairwise-Markov Ran- dom Fields, and present a loopy belief propagation algorithm for inference. The key aspect of our method is that it is the first unified approach that assigns the polarity of both word- and sense-level connotations, exploiting the innate bipar- tite graph structure encoded in WordNet. We present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons. ",,,,ACL
146,2014,Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification,"Duyu Tang, Furu Wei, Nan Yang, Ming Zhou","We present a method that learns word em- bedding for Twitter sentiment classifica- tion in this paper. Most existing algorithm- s for learning continuous word represen- tations typically only model the syntactic context of words but ignore the sentimen- t of text. This is problematic for senti- ment analysis as they usually map word- s with similar syntactic context but oppo- site sentiment polarity, such as good and bad, to neighboring word vectors. We address this issue by learning sentiment- specific word embedding (SSWE), which encodes sentiment information in the con- tinuous representation of words. Specif- ically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sen- tences or tweets) in their loss function- s. To obtain large scale training corpora, we learn the sentiment-specific word em- bedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SS- WE to a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the perfor- mance is further improved by concatenat- ing SSWE with existing feature set. ",,,,ACL
147,2014,Towards a General Rule for Identifying Deceptive Opinion Spam,"Jiwei Li, Myle Ott, Claire Cardie, Eduard Hovy","Consumers’ purchase decisions are in- creasingly influenced by user-generated online reviews. Accordingly, there has been growing concern about the poten- tial for posting deceptive opinion spam— fictitious reviews that have been deliber- ately written to sound authentic, to de- ceive the reader. In this paper, we ex- plore generalized approaches for identify- ing online deceptive opinion spam based on a new gold standard dataset, which is comprised of data from three different do- mains (i.e. Hotel, Restaurant, Doctor), each of which contains three types of re- views, i.e. customer generated truthful re- views, Turker generated deceptive reviews and employee (domain-expert) generated deceptive reviews. Our approach tries to capture the general difference of language usage between deceptive and truthful re- views, which we hope will help customers when making purchase decisions and re- view portal operators, such as TripAdvisor or Yelp, investigate possible fraudulent ac- tivity on their sites. 1 ",,,,ACL
1,2015,On Using Very Large Target Vocabulary for Neural Machine Translation,"Sébastien Jean, Kyunghyun Cho, Roland Memisevic, Yoshua Bengio","Neural machine translation, a recently proposed approach to machine transla- tion based purely on neural networks, has shown promising results compared to the existing approaches such as phrase- based statistical machine translation. De- spite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complex- ity as well as decoding complexity in- crease proportionally to the number of tar- get words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vo- cabulary without increasing training com- plexity. We show that decoding can be efficiently done even with the model hav- ing a very large target vocabulary by se- lecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases out- perform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Fur- thermore, when we use an ensemble of a few models with very large target vo- cabularies, we achieve performance com- parable to the state of the art (measured by BLEU) on both the English→German and English→French translation tasks of WMT’14. ",,,,ACL
2,2015,Addressing the Rare Word Problem in Neural Machine Translation,"Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals","Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are com- parable to traditional approaches. A sig- nificant weakness in conventional NMT systems is their inability to correctly trans- late very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and im- plement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sen- tence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our exper- iments on the WMT’14 English to French translation task show that this method pro- vides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT sys- tem is the first to surpass the best result achieved on a WMT’14 contest task. ",,,,ACL
3,2015,Encoding Source Language with Convolutional Neural Network for Machine Translation,"Fandong Meng, Zhengdong Lu, Mingxuan Wang, Hang Li","The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target lan- guage model with a heuristically cho- sen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more sys- tematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With dif- ferent guiding signals during decod- ing, our specifically designed convolu- tion+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of en- tire source sentence to form a unified representation. This representation, to- gether with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English trans- lation tasks show that the proposed model can achieve significant improve- ments over the previous NNJM by up to +1.08 BLEU points on average. ",,,,ACL
4,2015,Statistical Machine Translation Features with Multitask Tensor Networks,"Hendra Setiawan, Zhongqiang Huang, Jacob Devlin, Thomas Lamar","We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various non- local translation phenomena. Second, we augment the architecture of the neural net- work with tensor layers that capture im- portant higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improve- ments that are complementary. The over- all improvement is +2.7 and +1.8 BLEU points for Arabic-English and Chinese- English translation over a state-of-the-art system that already includes neural net- work features. ",,,,ACL
5,2015,Describing Images using Inferred Visual Dependency Representations,"Desmond Elliott, Arjen de Vries","The Visual Dependency Representation (VDR) is an explicit model of the spa- tial relationships between objects in an im- age. In this paper we present an approach to training a VDR Parsing Model without the extensive human supervision used in previous work. Our approach is to find the objects mentioned in a given descrip- tion using a state-of-the-art object detec- tor, and to use successful detections to pro- duce training data. The description of an unseen image is produced by first predict- ing its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR. The performance of our approach is comparable to a state-of- the-art multimodal deep neural network in images depicting actions. ",,,,ACL
6,2015,Text to 3D Scene Generation with Rich Lexical Grounding,"Angel Chang, Will Monroe, Manolis Savva, Christopher Potts","The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, educa- tion, and robotics. However, prior work on the text to 3D scene generation task has used manually specified object cate- gories and language that identifies them. We introduce a dataset of 3D scenes an- notated with natural language descriptions and learn from this data how to ground tex- tual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method im- proves 3D scene generation over previ- ous work using purely rule-based meth- ods. We evaluate the fidelity and plau- sibility of 3D scenes generated with our grounding approach through human judg- ments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments. ",,,,ACL
7,2015,MultiGranCNN: An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity,"Wenpeng Yin, Hinrich Schütze","We present MultiGranCNN, a general deep learning architecture for matching text chunks. MultiGranCNN supports multigranular comparability of represen- tations: shorter sequences in one chunk can be directly compared to longer se- quences in the other chunk. Multi- GranCNN also contains a flexible and modularized match feature component that is easily adaptable to different types of chunk matching. We demonstrate state- of-the-art performance of MultiGranCNN on clause coherence and paraphrase iden- tification tasks. ",,,,ACL
8,2015,Weakly Supervised Models of Aspect-Sentiment for Online Course Discussion Forums,"Arti Ramesh, Shachi H. Kumar, James Foulds, Lise Getoor","Massive open online courses (MOOCs) are redefining the education system and transcending boundaries posed by tradi- tional courses. With the increase in pop- ularity of online courses, there is a cor- responding increase in the need to under- stand and interpret the communications of the course participants. Identifying top- ics or aspects of conversation and inferring sentiment in online course forum posts can enable instructor interventions to meet the needs of the students, rapidly address course-related issues, and increase student retention. Labeled aspect-sentiment data for MOOCs are expensive to obtain and may not be transferable between courses, suggesting the need for approaches that do not require labeled data. We develop a weakly supervised joint model for aspect- sentiment in online courses, modeling the dependencies between various aspects and sentiment using a recently developed scal- able class of statistical relational models called hinge-loss Markov random fields. We validate our models on posts sam- pled from twelve online courses, each con- taining an average of 10,000 posts, and demonstrate that jointly modeling aspect with sentiment improves the prediction ac- curacy for both aspect and sentiment. ",,,,ACL
9,2015,Semantically Smooth Knowledge Graph Embedding,"Shu Guo, Quan Wang, Bin Wang, Lihong Wang","This paper considers the problem of em- bedding Knowledge Graphs (KGs) con- sisting of entities and relations into low- dimensional vector spaces. Most of the existing methods perform this task based solely on observed facts. The only re- quirement is that the learned embeddings should be compatible within each individ- ual fact. In this paper, aiming at further discovering the intrinsic geometric struc- ture of the embedding space, we propose Semantically Smooth Embedding (SSE). The key idea of SSE is to take full ad- vantage of additional semantic informa- tion and enforce the embedding space to be semantically smooth, i.e., entities be- longing to the same semantic category will lie close to each other in the embedding s- pace. Two manifold learning algorithms Laplacian Eigenmaps and Locally Linear Embedding are used to model the smooth- ness assumption. Both are formulated as geometrically based regularization terms to constrain the embedding task. We em- pirically evaluate SSE in two benchmark tasks of link prediction and triple classi- fication, and achieve significant and con- sistent improvements over state-of-the-art methods. Furthermore, SSE is a general framework. The smoothness assumption can be imposed to a wide variety of em- bedding models, and it can also be con- structed using other information besides entities’ semantic categories. ",,,,ACL
10,2015,SensEmbed: Learning Sense Embeddings for Word and Relational Similarity,"Ignacio Iacobacci, Mohammad Taher Pilehvar, Roberto Navigli","Word embeddings have recently gained considerable popularity for modeling words in different Natural Language Processing (NLP) tasks including seman- tic similarity measurement. However, notwithstanding their success, word embeddings are by their very nature unable to capture polysemy, as different meanings of a word are conflated into a single representation. In addition, their learning process usually relies on massive corpora only, preventing them from taking advantage of structured knowledge. We address both issues by proposing a multi- faceted approach that transforms word embeddings to the sense level and lever- ages knowledge from a large semantic network for effective semantic similarity measurement. We evaluate our approach on word similarity and relational similar- ity frameworks, reporting state-of-the-art performance on multiple datasets. ",,,,ACL
11,2015,Revisiting Word Embedding for Contrasting Meaning,"Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen","Contrasting meaning is a basic aspect of semantics. Recent word-embedding mod- els based on distributional semantics hy- pothesis are known to be weak for mod- eling lexical contrast. We present in this paper the embedding models that achieve an F-score of 92% on the widely-used, publicly available dataset, the GRE “most contrasting word” questions (Mohammad et al., 2008). This is the highest perfor- mance seen so far on this dataset. Sur- prisingly at the first glance, unlike what was suggested in most previous work, where relatedness statistics learned from corpora is claimed to yield extra gains over lexicon-based models, we obtained our best result relying solely on lexical re- sources (Roget’s and WordNet)—corpora statistics did not lead to further improve- ment. However, this should not be sim- ply taken as that distributional statistics is not useful. We examine several basic con- cerns in modeling contrasting meaning to provide detailed analysis, with the aim to shed some light on the future directions for this basic semantics modeling problem. ",,,,ACL
12,2015,Joint Models of Disagreement and Stance in Online Debate,"Dhanya Sridhar, James Foulds, Bert Huang, Lise Getoor","Online debate forums present a valu- able opportunity for the understanding and modeling of dialogue. To understand these debates, a key challenge is inferring the stances of the participants, all of which are interrelated and dependent. While collectively modeling users’ stances has been shown to be effective (Walker et al., 2012c; Hasan and Ng, 2013), there are many modeling decisions whose ramifi- cations are not well understood. To in- vestigate these choices and their effects, we introduce a scalable unified probabilis- tic modeling framework for stance clas- sification models that 1) are collective, 2) reason about disagreement, and 3) can model stance at either the author level or at the post level. We comprehensively evaluate the possible modeling choices on eight topics across two online debate cor- pora, finding accuracy improvements of up to 11.5 percentage points over a local classifier. Our results highlight the im- portance of making the correct modeling choices for online dialogues, and having a unified probabilistic modeling framework that makes this possible. ",,,,ACL
13,2015,Low-Rank Regularization for Sparse Conjunctive Feature Spaces: An Application to Named Entity Classification,"Audi Primadhanty, Xavier Carreras, Ariadna Quattoni","Entity classification, like many other important problems in NLP, involves learning classifiers over sparse high- dimensional feature spaces that result from the conjunction of elementary fea- tures of the entity mention and its context. In this paper we develop a low-rank reg- ularization framework for training max- entropy models in such sparse conjunctive feature spaces. Our approach handles con- junctive feature spaces using matrices and induces an implicit low-dimensional rep- resentation via low-rank constraints. We show that when learning entity classifiers under minimal supervision, using a seed set, our approach is more effective in con- trolling model capacity than standard tech- niques for linear classifiers. ",,,,ACL
14,2015,Learning Word Representations by Jointly Modeling Syntagmatic and Paradigmatic Relations,"Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu","Vector space representation of words has been widely used to capture fine-grained linguistic regularities, and proven to be successful in various natural language pro- cessing tasks in recent years. However, existing models for learning word repre- sentations focus on either syntagmatic or paradigmatic relations alone. In this pa- per, we argue that it is beneficial to jointly modeling both relations so that we can not only encode different types of linguistic properties in a unified way, but also boost the representation learning due to the mu- tual enhancement between these two types of relations. We propose two novel dis- tributional models for word representation using both syntagmatic and paradigmatic relations via a joint training objective. The proposed models are trained on a public Wikipedia corpus, and the learned rep- resentations are evaluated on word anal- ogy and word similarity tasks. The re- sults demonstrate that the proposed mod- els can perform significantly better than all the state-of-the-art baseline methods on both tasks. ",,,,ACL
15,2015,Learning Dynamic Feature Selection for Fast Sequential Prediction,"Emma Strubell, Luke Vilnis, Kate Silverstein, Andrew McCallum","We present paired learning and inference algorithms for significantly reducing com- putation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are or- dered such that high confidence can of- ten be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. Our approach is simpler and better suited to NLP than other related cascade methods. We present experiments in left-to-right part-of-speech tagging, named entity recognition, and transition-based dependency parsing. On the typical benchmarking datasets we can preserve POS tagging accuracy above 97% and parsing LAS above 88.5% both with over a five-fold reduction in run-time, and NER F1 above 88 with more than 2x in- crease in speed. ",,,,ACL
16,2015,Compositional Vector Space Models for Knowledge Base Completion,"Arvind Neelakantan, Benjamin Roth, Andrew McCallum","Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by infer- ring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop re- lational path treated as an atomic feature, like bornIn(X,Z) → containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implica- tions of a path using a recurrent neural network (RNN) that takes as inputs vec- tor embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assem- ble a new dataset of over 52M relational triples, and show that our method im- proves over a traditional classifier by 11%, and a method leveraging pre-trained em- beddings by 7%. ",,,,ACL
17,2015,Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks,"Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng","Traditional approaches to the task of ACE event extraction primarily rely on elabo- rately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack gener- alization, take a large amount of human effort and are prone to error propaga- tion and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically ex- tract lexical-level and sentence-level fea- tures without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic reg- ularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neu- ral network (DMCNN), which uses a dy- namic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimen- tal results show that our approach signif- icantly outperforms other state-of-the-art methods. ",,,,ACL
18,2015,Stacked Ensembles of Information Extractors for Knowledge-Base Population,"Vidhoon Viswanathan, Nazneen Fatema Rajani, Yinon Bentor, Raymond Mooney","We present results on using stacking to en- semble multiple systems for the Knowl- edge Base Population English Slot Fill- ing (KBP-ESF) task. In addition to us- ing the output and confidence of each sys- tem as input to the stacked classifier, we also use features capturing how well the systems agree about the provenance of the information they extract. We demon- strate that our stacking approach outper- forms the best system from the 2014 KBP- ESF competition as well as alternative en- sembling methods employed in the 2014 KBP Slot Filler Validation task and several other ensembling baselines. Additionally, we demonstrate that including provenance information further increases the perfor- mance of stacking. ",,,,ACL
19,2015,Generative Event Schema Induction with Entity Disambiguation,"Kiem-Hieu Nguyen, Xavier Tannier, Olivier Ferret, Romaric Besançon","This paper presents a generative model to event schema induction. Previous meth- ods in the literature only use head words to represent entities. However, elements other than head words contain useful in- formation. For instance, an armed man is more discriminative than man. Our model takes into account this information and precisely represents it using proba- bilistic topic distributions. We illustrate that such information plays an important role in parameter estimation. Mostly, it makes topic distributions more coherent and more discriminative. Experimental results on benchmark dataset empirically confirm this enhancement. ",,,,ACL
20,2015,Syntax-based Simultaneous Translation through Prediction of Unseen Syntactic Constituents,"Yusuke Oda, Graham Neubig, Sakriani Sakti, Tomoki Toda","Simultaneous translation is a method to reduce the latency of communication through machine translation (MT) by di- viding the input into short segments be- fore performing translation. However, short segments pose problems for syntax- based translation methods, as it is diffi- cult to generate accurate parse trees for sub-sentential segments. In this paper, we perform the first experiments applying syntax-based SMT to simultaneous trans- lation, and propose two methods to pre- vent degradations in accuracy: a method to predict unseen syntactic constituents that help generate complete parse trees, and a method that waits for more input when the current utterance is not enough to gener- ate a fluent translation. Experiments on English-Japanese translation show that the proposed methods allow for improvements in accuracy, particularly with regards to word order of the target sentences. ",,,,ACL
21,2015,Efficient Top-Down BTG Parsing for Machine Translation Preordering,Tetsuji Nakagawa,"We present an efficient incremental top- down parsing method for preordering based on Bracketing Transduction Gram- mar (BTG). The BTG-based preordering framework (Neubig et al., 2012) can be applied to any language using only par- allel text, but has the problem of compu- tational efficiency. Our top-down parsing algorithm allows us to use the early up- date technique easily for the latent vari- able structured Perceptron algorithm with beam search, and solves the problem. Experimental results showed that the top- down method is more than 10 times faster than a method using the CYK algorithm. A phrase-based machine translation sys- tem with the top-down method had statis- tically significantly higher BLEU scores for 7 language pairs without relying on supervised syntactic parsers, compared to baseline systems using existing preorder- ing methods. ",,,,ACL
22,2015,Online Multitask Learning for Machine Translation Quality Estimation,"José G. C. de Souza, Matteo Negri, Elisa Ricci, Marco Turchi","We present a method for predicting ma- chine translation output quality geared to the needs of computer-assisted translation. These include the capability to: i) con- tinuously learn and self-adapt to a stream of data coming from multiple translation jobs, ii) react to data diversity by ex- ploiting human feedback, and iii) leverage data similarity by learning and transferring knowledge across domains. To achieve these goals, we combine two supervised machine learning paradigms, online and multitask learning, adapting and unifying them in a single framework. We show the effectiveness of our approach in a re- gression task (HTER prediction), in which online multitask learning outperforms the competitive online single-task and pooling methods used for comparison. This in- dicates the feasibility of integrating in a CAT tool a single QE component capa- ble to simultaneously serve (and continu- ously learn from) multiple translation jobs involving different domains and users. ",,,,ACL
23,2015,A Context-Aware Topic Model for Statistical Machine Translation,"Jinsong Su, Deyi Xiong, Yang Liu, Xianpei Han","Lexical selection is crucial for statistical ma- chine translation. Previous studies separately exploit sentence-level contexts and document- level topics for lexical selection, neglecting their correlations. In this paper, we propose a context-aware topic model for lexical selec- tion, which not only models local contexts and global topics but also captures their correla- tions. The model uses target-side translations as hidden variables to connect document top- ics and source-side local contextual words. In order to learn hidden variables and distribu- tions from data, we introduce a Gibbs sam- pling algorithm for statistical estimation and inference. A new translation probability based on distributions learned by the model is inte- grated into a translation system for lexical se- lection. Experiment results on NIST Chinese- English test sets demonstrate that 1) our model significantly outperforms previous lexical se- lection methods and 2) modeling correlations between local words and global topics can fur- ther improve translation quality. ",,,,ACL
24,2015,Learning Answer-Entailing Structures for Machine Comprehension,"Mrinmaya Sachan, Kumar Dubey, Eric Xing, Matthew Richardson","Understanding open-domain text is one of the primary challenges in NLP. Ma- chine comprehension evaluates the sys- tem’s ability to understand text through a series of question-answering tasks on short pieces of text such that the correct answer can be found only in the given text. For this task, we posit that there is a hid- den (latent) structure that explains the rela- tion between the question, correct answer, and text. We call this the answer-entailing structure; given the structure, the correct- ness of the answer is evident. Since the structure is latent, it must be inferred. We present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs), and uses what it learns to answer machine comprehension questions on novel texts. We extend this framework to incorporate multi-task learning on the different sub- tasks that are required to perform machine comprehension. Evaluation on a publicly available dataset shows that our frame- work outperforms various IR and neural- network baselines, achieving an overall accuracy of 67.8% (vs. 59.9%, the best previously-published result.) ",,,,ACL
25,2015,Learning Continuous Word Embedding with Metadata for Question Retrieval in Community Question Answering,"Guangyou Zhou, Tingting He, Jun Zhao, Po Hu","Community question answering (cQA) has become an important issue due to the popularity of c QA archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in c QA archives aims to find the exist- ing questions that are semantically equiv- alent or relevant to the queried questions. However, the lexical gap problem brings about new challenge for question retrieval in cQA. In this paper, we propose to learn continuous word embeddings with meta- data of category information within cQA pages for question retrieval. To deal with the variable size of word embedding vec- tors, we employ the framework of fisher kernel to aggregated them into the fixed- length vectors. Experimental results on large-scale real world cQA data set show that our approach can significantly out- perform state-of-the-art translation models and topic-based models for question re- trieval in cQA. ",,,,ACL
26,2015,Question Answering over Freebase with Multi-Column Convolutional Neural Networks,"Li Dong, Furu Wei, Ming Zhou, Ke Xu","Answering natural language questions over a knowledge base is an important and challenging task. Most of existing sys- tems typically rely on hand-crafted fea- tures and rules to conduct question under- standing and/or answer ranking. In this pa- per, we introduce multi-column convolu- tional neural networks (MCCNNs) to un- derstand questions from three different as- pects (namely, answer path, answer con- text, and answer type) and learn their dis- tributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use F REEBASE as the knowledge base and conduct exten- sive experiments on the W EB Q UESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline sys- tems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn. ",,,,ACL
27,2015,Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning,"Angeliki Lazaridou, Georgiana Dinu, Marco Baroni","Zero-shot methods in language, vision and other domains rely on a cross-space map- ping function that projects vectors from the relevant feature space (e.g., visual- feature-based image representations) to a large semantic word space (induced in an unsupervised way from corpus data), where the entities of interest (e.g., objects images depict) are labeled with the words associated to the nearest neighbours of the mapped vectors. Zero-shot cross-space mapping methods hold great promise as a way to scale up annotation tasks well be- yond the labels in the training data (e.g., recognizing objects that were never seen in training). However, the current perfor- mance of cross-space mapping functions is still quite low, so that the strategy is not yet usable in practical applications. In this paper, we explore some general properties, both theoretical and empirical, of the cross-space mapping function, and we build on them to propose better meth- ods to estimate it. In this way, we attain large improvements over the state of the art, both in cross-linguistic (word trans- lation) and cross-modal (image labeling) zero-shot experiments. ",,,,ACL
28,2015,A Generalisation of Lexical Functions for Composition in Distributional Semantics,"Antoine Bride, Tim Van de Cruys, Nicholas Asher","Over the last two decades, numerous algo- rithms have been developed that success- fully capture something of the semantics of single words by looking at their distri- bution in text and comparing these distri- butions in a vector space model. How- ever, it is not straightforward to construct meaning representations beyond the level of individual words – i.e. the combina- tion of words into larger units – using dis- tributional methods. Our contribution is twofold. First of all, we carry out a large- scale evaluation, comparing different com- position methods within the distributional framework for the cases of both adjective- noun and noun-noun composition, making use of a newly developed dataset. Sec- ondly, we propose a novel method for composition, which generalises the ap- proach by Baroni and Zamparelli (2010). The performance of our novel method is also evaluated on our new dataset and proves competitive with the best methods. ",,,,ACL
29,2015,Simple Learning and Compositional Application of Perceptually Grounded Word Meanings for Incremental Reference Resolution,"Casey Kennington, David Schlangen","An elementary way of using language is to refer to objects. Often, these objects are physically present in the shared envi- ronment and reference is done via men- tion of perceivable properties of the ob- jects. This is a type of language use that is modelled well neither by logical semantics nor by distributional semantics, the former focusing on inferential relations between expressed propositions, the latter on simi- larity relations between words or phrases. We present an account of word and phrase meaning that is perceptually grounded, trainable, compositional, and ‘dialogue- plausible’ in that it computes meanings word-by-word. We show that the approach performs well (with an accuracy of 65% on a 1-out-of-32 reference resolution task) on direct descriptions and target/landmark descriptions, even when trained with less than 800 training examples and automati- cally transcribed utterances. ",,,,ACL
30,2015,Neural CRF Parsing,"Greg Durrett, Dan Klein","This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear fea- turization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but in- stead of linear potential functions based on sparse features, we use nonlinear po- tentials computed via a feedforward neu- ral network. Because potentials are still local to anchored rules, structured infer- ence (CKY) is unchanged from the sparse case. Computing gradients during learn- ing involves backpropagating an error sig- nal formed from standard CRF sufficient statistics (expected rule counts). Us- ing only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system 1 achieves 91.1 F 1 on section 23 of the Penn Tree- bank, and more generally outperforms the best prior single parser results on a range of languages. ",,,,ACL
31,2015,An Effective Neural Network Model for Graph-based Dependency Parsing,"Wenzhe Pei, Tao Ge, Baobao Chang","Most existing graph-based parsing models rely on millions of hand-crafted features, which limits their generalization ability and slows down the parsing speed. In this paper, we propose a general and effective Neural Network model for graph-based dependency parsing. Our model can auto- matically learn high-order feature combi- nations using only atomic features by ex- ploiting a novel activation function tanh- cube. Moreover, we propose a simple yet effective way to utilize phrase-level infor- mation that is expensive to use in conven- tional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. ",,,,ACL
32,2015,Structured Training for Neural Network Transition-Based Parsing,"David Weiss, Chris Alberti, Michael Collins, Slav Petrov","We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automat- ically parsed sentences. Given this fixed network representation, we learn a final layer using the struc- tured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% un- labeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in- depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy. ",,,,ACL
33,2015,Transition-Based Dependency Parsing with Stack Long Short-Term Memory,"Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews","We propose a technique for learning rep- resentations of parser states in transition- based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks— the stack LSTM. Like the conventional stack data structures used in transition- based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formu- late an efficient parsing model that cap- tures three facets of a parser’s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their inter- nal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance. ",,,,ACL
34,2015,Leveraging Linguistic Structure For Open Domain Information Extraction,"Gabor Angeli, Melvin Jose Johnson Premkumar, Christopher D. Manning","Relation triples produced by open domain information extraction (open IE) systems are useful for question answering, infer- ence, and other IE tasks. Traditionally these are extracted using a large set of pat- terns; however, this approach is brittle on out-of-domain text and long-range depen- dencies, and gives no insight into the sub- structure of the arguments. We replace this large pattern set with a few patterns for canonically structured sentences, and shift the focus to a classifier which learns to extract self-contained clauses from longer sentences. We then run natural logic infer- ence over these short clauses to determine the maximally specific arguments for each candidate triple. We show that our ap- proach outperforms a state-of-the-art open IE system on the end-to-end TAC-KBP 2013 Slot Filling task. ",,,,ACL
35,2015,Joint Information Extraction and Reasoning: A Scalable Statistical Relational Learning Approach,"William Yang Wang, William W. Cohen","A standard pipeline for statistical rela- tional learning involves two steps: one first constructs the knowledge base (KB) from text, and then performs the learn- ing and reasoning tasks using probabilis- tic first-order logics. However, a key is- sue is that information extraction (IE) er- rors from text affect the quality of the KB, and propagate to the reasoning task. In this paper, we propose a statistical rela- tional learning model for joint information extraction and reasoning. More specifi- cally, we incorporate context-based entity extraction with structure learning (SL) in a scalable probabilistic logic framework. We then propose a latent context inven- tion (LCI) approach to improve the per- formance. In experiments, we show that our approach outperforms state-of-the-art baselines over three real-world Wikipedia datasets from multiple domains; that joint learning and inference for IE and SL sig- nificantly improve both tasks; that latent context invention further improves the re- sults. ",,,,ACL
36,2015,A Knowledge-Intensive Model for Prepositional Phrase Attachment,"Ndapandula Nakashole, Tom M. Mitchell","Prepositional phrases (PPs) express cru- cial information that knowledge base con- struction methods need to extract. How- ever, PPs are a major source of syntactic ambiguity and still pose problems in pars- ing. We present a method for resolving ambiguities arising from PPs, making ex- tensive use of semantic knowledge from various resources. As training data, we use both labeled and unlabeled data, utilizing an expectation maximization algorithm for parameter estimation. Experiments show that our method yields improvements over existing methods including a state of the art dependency parser. ",,,,ACL
37,2015,A Convolution Kernel Approach to Identifying Comparisons in Text,"Maksim Tkachenko, Hady Lauw","Comparisons in text, such as in online re- views, serve as useful decision aids. In this paper, we focus on the task of iden- tifying whether a comparison exists be- tween a specific pair of entity mentions in a sentence. This formulation is trans- formative, as previous work only seeks to determine whether a sentence is com- parative, which is presumptuous in the event the sentence mentions multiple en- tities and is comparing only some, not all, of them. Our approach leverages not only lexical features such as salient words, but also structural features expressing the re- lationships among words and entity men- tions. To model these features seamlessly, we rely on a dependency tree representa- tion, and investigate the applicability of a series of tree kernels. This leads to the de- velopment of a new context-sensitive tree kernel: Skip-node Kernel (SNK). We fur- ther describe both its exact and approxi- mate computations. Through experiments on real-life datasets, we evaluate the effec- tiveness of our kernel-based approach for comparison identification, as well as the utility of SNK and its approximations. ",,,,ACL
38,2015,It Depends: Dependency Parser Comparison Using A Web-based Evaluation Tool,"Jinho D. Choi, Joel Tetreault, Amanda Stent","The last few years have seen a surge in the number of accurate, fast, publicly avail- able dependency parsers. At the same time, the use of dependency parsing in NLP applications has increased. It can be difficult for a non-expert to select a good “off-the-shelf” parser. We present a com- parative analysis of ten leading statistical dependency parsers on a multi-genre cor- pus of English. For our analysis, we de- veloped a new web-based tool that gives a convenient way of comparing depen- dency parser outputs. Our analysis will help practitioners choose a parser to op- timize their desired speed/accuracy trade- off, and our tool will help practitioners ex- amine and compare parser output. ",,,,ACL
39,2015,Generating High Quality Proposition Banks for Multilingual Semantic Role Labeling,"Alan Akbik, Laura Chiticariu, Marina Danilevsky, Yunyao Li","Semantic role labeling (SRL) is crucial to natural language understanding as it identi- fies the predicate-argument structure in text with semantic labels. Unfortunately, re- sources required to construct SRL models are expensive to obtain and simply do not exist for most languages. In this paper, we present a two-stage method to enable the construction of SRL models for resource- poor languages by exploiting monolingual SRL and multilingual parallel data. Exper- imental results show that our method out- performs existing methods. We use our method to generate Proposition Banks with high to reasonable quality for 7 languages in three language families and release these resources to the research community. ",,,,ACL
40,2015,Aligning Opinions: Cross-Lingual Opinion Mining with Dependencies,"Mariana S. C. Almeida, Cláudia Pinto, Helena Figueira, Pedro Mendes","We propose a cross-lingual framework for fine-grained opinion mining using bitext projection. The only requirements are a running system in a source language and word-aligned parallel data. Our method projects opinion frames from the source to the target language, and then trains a sys- tem on the target language using the auto- matic annotations. Key to our approach is a novel dependency-based model for opin- ion mining, which we show, as a byprod- uct, to be on par with the current state of the art for English, while avoiding the need for integer programming or rerank- ing. In cross-lingual mode (English to Por- tuguese), our approach compares favor- ably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. ",,,,ACL
41,2015,Learning to Adapt Credible Knowledge in Cross-lingual Sentiment Analysis,"Qiang Chen, Wenjie Li, Yu Lei, Xule Liu","Cross-lingual sentiment analysis is a task of identifying sentiment polarities of texts in a low-resource language by using sen- timent knowledge in a resource-abundant language. While most existing approaches are driven by transfer learning, their performance does not reach to a promising level due to the transferred errors. In this paper, we propose to integrate into knowl- edge transfer a knowledge validation mod- el, which aims to prevent the negative influence from the wrong knowledge by distinguishing highly credible knowledge. Experiment results demonstrate the neces- sity and effectiveness of the model. ",,,,ACL
42,2015,Learning Bilingual Sentiment Word Embeddings for Cross-language Sentiment Classification,"HuiWei Zhou, Long Chen, Fulin Shi, Degen Huang","The sentiment classification performance relies on high-quality sentiment resources. However, these resources are imbalanced in different languages. Cross-language sentiment classification (CLSC) can lever- age the rich resources in one language (source language) for sentiment classifica- tion in a resource-scarce language (target language). Bilingual embeddings could eliminate the semantic gap between two languages for CLSC, but ignore the senti- ment information of text. This paper pro- poses an approach to learning bilingual sentiment word embeddings (BSWE) for English-Chinese CLSC. The proposed B- SWE incorporate sentiment information of text into bilingual embeddings. Further- more, we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on large- scale parallel corpora. Experiments on NLP&CC 2013 CLSC dataset show that our approach outperforms the state-of-the- art systems. ",,,,ACL
43,2015,Content Models for Survey Generation: A Factoid-Based Evaluation,"Rahul Jha, Catherine Finegan-Dollak, Ben King, Reed Coke","We present a new factoid-annotated dataset for evaluating content models for scientific survey article generation containing 3,425 sentences from 7 topics in natural language processing. We also introduce a novel HITS-based content model for automated survey article gen- eration called H IT S UM that exploits the lexical network structure between sen- tences from citing and cited papers. Using the factoid-annotated data, we conduct a pyramid evaluation and compare H IT S UM with two previous state-of-the-art content models: C-Lexrank, a network based con- tent model, and T OPIC S UM , a Bayesian content model. Our experiments show that our new content model captures useful survey-worthy information and outper- forms C-Lexrank by 4% and T OPIC S UM by 7% in pyramid evaluation. ",,,,ACL
44,2015,Training a Natural Language Generator From Unaligned Data,"Ondřej Dušek, Filip Jurčíček","We present a novel syntax-based natural language generation system that is train- able from unaligned pairs of input mean- ing representations and output sentences. It is divided into sentence planning, which incrementally builds deep-syntactic de- pendency trees, and surface realization. Sentence planner is based on A* search with a perceptron ranker that uses novel differing subtree updates and a simple fu- ture promise estimation; surface realiza- tion uses a rule-based pipeline from the Treex NLP toolkit. Our first results show that training from unaligned data is feasible, the outputs of our generator are mostly fluent and rele- vant. ",,,,ACL
45,2015,Event-Driven Headline Generation,"Rui Sun, Yue Zhang, Meishan Zhang, Donghong Ji","We propose an event-driven model for headline generation. Given an input document, the system identifies a key event chain by extracting a set of structural events that describe them. Then a novel multi-sentence compression algorithm is used to fuse the extracted events, generating a headline for the document. Our model can be viewed as a novel combination of extractive and abstractive headline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-the-art systems. ",,,,ACL
46,2015,New Transfer Learning Techniques for Disparate Label Sets,"Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, Minwoo Jeong","In natural language understanding (NLU), a user utterance can be labeled differently depending on the domain or application (e.g., weather vs. calendar). Standard domain adaptation techniques are not di- rectly applicable to take advantage of the existing annotations because they assume that the label set is invariant. We propose a solution based on label embeddings in- duced from canonical correlation analysis (CCA) that reduces the problem to a stan- dard domain adaptation task and allows use of a number of transfer learning tech- niques. We also introduce a new trans- fer learning technique based on pretrain- ing of hidden-unit CRFs (HUCRFs). We perform extensive experiments on slot tag- ging on eight personal digital assistant do- mains and demonstrate that the proposed methods are superior to strong baselines. ",,,,ACL
47,2015,Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding,"Yun-Nung Chen, William Yang Wang, Anatole Gershman, Alexander Rudnicky","Spoken dialogue systems (SDS) typically require a predefined semantic ontology to train a spoken language understanding (SLU) module. In addition to the anno- tation cost, a key challenge for design- ing such an ontology is to define a coher- ent slot set while considering their com- plex relations. This paper introduces a novel matrix factorization (MF) approach to learn latent feature vectors for utter- ances and semantic elements without the need of corpus annotations. Specifically, our model learns the semantic slots for a domain-specific SDS in an unsupervised fashion, and carries out semantic pars- ing using latent MF techniques. To fur- ther consider the global semantic struc- ture, such as inter-word and inter-slot re- lations, we augment the latent MF-based model with a knowledge graph propaga- tion model based on a slot-based seman- tic graph and a word-based lexical graph. Our experiments show that the proposed MF approaches produce better SLU mod- els that are able to predict semantic slots and word patterns taking into account their relations and domain-specificity in a joint manner. ",,,,ACL
48,2015,Efficient Disfluency Detection with Transition-based Parsing,"Shuangzhi Wu, Dongdong Zhang, Ming Zhou, Tiejun Zhao","Automatic speech recognition (ASR) out- puts often contain various disfluencies. It is necessary to remove these disfluencies before processing downstream tasks. In this paper, an efficient disfluency detection approach based on right-to-left transition- based parsing is proposed, which can effi- ciently identify disfluencies and keep ASR outputs grammatical. Our method exploits a global view to capture long-range de- pendencies for disfluency detection by in- tegrating a rich set of syntactic and dis- fluency features with linear complexity. The experimental results show that our method outperforms state-of-the-art work and achieves a 85.1% f-score on the com- monly used English Switchboard test set. We also apply our method to in-house an- notated Chinese data and achieve a sig- nificantly higher f-score compared to the baseline of CRF-based approach. ",,,,ACL
49,2015,S-MART: Novel Tree-based Structured Learning Algorithms Applied to Tweet Entity Linking,"Yi Yang, Ming-Wei Chang","Non-linear models recently receive a lot of attention as people are starting to dis- cover the power of statistical and em- bedding features. However, tree-based models are seldom studied in the con- text of structured learning despite their re- cent success on various classification and ranking tasks. In this paper, we propose S- MART , a tree-based structured learning framework based on multiple additive re- gression trees. S- MART is especially suit- able for handling tasks with dense fea- tures, and can be used to learn many dif- ferent structures under various loss func- tions. We apply S- MART to the task of tweet entity linking — a core component of tweet information extraction, which aims to identify and link name mentions to en- tities in a knowledge base. A novel infer- ence algorithm is proposed to handle the special structure of the task. The exper- imental results show that S- MART signif- icantly outperforms state-of-the-art tweet entity linking systems. ",,,,ACL
50,2015,Entity Retrieval via Entity Factoid Hierarchy,"Chunliang Lu, Wai Lam, Yi Liao","We propose that entity queries are gener- ated via a two-step process: users first se- lect entity facts that can distinguish tar- get entities from the others; and then choose words to describe each selected fact. Based on this query generation paradigm, we propose a new entity rep- resentation model named as entity fac- toid hierarchy. An entity factoid hierar- chy is a tree structure composed of fac- toid nodes. A factoid node describes one or more facts about the entity in different information granularities. The entity fac- toid hierarchy is constructed via a factor graph model, and the inference on the fac- tor graph is achieved by a modified variant of Multiple-try Metropolis algorithm. En- tity retrieval is performed by decompos- ing entity queries and computing the query likelihood on the entity factoid hierarchy. Using an array of benchmark datasets, we demonstrate that our proposed framework significantly improves the retrieval perfor- mance over existing models. ",,,,ACL
51,2015,Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment,"Muyu Zhang, Bing Qin, Mao Zheng, Graeme Hirst","Document enrichment focuses on retriev- ing relevant knowledge from external re- sources, which is essential because text is generally replete with gaps. Since conven- tional work primarily relies on special re- sources, we instead use triples of Subject, Predicate, Object as knowledge and in- corporate distributional semantics to rank them. Our model first extracts these triples automatically from raw text and converts them into real-valued vectors based on the word semantics captured by Latent Dirich- let Allocation. We then represent these triples, together with the source document that is to be enriched, as a graph of triples, and adopt a global iterative algorithm to propagate relevance weight from source document to these triples so as to select the most relevant ones. Evaluated as a rank- ing problem, our model significantly out- performs multiple strong baselines. More- over, we conduct a task-based evaluation by incorporating these triples as additional features into document classification and enhances the performance by 3.02%. ",,,,ACL
52,2015,A Strategic Reasoning Model for Generating Alternative Answers,"Jon Stevens, Anton Benz, Sebastian Reuße, Ralf Klabunde","We characterize a class of indirect an- swers to yes/no questions, alternative an- swers, where information is given that is not directly asked about, but which might nonetheless address the underlying moti- vation for the question. We develop a model rooted in game theory that gener- ates these answers via strategic reasoning about possible unobserved domain-level user requirements. We implement the model within an interactive question an- swering system simulating real estate dia- logue. The system learns a prior probabil- ity distribution over possible user require- ments by analyzing training dialogues, which it uses to make strategic deci- sions about answer selection. The system generates pragmatically natural and inter- pretable answers which make for more ef- ficient interactions compared to a baseline. ",,,,ACL
53,2015,Modeling Argument Strength in Student Essays,"Isaac Persing, Vincent Ng","While recent years have seen a surge of in- terest in automated essay grading, includ- ing work on grading essays with respect to particular dimensions such as prompt adherence, coherence, and technical qual- ity, there has been relatively little work on grading the essay dimension of argu- ment strength, which is arguably the most important aspect of argumentative essays. We introduce a new corpus of argumen- tative student essays annotated with argu- ment strength scores and propose a su- pervised, feature-rich approach to auto- matically scoring the essays along this dimension. Our approach significantly outperforms a baseline that relies solely on heuristically applied sentence argument function labels by up to 16.1%. ",,,,ACL
54,2015,Summarization of Multi-Document Topic Hierarchies using Submodular Mixtures,"Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan, Jeff Bilmes","We study the problem of summarizing DAG-structured topic hierarchies over a given set of documents. Example appli- cations include automatically generating Wikipedia disambiguation pages for a set of articles, and generating candidate multi-labels for preparing machine learn- ing datasets (e.g., for text classification, functional genomics, and image classi- fication). Unlike previous work, which focuses on clustering the set of documents using the topic hierarchy as features, we directly pose the problem as a submodular optimization problem on a topic hierarchy using the documents as features. Desirable properties of the chosen topics include document coverage, specificity, topic diversity, and topic homogeneity, each of which, we show, is naturally modeled by a submodular function. Other information, provided say by unsupervised approaches such as LDA and its variants, can also be utilized by defining a submodular function that expresses coherence between the chosen topics and this information. We use a large-margin framework to learn convex mixtures over the set of submodular components. We empirically evaluate our method on the problem of automatically generating Wikipedia disambiguation pages using human generated clusterings as ground truth. We find that our frame- work improves upon several baselines according to a variety of standard evalua- tion metrics including the Jaccard Index, F1 score and NMI, and moreover, can be scaled to extremely large scale problems. ",,,,ACL
55,2015,Learning to Explain Entity Relationships in Knowledge Graphs,"Nikos Voskarides, Edgar Meij, Manos Tsagkias, Maarten de Rijke","We study the problem of explaining re- lationships between pairs of knowledge graph entities with human-readable de- scriptions. Our method extracts and en- riches sentences that refer to an entity pair from a corpus and ranks the sentences ac- cording to how well they describe the re- lationship between the entities. We model this task as a learning to rank problem for sentences and employ a rich set of fea- tures. When evaluated on a large set of manually annotated sentences, we find that our method significantly improves over state-of-the-art baseline models. ",,,,ACL
56,2015,Bring you to the past: Automatic Generation of Topically Relevant Event Chronicles,"Tao Ge, Wenzhe Pei, Heng Ji, Sujian Li","An event chronicle provides people with an easy and fast access to learn the past. In this paper, we propose the first novel approach to automatically generate a top- ically relevant event chronicle during a certain period given a reference chronicle during another period. Our approach con- sists of two core components – a time- aware hierarchical Bayesian model for event detection, and a learning-to-rank model to select the salient events to con- struct the final chronicle. Experimental re- sults demonstrate our approach is promis- ing to tackle this new problem. ",,,,ACL
57,2015,Context-aware Entity Morph Decoding,"Boliang Zhang, Hongzhao Huang, Xiaoman Pan, Sujian Li","People create morphs, a special type of fake alternative names, to achieve certain communication goals such as expressing strong sentiment or evading censors. For example, “Black Mamba”, the name for a highly venomous snake, is a morph that Kobe Bryant created for himself due to his agility and aggressiveness in playing bas- ketball games. This paper presents the first end-to-end context-aware entity morph de- coding system that can automatically iden- tify, disambiguate, verify morph mentions based on specific contexts, and resolve them to target entities. Our approach is based on an absolute “cold-start” - it does not require any candidate morph or tar- get entity lists as input, nor any manually constructed morph-target pairs for train- ing. We design a semi-supervised collec- tive inference framework for morph men- tion extraction, and compare various deep learning based approaches for morph res- olution. Our approach achieved signifi- cant improvement over the state-of-the-art method (Huang et al., 2013), which used a large amount of training data. 1 ",,,,ACL
58,2015,Multi-Objective Optimization for the Joint Disambiguation of Nouns and Named Entities,"Dirk Weissenborn, Leonhard Hennig, Feiyu Xu, Hans Uszkoreit","In this paper, we present a novel approach to joint word sense disambiguation (WSD) and entity linking (EL) that combines a set of complementary objectives in an exten- sible multi-objective formalism. During disambiguation the system performs con- tinuous optimization to find optimal prob- ability distributions over candidate senses. The performance of our system on nomi- nal WSD as well as EL improves state-of- the-art results on several corpora. These improvements demonstrate the importance of combining complementary objectives in a joint model for robust disambiguation. ",,,,ACL
59,2015,Building a Scientific Concept Hierarchy Database (SCHBase),"Eytan Adar, Srayan Datta","Extracted keyphrases can enhance numer- ous applications ranging from search to tracking the evolution of scientific dis- course. We present SCHB ASE , a hier- archical database of keyphrases extracted from large collections of scientific liter- ature. SCHB ASE relies on a tendency of scientists to generate new abbrevia- tions that “extend” existing forms as a form of signaling novelty. We demon- strate how these keyphrases/concepts can be extracted, and their viability as a database in relation to existing collections. We further show how keyphrases can be placed into a semantically-meaningful “phylogenetic” structure and describe key features of this structure. The com- plete SCHB ASE dataset is available at: http://cond.org/schbase.html. ",,,,ACL
60,2015,Sentiment-Aspect Extraction based on Restricted Boltzmann Machines,"Linlin Wang, Kang Liu, Zhu Cao, Jun Zhao",Aspect extraction and sentiment analysis of reviews are both important tasks in opinion mining. We propose a novel senti- ment and aspect extraction model based on Restricted Boltzmann Machines to jointly address these two tasks in an unsupervised setting. This model reflects the gener- ation process of reviews by introducing a heterogeneous structure into the hidden layer and incorporating informative priors. Experiments show that our model outper- forms previous state-of-the-art methods. ,,,,ACL
61,2015,Classifying Relations by Ranking with Convolutional Neural Networks,"Cícero dos Santos, Bing Xiang, Bowen Zhou","Relation classification is an important se- mantic processing task for which state-of- the-art systems still rely on costly hand- crafted features. In this work we tackle the relation classification task using a convo- lutional neural network that performs clas- sification by ranking (CR-CNN). We pro- pose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experi- ments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CR- CNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted fea- tures. Additionally, our experimental re- sults show that: (1) our approach is more effective than CNN followed by a soft- max classifier; (2) omitting the representa- tion of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals. ",,,,ACL
62,2015,Semantic Representations for Domain Adaptation: A Case Study on the Tree Kernel-based Method for Relation Extraction,"Thien Huu Nguyen, Barbara Plank, Ralph Grishman","We study the application of word embed- dings to generate semantic representations for the domain adaptation problem of re- lation extraction (RE) in the tree kernel- based method. We systematically evaluate various techniques to generate the seman- tic representations and demonstrate that they are effective to improve the general- ization performance of a tree kernel-based relation extractor across domains (up to 7% relative improvement). In addition, we compare the tree kernel-based and the feature-based method for RE in a compat- ible way, on the same resources and set- tings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. ",,,,ACL
63,2015,"Omnia Mutantur, Nihil Interit: Connecting Past with Present by Finding Corresponding Terms across Time","Yating Zhang, Adam Jatowt, Sourav Bhowmick, Katsumi Tanaka","In the current fast-paced world, people tend to possess limited knowledge about things from the past. For example, some young users may not know that Walkman played similar func- tion as iPod does nowadays. In this paper, we approach the temporal correspondence prob- lem in which, given an input term (e.g., iPod) and the target time (e.g. 1980s), the task is to find the counterpart of the query that existed in the target time. We propose an approach that transforms word contexts across time based on their neural network representations. We then experimentally demonstrate the ef- fectiveness of our method on the New York Times Annotated Corpus. ",,,,ACL
64,2015,Negation and Speculation Identification in Chinese Language,"Bowei Zou, Qiaoming Zhu, Guodong Zhou","Identifying negative or speculative narra- tive fragments from fact is crucial for natural language processing (NLP) appli- cations. Previous studies on negation and speculation identification in Chinese lan- guage suffers much from two problems: corpus scarcity and the bottleneck in fun- damental Chinese information processing. To resolve these problems, this paper constructs a Chinese corpus which con- sists of three sub-corpora from different resources. In order to detect the negative and speculative cues, a sequence labeling model is proposed. Moreover, a bilingual cue expansion method is proposed to in- crease the coverage in cue detection. In addition, this paper presents a new syn- tactic structure-based framework to iden- tify the linguistic scope of a cue, instead of the traditional chunking-based frame- work. Experimental results justify the usefulness of our Chinese corpus and the appropriateness of our syntactic struc- ture-based framework which obtained significant improvement over the state- of-the-art on negation and speculation identification in Chinese language. * ",,,,ACL
65,2015,Learning Relational Features with Backward Random Walks,"Ni Lao, Einat Minkov, William Cohen","The path ranking algorithm (PRA) has been recently proposed to address relational classification and retrieval tasks at large scale. We describe Cor-PRA, an enhanced system that can model a larger space of relational rules, including longer relational rules and a class of first order rules with constants, while maintaining scalability. We describe and test faster algorithms for searching for these features. A key contribution is to leverage backward random walks to efficiently discover these types of rules. An empirical study is conducted on the tasks of graph-based knowledge base inference, and person named entity extraction from parsed text. Our results show that learning paths with constants improves performance on both tasks, and that modeling longer paths dramatically improves performance for the named entity extraction task. ",,,,ACL
66,2015,Learning the Semantics of Manipulation Action,"Yezhou Yang, Yiannis Aloimonos, Cornelia Fermüller, Eren Erdal Aksoy","In this paper we present a formal compu- tational framework for modeling manip- ulation actions. The introduced formal- ism leads to semantics of manipulation ac- tion and has applications to both observ- ing and understanding human manipula- tion actions as well as executing them with a robotic mechanism (e.g. a humanoid robot). It is based on a Combinatory Cat- egorial Grammar. The goal of the intro- duced framework is to: (1) represent ma- nipulation actions with both syntax and se- mantic parts, where the semantic part em- ploys λ-calculus; (2) enable a probabilis- tic semantic parsing schema to learn the lambda-calculus representation of manip- ulation action from an annotated action corpus of videos; (3) use (1) and (2) to de- velop a system that visually observes ma- nipulation actions and understands their meaning while it can reason beyond ob- servations using propositional logic and axiom schemata. The experiments con- ducted on a public available large manip- ulation action dataset validate the theoret- ical framework and our implementation. ",,,,ACL
67,2015,Knowledge Graph Embedding via Dynamic Mapping Matrix,"Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu","Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR re- gard a relation as translation from head en- tity to tail entity and the CTransR achieves state-of-the-art performance. In this pa- per, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named sym- bol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to con- struct mapping matrix dynamically. Com- pared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less param- eters and has no matrix-vector multipli- cation operations, which makes it can be applied on large scale graphs. In Experi- ments, we evaluate our model on two typ- ical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms state- of-the-art methods. ",,,,ACL
68,2015,How Far are We from Fully Automatic High Quality Grammatical Error Correction?,"Christopher Bryant, Hwee Tou Ng","In this paper, we first explore the role of inter-annotator agreement statistics in grammatical error correction and conclude that they are less informative in fields where there may be more than one correct answer. We next created a dataset of 50 student essays, each corrected by 10 dif- ferent annotators for all error types, and in- vestigated how both human and GEC sys- tem scores vary when different combina- tions of these annotations are used as the gold standard. Upon learning that even hu- mans are unable to score higher than 75% F 0.5 , we propose a new metric based on the ratio between human and system per- formance. We also use this method to in- vestigate the extent to which annotators agree on certain error categories, and find that similar results can be obtained from a smaller subset of just 10 essays. ",,,,ACL
69,2015,Knowledge Portability with Semantic Expansion of Ontology Labels,"Mihael Arcan, Marco Turchi, Paul Buitelaar","Our research focuses on the multilin- gual enhancement of ontologies that, of- ten represented only in English, need to be translated in different languages to en- able knowledge access across languages. Ontology translation is a rather different task then the classic document translation, because ontologies contain highly specific vocabulary and they lack contextual in- formation. For these reasons, to improve automatic ontology translations, we first focus on identifying relevant unambigu- ous and domain-specific sentences from a large set of generic parallel corpora. Then, we leverage Linked Open Data resources, such as DBPedia, to isolate ontology- specific bilingual lexical knowledge. In both cases, we take advantage of the se- mantic information of the labels to se- lect relevant bilingual data with the aim of building an ontology-specific statistical machine translation system. We evaluate our approach on the translation of a medi- cal ontology, translating from English into German. Our experiment shows a sig- nificant improvement of around 3 BLEU points compared to a generic as well as a domain-specific translation approach. ",,,,ACL
70,2015,Automatic disambiguation of English puns,"Tristan Miller, Iryna Gurevych","Traditional approaches to word sense dis- ambiguation (WSD) rest on the assump- tion that there exists a single, unambigu- ous communicative intention underlying every word in a document. However, writ- ers sometimes intend for a word to be in- terpreted as simultaneously carrying mul- tiple distinct meanings. This deliberate use of lexical ambiguity—i.e., punning— is a particularly common source of humour. In this paper we describe how traditional, language-agnostic WSD approaches can be adapted to “disambiguate” puns, or rather to identify their double meanings. We eval- uate several such approaches on a manually sense-annotated collection of English puns and observe performance exceeding that of some knowledge-based and supervised baselines. ",,,,ACL
71,2015,Unsupervised Cross-Domain Word Representation Learning,"Danushka Bollegala, Takanori Maehara, Ken-ichi Kawarabayashi","Meaning of a word varies from one do- main to another. Despite this impor- tant domain dependence in word seman- tics, existing word representation learning methods are bound to a single domain. Given a pair of source-target domains, we propose an unsupervised method for learning domain-specific word representa- tions that accurately capture the domain- specific aspects of word semantics. First, we select a subset of frequent words that occur in both domains as pivots. Next, we optimize an objective function that enforces two constraints: (a) for both source and target domain documents, piv- ots that appear in a document must accu- rately predict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two do- mains. Moreover, we propose a method to perform domain adaptation using the learnt word representations. Our proposed method significantly outperforms compet- itive baselines including the state-of-the- art domain-insensitive word representa- tions, and reports best sentiment classifi- cation accuracies for all domain-pairs in a benchmark dataset. ",,,,ACL
72,2015,A Unified Multilingual Semantic Representation of Concepts,"José Camacho-Collados, Mohammad Taher Pilehvar, Roberto Navigli","Semantic representation lies at the core of several applications in Natural Language Processing. However, most existing se- mantic representation techniques cannot be used effectively for the representation of individual word senses. We put for- ward a novel multilingual concept repre- sentation, called M UFFIN , which not only enables accurate representation of word senses in different languages, but also pro- vides multiple advantages over existing approaches. M UFFIN represents a given concept in a unified semantic space irre- spective of the language of interest, en- abling cross-lingual comparison of differ- ent concepts. We evaluate our approach in two different evaluation benchmarks, se- mantic similarity and Word Sense Disam- biguation, reporting state-of-the-art per- formance on several standard datasets. ",,,,ACL
73,2015,Demographic Factors Improve Classification Performance,Dirk Hovy,"Extra-linguistic factors influence language use, and are accounted for by speakers and listeners. Most natural language pro- cessing (NLP) tasks to date, however, treat language as uniform. This assump- tion can harm performance. We investi- gate the effect of including demographic information on performance in a variety of text-classification tasks. We find that by including age or gender information, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages. ",,,,ACL
74,2015,Vector-space calculation of semantic surprisal for predicting word pronunciation duration,"Asad Sayeed, Stefan Fischer, Vera Demberg","In order to build psycholinguistic mod- els of processing difficulty and evaluate these models against human data, we need highly accurate language models. Here we specifically consider surprisal, a word’s predictability in context. Existing ap- proaches have mostly used n-gram models or more sophisticated syntax-based pars- ing models; this largely does not account for effects specific to semantics. We build on the work by Mitchell et al. (2010) and show that the semantic prediction model suggested there can successfully predict spoken word durations in naturalistic con- versational data. An interesting finding is that the training data for the semantic model also plays a strong role: the model trained on in- domain data, even though a better lan- guage model for our data, is not able to predict word durations, while the out-of- domain trained language model does pre- dict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data. ",,,,ACL
75,2015,Efficient Methods for Inferring Large Sparse Topic Hierarchies,"Doug Downey, Chandra Bhagavatula, Yi Yang","Latent variable topic models such as La- tent Dirichlet Allocation (LDA) can dis- cover topics from text in an unsupervised fashion. However, scaling the models up to the many distinct topics exhibited in modern corpora is challenging. “Flat” topic models like LDA have difficulty modeling sparsely expressed topics, and richer hierarchical models become compu- tationally intractable as the number of top- ics increases. In this paper, we introduce efficient meth- ods for inferring large topic hierarchies. Our approach is built upon the Sparse Backoff Tree (SBT), a new prior for la- tent topic distributions that organizes the latent topics as leaves in a tree. We show how a document model based on SBTs can effectively infer accurate topic spaces of over a million topics. We introduce a collapsed sampler for the model that ex- ploits sparsity and the tree structure in or- der to make inference efficient. In exper- iments with multiple data sets, we show that scaling to large topic spaces results in much more accurate models, and that SBT document models make use of large topic spaces more effectively than flat LDA. ",,,,ACL
76,2015,Trans-dimensional Random Fields for Language Modeling,"Bin Wang, Zhijian Ou, Zhiqiang Tan","Language modeling (LM) involves determining the joint probability of words in a sentence. The conditional approach is dominant, representing the joint probability in terms of conditionals. Examples include n-gram LMs and neural network LMs. An alternative approach, called the random field (RF) approach, is used in whole-sentence maximum entropy (WSME) LMs. Although the RF approach has potential benefits, the empirical results of previous WSME models are not satisfactory. In this paper, we revisit the RF approach for language modeling, with a number of innovations. We propose a trans-dimensional RF (TDRF) model and develop a training algorithm using joint stochastic approximation and trans-dimensional mixture sampling. We perform speech recognition experiments on Wall Street Journal data, and find that our TDRF models lead to performances as good as the recurrent neural network LMs but are computationally more efficient in computing sentence probability. ",,,,ACL
77,2015,Gaussian LDA for Topic Models with Word Embeddings,"Rajarshi Das, Manzil Zaheer, Chris Dyer","Continuous space word embeddings learned from large, unstructured corpora have been shown to be effective at cap- turing semantic regularities in language. In this paper we replace LDA’s param- eterization of “topics” as categorical distributions over opaque word types with multivariate Gaussian distributions on the embedding space. This encourages the model to group words that are a priori known to be semantically related into topics. To perform inference, we introduce a fast collapsed Gibbs sampling algorithm based on Cholesky decom- positions of covariance matrices of the posterior predictive distributions. We fur- ther derive a scalable algorithm that draws samples from stale posterior predictive distributions and corrects them with a Metropolis–Hastings step. Using vectors learned from a domain-general corpus (English Wikipedia), we report results on two document collections (20-newsgroups and NIPS). Qualitatively, Gaussian LDA infers different (but still very sensible) topics relative to standard LDA. Quantita- tively, our technique outperforms existing models at dealing with OOV words in held-out documents. ",,,,ACL
78,2015,Pairwise Neural Machine Translation Evaluation,"Francisco Guzmán, Shafiq Joty, Lluís Màrquez, Preslav Nakov","We present a novel framework for ma- chine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypothe- ses and the reference, as well as between the two hypotheses. These compact repre- sentations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art. ",,,,ACL
79,2015,String-to-Tree Multi Bottom-up Tree Transducers,"Nina Seemann, Fabienne Braune, Andreas Maletti",We achieve significant improvements in several syntax-based machine translation experiments using a string-to-tree vari- ant of multi bottom-up tree transducers. Our new parameterized rule extraction al- gorithm extracts string-to-tree rules that can be discontiguous and non-minimal in contrast to existing algorithms for the tree-to-tree setting. The obtained models significantly outperform the string-to-tree component of the Moses framework in a large-scale empirical evaluation on several known translation tasks. Our linguistic analysis reveals the remarkable benefits of discontiguous and non-minimal rules. ,,,,ACL
80,2015,Non-linear Learning for Statistical Machine Translation,"Shujian Huang, Huadong Chen, Xin-Yu Dai, Jiajun Chen","Modern statistical machine translation (SMT) systems usually use a linear com- bination of features to model the quality of each translation hypothesis. The linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest fea- tures in an linear manner, which might limit the expressive power of the model and lead to a under-fit model on the cur- rent data. In this paper, we propose a non- linear modeling for the quality of transla- tion hypotheses based on neural networks, which allows more complex interaction between features. A learning framework is presented for training the non-linear mod- els. We also discuss possible heuristics in designing the network structure which may improve the non-linear learning per- formance. Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model. ",,,,ACL
81,2015,Unifying Bayesian Inference and Vector Space Models for Improved Decipherment,"Qing Dou, Ashish Vaswani, Kevin Knight, Chris Dyer","We introduce into Bayesian decipherment a base distribution derived from similari- ties of word embeddings. We use Dirich- let multinomial regression (Mimno and McCallum, 2012) to learn a mapping be- tween ciphertext and plaintext word em- beddings from non-parallel data. Exper- imental results show that the base dis- tribution is highly beneficial to decipher- ment, improving state-of-the-art decipher- ment accuracy from 45.8% to 67.4% for Spanish/English, and from 5.1% to 11.2% for Malagasy/English. ",,,,ACL
82,2015,Non-projective Dependency-based Pre-Reordering with Recurrent Neural Network for Machine Translation,"Antonio Valerio Miceli-Barone, Giuseppe Attardi","The quality of statistical machine translation performed with phrase based approaches can be increased by permuting the words in the source sentences in an order which resem- bles that of the target language. We propose a class of recurrent neu- ral models which exploit source-side dependency syntax features to re- order the words into a target-like or- der. We evaluate these models on the German-to-English and Italian-to- English language pairs, showing sig- nificant improvements over a phrase- based Moses baseline. We also com- pare with state of the art German-to- English pre-reordering rules, showing that our method obtains similar or bet- ter results. ",,,,ACL
83,2015,Detecting Deceptive Groups Using Conversations and Network Analysis,"Dian Yu, Yulia Tyshchuk, Heng Ji, William Wallace","Deception detection has been formulated as a supervised binary classification prob- lem on single documents. However, in daily life, millions of fraud cases involve detailed conversations between deceivers and victims. Deceivers may dynamically adjust their deceptive statements accord- ing to the reactions of victims. In addition, people may form groups and collaborate to deceive others. In this paper, we seek to identify deceptive groups from their con- versations. We propose a novel subgroup detection method that combines linguis- tic signals and signed network analysis for dynamic clustering. A social-elimination game called Killer Game is introduced as a case study 1 . Experimental results demon- strate that our approach significantly out- performs human voting and state-of-the- art subgroup detection methods at dynam- ically differentiating the deceptive groups from truth-tellers. ",,,,ACL
84,2015,WikiKreator: Improving Wikipedia Stubs Automatically,"Siddhartha Banerjee, Prasenjit Mitra","Stubs on Wikipedia often lack comprehen- sive information. The huge cost of edit- ing Wikipedia and the presence of only a limited number of active contributors curb the consistent growth of Wikipedia. In this work, we present WikiKreator, a system that is capable of generating content au- tomatically to improve existing stubs on Wikipedia. The system has two compo- nents. First, a text classifier built using topic distribution vectors is used to as- sign content from the web to various sec- tions on a Wikipedia article. Second, we propose a novel abstractive summariza- tion technique based on an optimization framework that generates section-specific summaries for Wikipedia stubs. Experi- ments show that WikiKreator is capable of generating well-formed informative con- tent. Further, automatically generated con- tent from our system have been appended to Wikipedia stubs and the content has been retained successfully proving the ef- fectiveness of our approach. ",,,,ACL
85,2015,Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes,"Chris Quirk, Raymond Mooney, Michel Galley","Using natural language to write programs is a touchstone problem for computational linguistics. We present an approach that learns to map natural-language descrip- tions of simple “if-then” rules to executable code. By training and testing on a large cor- pus of naturally-occurring programs (called “recipes”) and their natural language de- scriptions, we demonstrate the ability to effectively map language to code. We compare a number of semantic parsing ap- proaches on the highly noisy training data collected from ordinary users, and find that loosely synchronous systems perform best. ",,,,ACL
86,2015,Deep Questions without Deep Understanding,"Igor Labutov, Sumit Basu, Lucy Vanderwende","We develop an approach for generating deep (i.e, high-level) comprehension questions from novel text that bypasses the myriad challenges of creating a full se- mantic representation. We do this by de- composing the task into an ontology- crowd-relevance workflow, consisting of first representing the original text in a low-dimensional ontology, then crowd- sourcing candidate question templates aligned with that space, and finally rank- ing potentially relevant templates for a novel region of text. If ontological labels are not available, we infer them from the text. We demonstrate the effectiveness of this method on a corpus of articles from Wikipedia alongside human judgments, and find that we can generate relevant deep questions with a precision of over 85% while maintaining a recall of 70%. ",,,,ACL
87,2015,The NL2KR Platform for building Natural Language Translation Systems,"Nguyen Vo, Arindam Mitra, Chitta Baral","This paper presents the NL2KR platform to build systems that can translate text to different formal languages. It is freely- available 1 , customizable, and comes with an Interactive GUI support that is use- ful in the development of a translation system. Our key contribution is a user- friendly system based on an interactive multistage learning algorithm. This effec- tive algorithm employs Inverse-λ, Gener- alization and user provided dictionary to learn new meanings of words from sen- tences and their representations. Using the learned meanings, and the Generaliza- tion approach, it is able to translate new sentences. NL2KR is evaluated on two standard corpora, Jobs and GeoQuery and it exhibits state-of-the-art performance on both of them. ",,,,ACL
88,2015,Multiple Many-to-Many Sequence Alignment for Combining String-Valued Variables: A G2P Experiment,Steffen Eger,"We investigate multiple many-to-many alignments as a primary step in integrat- ing supplemental information strings in string transduction. Besides outlining DP based solutions to the multiple alignment problem, we detail an approximation of the problem in terms of multiple sequence segmentations satisfying a coupling con- straint. We apply our approach to boosting baseline G2P systems using homogeneous as well as heterogeneous sources of sup- plemental information. ",,,,ACL
89,2015,Tweet Normalization with Syllables,"Ke Xu, Yunqing Xia, Chin-Hui Lee","In this paper, we propose a syllable-based method for tweet normalization to study the cognitive process of non-standard word creation in social media. Assuming that syllable plays a fundamental role in forming the non-standard tweet words, we choose syllable as the basic unit and extend the conventional noisy channel model by incorporating the syllables to represent the word-to-word transitions at both word and syllable levels. The syllables are used in our method not only to suggest more candidates, but also to measure similarity between words. Novelty of this work is three-fold: First, to the best of our knowledge, this is an early attempt to explore syllables in tweet normalization. Second, our proposed normalization method relies on unlabeled samples, making it much easier to adapt our method to handle non-standard words in any period of history. And third, we conduct a series of experiments and prove that the proposed method is advantageous over the state-of-art solutions for tweet normalization. ",,,,ACL
90,2015,Improving Named Entity Recognition in Tweets via Detecting Non-Standard Words,"Chen Li, Yang Liu","Most previous work of text normalization on informal text made a strong assumption that the system has already known which tokens are non-standard words (NSW) and thus need normalization. However, this is not realistic. In this paper, we propose a method for NSW detection. In addi- tion to the information based on the dic- tionary, e.g., whether a word is out-of- vocabulary (OOV), we leverage novel in- formation derived from the normalization results for OOV words to help make deci- sions. Second, this paper investigates two methods using NSW detection results for named entity recognition (NER) in social media data. One adopts a pipeline strat- egy, and the other uses a joint decoding fashion. We also create a new data set with newly added normalization annota- tion beyond the existing named entity la- bels. This is the first data set with such annotation and we release it for research purpose. Our experiment results demon- strate the effectiveness of our NSW detec- tion method and the benefit of NSW detec- tion for NER. Our proposed methods per- form better than the state-of-the-art NER system. ",,,,ACL
91,2015,A Unified Kernel Approach for Learning Typed Sentence Rewritings,"Martin Gleize, Brigitte Grau","Many high level natural language process- ing problems can be framed as determin- ing if two given sentences are a rewrit- ing of each other. In this paper, we pro- pose a class of kernel functions, referred to as type-enriched string rewriting ker- nels, which, used in kernel-based machine learning algorithms, allow to learn sen- tence rewritings. Unlike previous work, this method can be fed external lexical se- mantic relations to capture a wider class of rewriting rules. It also does not assume preliminary syntactic parsing but is still able to provide a unified framework to cap- ture syntactic structure and alignments be- tween the two sentences. We experiment on three different natural sentence rewrit- ing tasks and obtain state-of-the-art results for all of them. ",,,,ACL
92,2015,Perceptually Grounded Selectional Preferences,"Ekaterina Shutova, Niket Tandon, Gerard de Melo","Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been tradi- tionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present the first SP learning method that si- multaneously draws knowledge from text, images and videos, using image and video descriptions to obtain visual features. Our results show that it outperforms linguistic and visual models in isolation, as well as the existing SP induction approaches. ",,,,ACL
93,2015,Joint Case Argument Identification for Japanese Predicate Argument Structure Analysis,"Hiroki Ouchi, Hiroyuki Shindo, Kevin Duh, Yuji Matsumoto","Existing methods for Japanese predicate argument structure (PAS) analysis identify case arguments of each predicate without considering interactions between the tar- get PAS and others in a sentence. How- ever, the argument structures of the pred- icates in a sentence are semantically re- lated to each other. This paper proposes new methods for Japanese PAS analysis to jointly identify case arguments of all predicates in a sentence by (1) modeling multiple PAS interactions with a bipar- tite graph and (2) approximately search- ing optimal PAS combinations. Perform- ing experiments on the NAIST Text Cor- pus, we demonstrate that our joint analysis methods substantially outperform a strong baseline and are comparable to previous work. ",,,,ACL
94,2015,Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model,"Nghia The Pham, Germán Kruszewski, Angeliki Lazaridou, Marco Baroni","We introduce C-PHRASE, a distributional semantic model that learns word repre- sentations by optimizing context predic- tion for phrases at all levels in a syntactic tree, from single words to full sentences. C-PHRASE outperforms the state-of-the- art C-BOW model on a variety of lexical tasks. Moreover, since C-PHRASE word vectors are induced through a composi- tional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sen- tence representations that rival those gen- erated by ad-hoc compositional models. ",,,,ACL
95,2015,Robust Subgraph Generation Improves Abstract Meaning Representation Parsing,"Keenon Werling, Gabor Angeli, Christopher D. Manning","Meaning Representation (AMR) is a representation for open- domain rich semantics, with potential use in fields like event extraction and machine translation. Node generation, typically done using a simple dictionary lookup, is currently an important limiting factor in AMR parsing. We propose a small set of actions that derive AMR subgraphs by transformations on spans of text, which allows for more robust learning of this stage. Our set of construction actions generalize better than the previous ap- proach, and can be learned with a sim- ple classifier. We improve on the previ- ous state-of-the-art result for AMR pars- ing, boosting end-to-end performance by 3 F 1 on both the LDC2013E117 and LDC2014T12 datasets. ",,,,ACL
96,2015,Environment-Driven Lexicon Induction for High-Level Instructions,"Dipendra Kumar Misra, Kejia Tao, Percy Liang, Ashutosh Saxena","We focus on the task of interpreting com- plex natural language instructions to a robot, in which we must ground high-level commands such as microwave the cup to low-level actions such as grasping. Pre- vious approaches that learn a lexicon dur- ing training have inadequate coverage at test time, and pure search strategies can- not handle the exponential search space. We propose a new hybrid approach that leverages the environment to induce new lexical entries at test time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to suc- cessfully ground new verbs such as dis- tribute, mix, arrange to complex logical forms, each containing up to four predi- cates. ",,,,ACL
97,2015,Structural Representations for Learning Relations between Pairs of Texts,"Simone Filice, Giovanni Da San Martino, Alessandro Moschitti","This paper studies the use of structural representations for learning relations be- tween pairs of short texts (e.g., sentences or paragraphs) of the kind: the second text answers to, or conveys exactly the same information of, or is implied by, the first text. Engineering effective features that can capture syntactic and semantic re- lations between the constituents compos- ing the target text pairs is rather complex. Thus, we define syntactic and semantic structures representing the text pairs and then apply graph and tree kernels to them for automatically engineering features in Support Vector Machines. We carry out an extensive comparative analysis of state- of-the-art models for this type of relational learning. Our findings allow for achiev- ing the highest accuracy in two differ- ent and important related tasks, i.e., Para- phrasing Identification and Textual Entail- ment Recognition. ",,,,ACL
98,2015,Learning Semantic Representations of Users and Products for Document Level Sentiment Classification,"Duyu Tang, Bing Qin, Ting Liu","Neural network methods have achieved promising results for sentiment classifica- tion of text. However, these models on- ly use semantics of texts, while ignoring users who express the sentiment and prod- ucts which are evaluated, both of which have great influences on interpreting the sentiment of text. In this paper, we ad- dress this issue by incorporating user- and product- level information into a neural network approach for document level sen- timent classification. Users and product- s are modeled using vector space mod- els, the representations of which capture important global clues such as individu- al preferences of users or overall quali- ties of products. Such global evidence in turn facilitates embedding learning pro- cedure at document level, yielding better text representations. By combining ev- idence at user-, product- and document- level in a unified neural framework, the proposed model achieves state-of-the-art performances on IMDB and Yelp dataset- s 1 . ",,,,ACL
99,2015,Towards Debugging Sentiment Lexicons,"Andrew Schneider, Eduard Dragut","Central to many sentiment analysis tasks are sentiment lexicons (SLs). SLs exhibit polarity inconsistencies. Previous work studied the problem of checking the con- sistency of an SL for the case when the en- tries have categorical labels (positive, neg- ative or neutral) and showed that it is NP- hard. In this paper, we address the more general problem, in which polarity tags take the form of a continuous distribution in the interval [0, 1]. We show that this problem is polynomial. We develop a gen- eral framework for addressing the consis- tency problem using linear programming (LP) theory. LP tools allow us to uncover inconsistencies efficiently, paving the way to building SL debugging tools. We show that previous work corresponds to 0-1 inte- ger programming, a particular case of LP. Our experimental studies show a strong correlation between polarity consistency in SLs and the accuracy of sentiment tag- ging in practice. ",,,,ACL
100,2015,"Sparse, Contextually Informed Models for Irony Detection: Exploiting User Communities, Entities and Sentiment","Byron C. Wallace, Do Kook Choe, Eugene Charniak","Automatically detecting verbal irony (roughly, sarcasm) in online content is important for many practical applications (e.g., sentiment detection), but it is dif- ficult. Previous approaches have relied predominantly on signal gleaned from word counts and grammatical cues. But such approaches fail to exploit the context in which comments are embedded. We thus propose a novel strategy for verbal irony classification that exploits contex- tual features, specifically by combining noun phrases and sentiment extracted from comments with the forum type (e.g., conservative or liberal) to which they were posted. We show that this approach improves verbal irony classifica- tion performance. Furthermore, because this method generates a very large feature space (and we expect predictive contextual features to be strong but few), we propose a mixed regularization strategy that places a sparsity-inducing 1 penalty on the contextual feature weights on top of the 2 penalty applied to all model coefficients. This increases model sparsity and reduces the variance of model performance. ",,,,ACL
101,2015,Sentence-level Emotion Classification with Label and Context Dependence,"Shoushan Li, Lei Huang, Rong Wang, Guodong Zhou","Predicting emotion categories, such as anger, joy, and anxiety, expressed by a sentence is challenging due to its inherent multi-label classification difficulty and data sparseness. In this paper, we address above two chal- lenges by incorporating the label dependence among the emotion labels and the context de- pendence among the contextual instances into a factor graph model. Specifically, we recast sentence-level emotion classification as a fac- tor graph inferring problem in which the label and context dependence are modeled as vari- ous factor functions. Empirical evaluation demonstrates the great potential and effective- ness of our proposed approach to sentence- level emotion classification. 1 ",,,,ACL
102,2015,Co-training for Semi-supervised Sentiment Classification Based on Dual-view Bags-of-words Representation,"Rui Xia, Cheng Wang, Xin-Yu Dai, Tao Li","A review text is normally represented as a bag-of-words (BOW) in sentiment clas- sification. Such a simplified BOW model has fundamental deficiencies in modeling some complex linguistic phenomena such as negation. In this work, we propose a dual-view co-training algorithm based on dual-view BOW representation for semi- supervised sentiment classification. In dual-view BOW, we automatically con- struct antonymous reviews and model a review text by a pair of bags-of-words with opposite views. We make use of the original and antonymous views in pairs, in the training, bootstrapping and test- ing process, all based on a joint observa- tion of two views. The experimental re- sults demonstrate the advantages of our ap- proach, in meeting the two co-training re- quirements, addressing the negation prob- lem, and enhancing the semi-supervised sentiment classification efficiency. ",,,,ACL
103,2015,Improving social relationships in face-to-face human-agent interactions: when the agent wants to know user’s likes and dislikes,"Caroline Langlet, Chloé Clavel","This paper tackles the issue of the detec- tion of user’s verbal expressions of likes and dislikes in a human-agent interaction. We present a system grounded on the theo- retical framework provided by (Martin and White, 2005) that integrates the interac- tion context by jointly processing agent’s and user’s utterances. It is designed as a rule-based and bottom-up process based on a symbolic representation of the struc- ture of the sentence. This article also describes the annotation campaign – car- ried out through Amazon Mechanical Turk – for the creation of the evaluation data- set. Finally, we present all measures for rating agreement between our system and the human reference and obtain agreement scores that are equal or higher than sub- stantial agreements. ",,,,ACL
104,2015,Learning Word Representations from Scarce and Noisy Data with Embedding Subspaces,"Ramon Astudillo, Silvio Amir, Wang Ling, Mário Silva","We investigate a technique to adapt unsu- pervised word embeddings to specific ap- plications, when only small and noisy la- beled datasets are available. Current meth- ods use pre-trained embeddings to initial- ize model parameters, and then use the la- beled data to tailor them for the intended task. However, this approach is prone to overfitting when the training is performed with scarce and noisy data. To overcome this issue, we use the supervised data to find an embedding subspace that fits the task complexity. All the word representa- tions are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset. This ap- proach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results. Here we show results improving those of the chal- lenge, as well as additional experiments in a Twitter Part-Of-Speech tagging task. ",,,,ACL
105,2015,Automatic Spontaneous Speech Grading: A Novel Feature Derivation Technique using the Crowd,"Vinay Shashidhar, Nishant Pandey, Varun Aggarwal","In this paper, we address the problem of evaluating spontaneous speech us- ing a combination of machine learning and crowdsourcing. Machine learning techniques inadequately solve the stated problem because automatic speaker- independent speech transcription is inaccurate. The features derived from it are also inaccurate and so is the machine learning model developed for speech evaluation. To address this, we post the task of speech transcription to a large community of online workers (crowd). We also get spoken English grades from the crowd. We achieve 95% transcription accuracy by combining transcriptions from multiple crowd workers. Speech and prosody features are derived by force aligning the speech samples on these highly accurate transcriptions. Addi- tionally, we derive surface and semantic level features directly from the transcrip- tion. To demonstrate the efficacy of our approach we performed experiments on an expert–graded speech sample of 319 adult non–native speakers. Using these features in a regression model, we are able achieve a Pearson correlation of 0.76 with expert grades, an accuracy much higher than any previously reported machine learning approach. Our approach has an accuracy that rivals that of expert agreement. This work is timely given the huge requirement of spoken English training and assessment. ",,,,ACL
106,2015,Driving ROVER with Segment-based ASR Quality Estimation,"Shahab Jalalvand, Matteo Negri, Daniele Falavigna, Marco Turchi","ROVER is a widely used method to combine the output of multiple auto- matic speech recognition (ASR) systems. Though effective, the basic approach and its variants suffer from potential draw- backs: i) their results depend on the order in which the hypotheses are used to feed the combination process, ii) when applied to combine long hypotheses, they disre- gard possible differences in transcription quality at local level, iii) they often rely on word confidence information. We address these issues by proposing a segment-based ROVER in which hypothesis ranking is obtained from a confidence-independent ASR quality estimation method. Our re- sults on English data from the IWSLT2012 and IWSLT2013 evaluation campaigns significantly outperform standard ROVER and approximate two strong oracles. ",,,,ACL
107,2015,A Hierarchical Neural Autoencoder for Paragraphs and Documents,"Jiwei Li, Thang Luong, Dan Jurafsky","Natural language generation of coherent long texts like paragraphs or longer doc- uments is a challenging problem for re- current networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long- short term memory) auto-encoder to pre- serve and reconstruct multi-sentence para- graphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph us- ing standard metrics like ROUGE and En- tity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coher- ence. While only a first step toward gener- ating coherent text units from neural mod- els, our work has the potential to signifi- cantly impact natural language generation and summarization 1 . ",,,,ACL
108,2015,Joint Dependency Parsing and Multiword Expression Tokenization,"Alexis Nasr, Carlos Ramisch, José Deulofeu, André Valli","Complex conjunctions and determiners are often considered as pretokenized units in parsing. This is not always realistic, since they can be ambiguous. We pro- pose a model for joint dependency parsing and multiword expressions identification, in which complex function words are rep- resented as individual tokens linked with morphological dependencies. Our graph- based parser includes standard second- order features and verbal subcategoriza- tion features derived from a syntactic lex- icon.We train it on a modified version of the French Treebank enriched with mor- phological dependencies. It recognizes 81.79% of ADV +que conjunctions with 91.57% precision, and 82.74% of de+ DET determiners with 86.70% precision. ",,,,ACL
109,2015,End-to-end learning of semantic role labeling using recurrent neural networks,"Jie Zhou, Wei Xu","Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems. To this date, most of the suc- cessful SRL systems were built on top of some form of parsing results (Koomen et al., 2005; Palmer et al., 2010; Pradhan et al., 2013), where pre-defined feature tem- plates over the syntactic structure are used. The attempts of building an end-to-end SRL learning system without using pars- ing were less successful (Collobert et al., 2011). In this work, we propose to use deep bi-directional recurrent network as an end-to-end system for SRL. We take on- ly original text information as input fea- ture, without using any syntactic knowl- edge. The proposed algorithm for seman- tic role labeling was mainly evaluated on CoNLL-2005 shared task and achieved F 1 score of 81.07. This result outperforms the previous state-of-the-art system from the combination of different parsing trees or models. We also obtained the same conclusion with F 1 = 81.27 on CoNLL- 2012 shared task. As a result of simplicity, our model is also computationally efficient that the parsing speed is 6.7k tokens per second. Our analysis shows that our model is better at handling longer sentences than traditional models. And the latent vari- ables of our model implicitly capture the syntactic structure of a sentence. ",,,,ACL
110,2015,Feature Optimization for Constituent Parsing via Neural Networks,"Zhiguo Wang, Haitao Mi, Nianwen Xue","The performance of discriminative con- stituent parsing relies crucially on feature engineering, and effective features usu- ally have to be carefully selected through a painful manual process. In this paper, we propose to automatically learn a set of effective features via neural networks. Specifically, we build a feedforward neu- ral network model, which takes as input a few primitive units (words, POS tags and certain contextual tokens) from the lo- cal context, induces the feature represen- tation in the hidden layer and makes pars- ing predictions in the output layer. The network simultaneously learns the feature representation and the prediction model parameters using a back propagation al- gorithm. By pre-training the model on a large amount of automatically parsed data, and then fine-tuning on the manually an- notated Treebank data, our parser achieves the highest F 1 score at 86.6% on Chi- nese Treebank 5.1, and a competitive F 1 score at 90.7% on English Treebank. More importantly, our parser generalizes well on cross-domain test sets, where we sig- nificantly outperform Berkeley parser by 3.4 points on average for Chinese and 2.5 points for English. ",,,,ACL
111,2015,Identifying Cascading Errors using Constraints in Dependency Parsing,"Dominick Ng, James R. Curran","Dependency parsers are usually evaluated on attachment accuracy. Whilst easily in- terpreted, the metric does not illustrate the cascading impact of errors, where the parser chooses an incorrect arc, and is sub- sequently forced to choose further incor- rect arcs elsewhere in the parse. We apply arc-level constraints to MST- parser and ZPar, enforcing the correct analysis of specific error classes, whilst otherwise continuing with decoding. We investigate the direct and indirect impact of applying constraints to the parser. Er- roneous NP and punctuation attachments cause the most cascading errors, while in- correct PP and coordination attachments are frequent but less influential. Punctu- ation is especially challenging, as it has long been ignored in parsing, and serves a variety of disparate syntactic roles. ",,,,ACL
112,2015,A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network,"Chenxi Zhu, Xipeng Qiu, Xinchi Chen, Xuanjing Huang","In this work, we address the prob- lem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntac- tic and compositional-semantic represen- tations of phrases and words in a depen- dency tree. Different with the original re- cursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most infor- mative compositions by the pooling lay- ers. Based on RCNN, we use a discrimina- tive model to re-rank a k-best list of can- didate dependency parsing trees. The ex- periments show that RCNN is very effec- tive to improve the state-of-the-art depen- dency parsing on both English and Chi- nese datasets. ",,,,ACL
113,2015,Transition-based Neural Constituent Parsing,"Taro Watanabe, Eiichiro Sumita","Constituent parsing is typically modeled by a chart-based algorithm under prob- abilistic context-free grammars or by a transition-based algorithm with rich fea- tures. Previous models rely heavily on richer syntactic information through lex- icalizing rules, splitting categories, or memorizing long histories. However en- riched models incur numerous parameters and sparsity issues, and are insufficient for capturing various syntactic phenomena. We propose a neural network structure that explicitly models the unbounded history of actions performed on the stack and queue employed in transition-based parsing, in addition to the representations of partially parsed tree structure. Our transition-based neural constituent parsing achieves perfor- mance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, with- out reranking, feature templates or addi- tional data to train model parameters. ",,,,ACL
114,2015,Feature Selection in Kernel Space: A Case Study on Dependency Parsing,"Xian Qian, Yang Liu","Given a set of basic binary features, we propose a new L 1 norm SVM based feature selection method that explicitly selects the features in their polynomial or tree kernel spaces. The efficiency comes from the anti-monotone property of the subgradients: the subgradient with respect to a combined feature can be bounded by the subgradient with respect to each of its component features, and a feature can be pruned safely without further consideration if its corresponding subgradient is not steep enough. We conduct experiments on the English dependency parsing task with a third order graph-based parser. Benefiting from the rich features selected in the tree kernel space, our model achieved the best reported unlabeled attachment score of 93.72 without using any additional resource. ",,,,ACL
115,2015,Semantic Role Labeling Improves Incremental Parsing,"Ioannis Konstas, Frank Keller","Incremental parsing is the task of assign- ing a syntactic structure to an input sen- tence as it unfolds word by word. Incre- mental parsing is more difficult than full- sentence parsing, as incomplete input in- creases ambiguity. Intuitively, an incre- mental parser that has access to seman- tic information should be able to reduce ambiguity by ruling out semantically im- plausible analyses, even for incomplete in- put. In this paper, we test this hypothesis by combining an incremental TAG parser with an incremental semantic role labeler in a discriminative framework. We show a substantial improvement in parsing per- formance compared to the baseline parser, both in full-sentence F-score and in incre- mental F-score. ",,,,ACL
116,2015,Discontinuous Incremental Shift-reduce Parsing,Wolfgang Maier,"We present an extension to incremental shift-reduce parsing that handles discon- tinuous constituents, using a linear clas- sifier and beam search. We achieve very high parsing speeds (up to 640 sent./sec.) and accurate results (up to 79.52 F 1 on TiGer). ",,,,ACL
117,2015,A Neural Probabilistic Structured-Prediction Model for Transition-Based Dependency Parsing,"Hao Zhou, Yue Zhang, Shujian Huang, Jiajun Chen","Neural probabilistic parsers are attrac- tive for their capability of automatic fea- ture combination and small data sizes. A transition-based greedy neural parser has given better accuracies over its lin- ear counterpart. We propose a neural probabilistic structured-prediction model for transition-based dependency parsing, which integrates search and learning. Beam search is used for decoding, and contrastive learning is performed for max- imizing the sentence-level log-likelihood. In standard Penn Treebank experiments, the structured neural parser achieves a 1.8% accuracy improvement upon a com- petitive greedy neural parser baseline, giv- ing performance comparable to the best linear parser. ",,,,ACL
118,2015,Parsing Paraphrases with Joint Inference,"Do Kook Choe, David McClosky","Treebanks are key resources for develop- ing accurate statistical parsers. However, building treebanks is expensive and time- consuming for humans. For domains re- quiring deep subject matter expertise such as law and medicine, treebanking is even more difficult. To reduce annotation costs for these domains, we develop methods to improve cross-domain parsing inference using paraphrases. Paraphrases are eas- ier to obtain than full syntactic analyses as they do not require deep linguistic knowl- edge, only linguistic fluency. A sentence and its paraphrase may have similar syn- tactic structures, allowing their parses to mutually inform each other. We present several methods to incorporate paraphrase information by jointly parsing a sentence with its paraphrase. These methods are ap- plied to state-of-the-art constituency and dependency parsers and provide signif- icant improvements across multiple do- mains. ",,,,ACL
119,2015,Cross-lingual Dependency Parsing Based on Distributed Representations,"Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng Wang","This paper investigates the problem of cross-lingual dependency parsing, aim- ing at inducing dependency parsers for low-resource languages while using only training data from a resource-rich lan- guage (e.g. English). Existing approaches typically don’t include lexical features, which are not transferable across lan- guages. In this paper, we bridge the lex- ical feature gap by using distributed fea- ture representations and their composition. We provide two algorithms for inducing cross-lingual distributed representations of words, which map vocabularies from two different languages into a common vector space. Consequently, both lexical features and non-lexical features can be used in our model for cross-lingual transfer. Furthermore, our framework is able to in- corporate additional useful features such as cross-lingual word clusters. Our com- bined contributions achieve an average rel- ative error reduction of 10.9% in labeled attachment score as compared with the delexicalized parser, trained on English universal treebank and transferred to three other languages. It also significantly out- performs McDonald et al. (2013) aug- mented with projected cluster features on identical data. ",,,,ACL
120,2015,Can Natural Language Processing Become Natural Language Coaching?,Marti A. Hearst,"How we teach and learn is undergoing a revolution, due to changes in technology and connectivity. Education may be one of the best application areas for advanced NLP techniques, and NLP researchers have much to contribute to this problem, especially in the areas of learning to write, mastery learning, and peer learning. In this paper I consider what happens when we convert natural language processors into natural language coaches. ",,,,ACL
121,2015,Machine Comprehension with Discourse Relations,"Karthik Narasimhan, Regina Barzilay","This paper proposes a novel approach for incorporating discourse information into machine comprehension applications. Traditionally, such information is com- puted using off-the-shelf discourse analyz- ers. This design provides limited oppor- tunities for guiding the discourse parser based on the requirements of the target task. In contrast, our model induces re- lations between sentences while optimiz- ing a task-specific objective. This ap- proach enables the model to benefit from discourse information without relying on explicit annotations of discourse structure during training. The model jointly iden- tifies relevant sentences, establishes rela- tions between them and predicts an an- swer. We implement this idea in a discrim- inative framework with hidden variables that capture relevant sentences and rela- tions unobserved during training. Our ex- periments demonstrate that the discourse aware model outperforms state-of-the-art machine comprehension systems. 1 ",,,,ACL
122,2015,Implicit Role Linking on Chinese Discourse: Exploiting Explicit Roles and Frame-to-Frame Relations,"Ru Li, Juan Wu, Zhiqiang Wang, Qinghua Chai","There is a growing interest in research- ing null instantiations, which are those implicit semantic arguments. Many of these implicit arguments can be linked to referents in context, and their discoveries are of great benefits to semantic process- ing. We address the issue of automat- ically identifying and resolving implicit arguments in Chinese discourse. For their resolutions, we present an approach that combines the information about overtly la- beled arguments and frame-to-frame rela- tions defined by FrameNet. Experimental results on our created corpus demonstrate the effectiveness of our approach. ",,,,ACL
123,2015,Discourse-sensitive Automatic Identification of Generic Expressions,"Annemarie Friedrich, Manfred Pinkal","This paper describes a novel sequence la- beling method for identifying generic ex- pressions, which refer to kinds or arbitrary members of a class, in discourse context. The automatic recognition of such expres- sions is important for any natural language processing task that requires text under- standing. Prior work has focused on iden- tifying generic noun phrases; we present a new corpus in which not only subjects but also clauses are annotated for generic- ity according to an annotation scheme mo- tivated by semantic theory. Our context- aware approach for automatically identi- fying generic expressions uses conditional random fields and outperforms previous work based on local decisions when evalu- ated on this corpus and on related data sets (ACE-2 and ACE-2005). ",,,,ACL
124,2015,Model-based Word Embeddings from Decompositions of Count Matrices,"Karl Stratos, Michael Collins, Daniel Hsu","This work develops a new statistical un- derstanding of word embeddings induced from transformed count data. Using the class of hidden Markov models (HMMs) underlying Brown clustering as a genera- tive model, we demonstrate how canoni- cal correlation analysis (CCA) and certain count transformations permit efficient and effective recovery of model parameters with lexical semantics. We further show in experiments that these techniques empir- ically outperform existing spectral meth- ods on word similarity and analogy tasks, and are also competitive with other pop- ular methods such as WORD2VEC and GLOVE. ",,,,ACL
125,2015,Entity Hierarchy Embedding,"Zhiting Hu, Poyao Huang, Yuntian Deng, Yingkai Gao","Existing distributed representations are limited in utilizing structured knowledge to improve semantic relatedness modeling. We propose a principled framework of em- bedding entities that integrates hierarchi- cal information from large-scale knowl- edge bases. The novel embedding model associates each category node of the hi- erarchy with a distance metric. To cap- ture structured semantics, the entity sim- ilarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and cat- egory distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. ",,,,ACL
126,2015,Orthogonality of Syntax and Semantics within Distributional Spaces,"Jeff Mitchell, Mark Steedman","A recent distributional approach to word- analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that per- formance on this task can be related to orthogonality within the space. Explic- itly designing such structure into a neu- ral network model results in represen- tations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within En- glish Wikipedia text to enable this de- composition can produce substantial im- provements on semantic-similarity, pos- induction and word-analogy tasks. ",,,,ACL
127,2015,Scalable Semantic Parsing with Partial Ontologies,"Eunsol Choi, Tom Kwiatkowski, Luke Zettlemoyer","We consider the problem of building scal- able semantic parsers for Freebase, and present a new approach for learning to do partial analyses that ground as much of the input text as possible without requiring that all content words be mapped to Freebase concepts. We study this problem on two newly introduced large-scale noun phrase datasets, and present a new semantic pars- ing model and semi-supervised learning approach for reasoning with partial onto- logical support. Experiments demonstrate strong performance on two tasks: refer- ring expression resolution and entity at- tribute extraction. In both cases, the par- tial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques. ",,,,ACL
128,2015,Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base,"Wen-tau Yih, Ming-Wei Chang, Xiaodong He, Jianfeng Gao","We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowl- edge base and can be directly mapped to a logical form. Semantic parsing is re- duced to query graph generation, formu- lated as a staged search problem. Unlike traditional approaches, our method lever- ages the knowledge base in an early stage to prune the search space and thus simpli- fies the semantic matching problem. By applying an advanced entity linking sys- tem and a deep convolutional neural net- work model that matches questions and predicate sequences, our system outper- forms previous methods substantially, and achieves an F 1 measure of 52.5% on the W EB Q UESTIONS dataset. ",,,,ACL
129,2015,Building a Semantic Parser Overnight,"Yushi Wang, Jonathan Berant, Percy Liang","How do we build a semantic parser in a new domain starting with zero training ex- amples? We introduce a new methodol- ogy for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canon- ical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We fur- ther study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate se- mantic parser in just a few hours. ",,,,ACL
130,2015,Predicting Polarities of Tweets by Composing Word Embeddings with Long Short-Term Memory,"Xin Wang, Yuanchao Liu, Chengjie Sun, Baoxun Wang","In this paper, we introduce Long Short- Term Memory (LSTM) recurrent network for twitter sentiment prediction. With the help of gates and constant error carousels in the memory block structure, the model could handle interactions between words through a flexible compositional function. Experiments on a public noisy labelled data show that our model outperforms sev- eral feature-engineering approaches, with the result comparable to the current best data-driven technique. According to the evaluation on a generated negation phrase test set, the proposed architecture dou- bles the performance of non-neural model based on bag-of-word features. Further- more, words with special functions (such as negation and transition) are distin- guished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expres- sion processing shows a promising poten- tial of the architecture dealing with com- plex sentiment phrases. ",,,,ACL
131,2015,Topic Modeling based Sentiment Analysis on Social Media for Stock Market Prediction,"Thien Hai Nguyen, Kiyoaki Shirai","The goal of this research is to build a model to predict stock price movement us- ing sentiments on social media. A new feature which captures topics and their sentiments simultaneously is introduced in the prediction model. In addition, a new topic model TSLDA is proposed to obtain this feature. Our method outperformed a model using only historical prices by about 6.07% in accuracy. Furthermore, when comparing to other sentiment anal- ysis methods, the accuracy of our method was also better than LDA and JST based methods by 6.43% and 6.07%. The results show that incorporation of the sentiment information from social media can help to improve the stock prediction. ",,,,ACL
132,2015,Learning Tag Embeddings and Tag-specific Composition Functions in Recursive Neural Network,"Qiao Qian, Bo Tian, Minlie Huang, Yang Liu","Recursive neural network is one of the most successful deep learning models for natural language processing due to the compositional nature of text. The model recursively composes the vector of a parent phrase from those of child words or phrases, with a key compo- nent named composition function. Al- though a variety of composition func- tions have been proposed, the syntactic information has not been fully encoded in the composition process. We pro- pose two models, Tag Guided RNN (TG- RNN for short) which chooses a compo- sition function according to the part-of- speech tag of a phrase, and Tag Embedded RNN/RNTN (TE-RNN/RNTN for short) which learns tag embeddings and then combines tag and word embeddings to- gether. In the fine-grained sentiment classification, experiment results show the proposed models obtain remarkable improvement: TG-RNN/TE-RNN obtain remarkable improvement over baselines, TE-RNTN obtains the second best result among all the top performing models, and all the proposed models have much less parameters/complexity than their counter- parts. ",,,,ACL
133,2015,A convex and feature-rich discriminative approach to dependency grammar induction,"Édouard Grave, Noémie Elhadad","In this paper, we introduce a new method for the problem of unsupervised depen- dency parsing. Most current approaches are based on generative models. Learning the parameters of such models relies on solving a non-convex optimization prob- lem, thus making them sensitive to initial- ization. We propose a new convex formu- lation to the task of dependency grammar induction. Our approach is discriminative, allowing the use of different kinds of fea- tures. We describe an efficient optimiza- tion algorithm to learn the parameters of our model, based on the Frank-Wolfe algo- rithm. Our method can easily be general- ized to other unsupervised learning prob- lems. We evaluate our approach on ten languages belonging to four different fam- ilies, showing that our method is competi- tive with other state-of-the-art methods. ",,,,ACL
134,2015,Parse Imputation for Dependency Annotations,"Jason Mielens, Liang Sun, Jason Baldridge","Syntactic annotation is a hard task, but it can be made easier by allowing annotators flexibility to leave aspects of a sentence underspecified. Unfortunately, partial an- notations are not typically directly usable for training parsers. We describe a method for imputing missing dependencies from sentences that have been partially anno- tated using the Graph Fragment Language, such that a standard dependency parser can then be trained on all annotations. We show that this strategy improves perfor- mance over not using partial annotations for English, Chinese, Portuguese and Kin- yarwanda, and that performance competi- tive with state-of-the-art unsupervised and weakly-supervised parsers can be reached with just a few hours of annotation. ",,,,ACL
135,2015,Probing the Linguistic Strengths and Limitations of Unsupervised Grammar Induction,"Yonatan Bisk, Julia Hockenmaier","Work in grammar induction should help shed light on the amount of syntactic struc- ture that is discoverable from raw word or tag sequences. But since most cur- rent grammar induction algorithms pro- duce unlabeled dependencies, it is diffi- cult to analyze what types of constructions these algorithms can or cannot capture, and, therefore, to identify where additional supervision may be necessary. This pa- per provides an in-depth analysis of the errors made by unsupervised CCG parsers by evaluating them against the labeled de- pendencies in CCGbank, hinting at new research directions necessary for progress in grammar induction. ",,,,ACL
136,2015,Entity-Centric Coreference Resolution with Model Stacking,"Kevin Clark, Christopher D. Manning","Mention pair models that predict whether or not two mentions are coreferent have historically been very effective for coref- erence resolution, but do not make use of entity-level information. However, we show that the scores produced by such models can be aggregated to define pow- erful entity-level features between clusters of mentions. Using these features, we train an entity-centric coreference system that learns an effective policy for building up coreference chains incrementally. The mention pair scores are also used to prune the search space the system works in, al- lowing for efficient training with an exact loss function. We evaluate our system on the English portion of the 2012 CoNLL Shared Task dataset and show that it im- proves over the current state of the art. ",,,,ACL
137,2015,Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution,"Sam Wiseman, Alexander M. Rush, Stuart Shieber, Jason Weston","We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representa- tions, and we report the best overall score on the CoNLL 2012 English test set to date. ",,,,ACL
138,2015,Transferring Coreference Resolvers with Posterior Regularization,André F. T. Martins,"We propose a cross-lingual framework for learning coreference resolvers for resource-poor target languages, given a re- solver in a source language. Our method uses word-aligned bitext to project infor- mation from the source to the target. To handle task-specific costs, we propose a softmax-margin variant of posterior regu- larization, and we use it to achieve robust- ness to projection errors. We show empir- ically that this strategy outperforms com- petitive cross-lingual methods, such as delexicalized transfer with bilingual word embeddings, bitext direct projection, and vanilla posterior regularization. ",,,,ACL
139,2015,Tea Party in the House: A Hierarchical Ideal Point Topic Model and Its Application to Republican Legislators in the 112th Congress,"Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik, Kristina Miler","We introduce the Hierarchical Ideal Point Topic Model, which provides a rich picture of policy issues, framing, and voting behav- ior using a joint model of votes, bill text, and the language that legislators use when debating bills. We use this model to look at the relationship between Tea Party Repub- licans and “establishment” Republicans in the U.S. House of Representatives during the 112 th Congress. ",,,,ACL
140,2015,"KB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations, and Facts","Dana Movshovitz-Attias, William W. Cohen","Many existing knowledge bases (KBs), in- cluding Freebase, Yago, and NELL, rely on a fixed ontology, given as an input to the system, which defines the data to be cataloged in the KB, i.e., a hierar- chy of categories and relations between them. The system then extracts facts that match the predefined ontology. We pro- pose an unsupervised model that jointly learns a latent ontological structure of an input corpus, and identifies facts from the corpus that match the learned structure. Our approach combines mixed member- ship stochastic block models and topic models to infer a structure by jointly mod- eling text, a latent concept hierarchy, and latent semantic relationships among the entities mentioned in the text. As a case study, we apply the model to a corpus of Web documents from the software do- main, and evaluate the accuracy of the var- ious components of the learned ontology. ",,,,ACL
141,2015,A Computationally Efficient Algorithm for Learning Topical Collocation Models,"Zhendong Zhao, Lan Du, Benjamin Börschinger, John K Pate","Most existing topic models make the bag- of-words assumption that words are gener- ated independently, and so ignore poten- tially useful information about word or- der. Previous attempts to use collocations (short sequences of adjacent words) in topic models have either relied on a pipe- line approach, restricted attention to bi- grams, or resulted in models whose infer- ence does not scale to large corpora. This paper studies how to simultaneously learn both collocations and their topic assign- ments. We present an efficient reformula- tion of the Adaptor Grammar-based topi- cal collocation model (AG-colloc) (John- son, 2010), and develop a point-wise sam- pling algorithm for posterior inference in this new formulation. We further improve the efficiency of the sampling algorithm by exploiting sparsity and parallelising in- ference. Experimental results derived in text classification, information retrieval and human evaluation tasks across a range of datasets show that this reformulation scales to hundreds of thousands of docu- ments while maintaining the good perfor- mance of the AG-colloc model. ",,,,ACL
142,2015,Compositional Semantic Parsing on Semi-Structured Tables,"Panupong Pasupat, Percy Liang","Two important aspects of semantic pars- ing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured ta- bles using question-answer pairs as super- vision. The central challenge arises from two compounding factors: the broader do- main results in an open-ended set of re- lations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improve- ments over natural baselines. For evalua- tion, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available. ",,,,ACL
143,2015,Graph parsing with s-graph grammars,"Jonas Groschwitz, Alexander Koller, Christoph Teichmann","A key problem in semantic parsing with graph-based semantic representations is graph parsing, i.e. computing all pos- sible analyses of a given graph accord- ing to a grammar. This problem arises in training synchronous string-to-graph grammars, and when generating strings from them. We present two algorithms for graph parsing (bottom-up and top-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementa- tions outperform the best previous system by several orders of magnitude. ",,,,ACL
144,2015,Sparse Overcomplete Word Vector Representations,"Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer","Current distributed representations of words show little resemblance to theo- ries of lexical semantics. The former are dense and uninterpretable, the lat- ter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We pro- pose methods that transform word vec- tors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are dis- covered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they out- perform the original vectors on benchmark tasks. ",,,,ACL
145,2015,Learning Semantic Word Embeddings based on Ordinal Knowledge Constraints,"Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling","In this paper, we propose a general frame- work to incorporate semantic knowledge into the popular data-driven learning pro- cess of word embeddings to improve the quality of them. Under this framework, we represent semantic knowledge as many ordinal ranking inequalities and formu- late the learning of semantic word embed- dings (SWE) as a constrained optimiza- tion problem, where the data-derived ob- jective function is optimized subject to all ordinal knowledge inequality constraints extracted from available knowledge re- sources such as Thesaurus and Word- Net. We have demonstrated that this con- strained optimization problem can be ef- ficiently solved by the stochastic gradient descent (SGD) algorithm, even for a large number of inequality constraints. Experi- mental results on four standard NLP tasks, including word similarity measure, sen- tence completion, name entity recogni- tion, and the TOEFL synonym selection, have all demonstrated that the quality of learned word vectors can be significantly improved after semantic knowledge is in- corporated as inequality constraints during the learning process of word embeddings. ",,,,ACL
146,2015,Adding Semantics to Data-Driven Paraphrasing,"Ellie Pavlick, Johan Bos, Malvina Nissim, Charley Beller","We add an interpretable semantics to the paraphrase database (PPDB). To date, the relationship between phrase pairs in the database has been weakly de- fined as approximately equivalent. We show that these pairs represent a vari- ety of relations, including directed entail- ment (little girl/girl) and exclusion (no- body/someone). We automatically assign semantic entailment relations to entries in PPDB using features derived from past work on discovering inference rules from text and semantic taxonomy induction. We demonstrate that our model assigns these relations with high accuracy. In a down- stream RTE task, our labels rival relations from WordNet and improve the coverage of a proof-based RTE system by 17%. ",,,,ACL
147,2015,Parsing as Reduction,"Daniel Fernández-González, André F. T. Martins","We reduce phrase-based parsing to depen- dency parsing. Our reduction is grounded on a new intermediate representation, “head-ordered dependency trees,” shown to be isomorphic to constituent trees. By encoding order information in the depen- dency labels, we show that any off-the- shelf, trainable dependency parser can be used to produce constituents. When this parser is non-projective, we can perform discontinuous parsing in a very natural manner. Despite the simplicity of our ap- proach, experiments show that the result- ing parsers are on par with strong base- lines, such as the Berkeley parser for En- glish and the best non-reranking system in the SPMRL-2014 shared task. Results are particularly striking for discontinuous parsing of German, where we surpass the current state of the art by a wide margin. ",,,,ACL
148,2015,Optimal Shift-Reduce Constituent Parsing with Structured Perceptron,"Le Quang Thang, Hiroshi Noji, Yusuke Miyao","We present a constituent shift-reduce parser with a structured perceptron that finds the optimal parse in a practical run- time. The key ideas are new feature tem- plates that facilitate state merging of dy- namic programming and A* search. Our system achieves 91.1 F1 on a standard English experiment, a level which cannot be reached by other beam-based systems even with large beam sizes. 1 ",,,,ACL
149,2015,"A Data-Driven, Factorization Parser for CCG Dependency Structures","Yantao Du, Weiwei Sun, Xiaojun Wan","This paper is concerned with building CCG-grounded, semantics-oriented deep dependency structures with a data-driven, factorization model. Three types of fac- torization together with different higher- order features are designed to capture different syntacto-semantic properties of functor-argument dependencies. Integrat- ing heterogeneous factorizations results in intractability in decoding. We pro- pose a principled method to obtain opti- mal graphs based on dual decomposition. Our parser obtains an unlabeled f-score of 93.23 on the CCGBank data, resulting in an error reduction of 6.5% over the best published result. which yields a signifi- cant improvement over the best published result in the literature. Our implementa- tion is available at http://www.icst. pku.edu.cn/lcwm/grass. ",,,,ACL
150,2015,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,"Kai Sheng Tai, Richard Socher, Christopher D. Manning","Because of their superior ability to pre- serve sequence information over time, Long Short-Term Memory (LSTM) net- works, a type of recurrent neural net- work with a more complex computational unit, have obtained strong results on a va- riety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syn- tactic properties that would naturally com- bine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree- LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Senti- ment Treebank). ",,,,ACL
151,2015,genCNN: A Convolutional Architecture for Word Sequence Prediction,"Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang","We propose a convolutional neural net- work, named gen CNN, for word se- quence prediction. Different from previous work on neural network- based language modeling and genera- tion (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. In- stead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feed- forward networks for language mod- eling, our model can effectively fuse the local correlation and global cor- relation in the word sequence, with a convolution-gating strategy specifi- cally designed for the task. We argue that our model can give adequate rep- resentation of the history, and there- fore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and read- ily parallelized. Our extensive exper- iments on text generation and n-best re-ranking in machine translation show that gen CNN outperforms the state-of- the-arts with big margins. ",,,,ACL
152,2015,Neural Responding Machine for Short-Text Conversation,"Lifeng Shang, Zhengdong Lu, Hang Li","We propose Neural Responding Ma- chine (NRM), a neural network-based re- sponse generator for Short-Text Conver- sation. NRM takes the general encoder- decoder framework: it formalizes the gen- eration of response as a decoding process based on the latent representation of the in- put text, while both encoding and decod- ing are realized with recurrent neural net- works (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging ser- vice. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state- of-the-arts in the same setting, including retrieval-based and SMT-based models. ",,,,ACL
153,2015,Abstractive Multi-Document Summarization via Phrase Selection and Merging,"Lidong Bing, Piji Li, Yi Liao, Wai Lam","We propose an abstraction-based multi- document summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sen- tences, namely, noun/verb phrases. Dif- ferent from existing abstraction-based ap- proaches, our method first constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to max- imize the salience of phrases and mean- while satisfy the sentence construction constraints. We employ integer linear op- timization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-of- the-art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation. ",,,,ACL
154,2015,Joint Graphical Models for Date Selection in Timeline Summarization,"Giang Tran, Eelco Herder, Katja Markert","Automatic timeline summarization (TLS) generates precise, dated overviews over (often prolonged) events, such as wars or economic crises. One subtask of TLS se- lects the most important dates for an event within a certain time frame. Date selec- tion has up to now been handled via su- pervised machine learning approaches that estimate the importance of each date sepa- rately, using features such as the frequency of date mentions in news corpora. This ap- proach neglects interactions between dif- ferent dates that occur due to connections between subevents. We therefore suggest a joint graphical model for date selection. Even unsupervised versions of this model perform as well as supervised state-of-the- art approaches. With parameter tuning on training data, it outperforms prior super- vised models by a considerable margin. ",,,,ACL
155,2015,Predicting Salient Updates for Disaster Summarization,"Chris Kedzie, Kathleen McKeown, Fernando Diaz","During crises such as natural disasters or other human tragedies, information needs of both civilians and responders often re- quire urgent, specialized treatment. Moni- toring and summarizing a text stream dur- ing such an event remains a difficult prob- lem. We present a system for update sum- marization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence se- lection, increasing the quality of the up- dates. We use novel, disaster-specific features for salience prediction, including geo-locations and language models repre- senting the language of disaster. Our eval- uation on a standard set of retrospective events using ROUGE shows that salience prediction provides a significant improve- ment over other approaches. ",,,,ACL
156,2015,Unsupervised Prediction of Acceptability Judgements,"Jey Han Lau, Alexander Clark, Shalom Lappin","In this paper we present the task of un- supervised prediction of speakers’ accept- ability judgements. We use a test set generated from the British National Cor- pus ( BNC ) containing both grammatical sentences and sentences containing a va- riety of syntactic infelicities introduced by round trip machine translation. This set was annotated for acceptability judge- ments through crowd sourcing. We trained a variety of unsupervised language mod- els on the original BNC , and tested them to see the extent to which they could pre- dict mean speakers’ judgements on the test set. To map probability to acceptability, we experimented with several normalisa- tion functions to neutralise the effects of sentence length and word frequencies. We found encouraging results with the unsu- pervised models predicting acceptability across two different datasets. Our method- ology is highly portable to other domains and languages, and the approach has po- tential implications for the representation and the acquisition of linguistic knowl- edge. ",,,,ACL
157,2015,A Frame of Mind: Using Statistical Models for Detection of Framing and Agenda Setting Campaigns,"Oren Tsur, Dan Calacci, David Lazer","Framing is a sophisticated form of dis- course in which the speaker tries to in- duce a cognitive bias through consis- tent linkage between a topic and a spe- cific context (frame). We build on po- litical science and communication theory and use probabilistic topic models com- bined with time series regression analy- sis (autoregressive distributed-lag models) to gain insights about the language dy- namics in the political processes. Pro- cessing four years of public statements is- sued by members of the U.S. Congress, our results provide a glimpse into the com- plex dynamic processes of framing, atten- tion shifts and agenda setting, commonly known as ‘spin’. We further provide new evidence for the divergence in party disci- pline in U.S. politics. ",,,,ACL
158,2015,Why discourse affects speakers’ choice of referring expressions,"Naho Orita, Eliana Vornov, Naomi Feldman, Hal Daumé III","We propose a language production model that uses dynamic discourse information to account for speakers’ choices of refer- ring expressions. Our model extends pre- vious rational speech act models (Frank and Goodman, 2012) to more naturally dis- tributed linguistic data, instead of assuming a controlled experimental setting. Simula- tions show a close match between speakers’ utterances and model predictions, indicat- ing that speakers’ behavior can be modeled in a principled way by considering the prob- abilities of referents in the discourse and the information conveyed by each word. ",,,,ACL
159,2015,Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy Game,"Vlad Niculae, Srijan Kumar, Jordan Boyd-Graber, Cristian Danescu-Niculescu-Mizil","Interpersonal relations are fickle, with close friendships often dissolving into en- mity. In this work, we explore linguis- tic cues that presage such transitions by studying dyadic interactions in an on- line strategy game where players form al- liances and break those alliances through betrayal. We characterize friendships that are unlikely to last and examine temporal patterns that foretell betrayal. We reveal that subtle signs of imminent betrayal are encoded in the conversational patterns of the dyad, even if the victim is not aware of the relationship’s fate. In particular, we find that lasting friend- ships exhibit a form of balance that man- ifests itself through language. In contrast, sudden changes in the balance of certain conversational attributes—such as positive sentiment, politeness, or focus on future planning—signal impending betrayal. ",,,,ACL
160,2015,Who caught a cold ? - Identifying the subject of a symptom,"Shin Kanouchi, Mamoru Komachi, Naoaki Okazaki, Eiji Aramaki","The development and proliferation of so- cial media services has led to the emer- gence of new approaches for surveying the population and addressing social issues. One popular application of social media data is health surveillance, e.g., predicting the outbreak of an epidemic by recogniz- ing diseases and symptoms from text mes- sages posted on social media platforms. In this paper, we propose a novel task that is crucial and generic from the viewpoint of health surveillance: estimating a sub- ject (carrier) of a disease or symptom men- tioned in a Japanese tweet. By designing an annotation guideline for labeling the subject of a disease/symptom in a tweet, we perform annotations on an existing cor- pus for public surveillance. In addition, we present a supervised approach for pre- dicting the subject of a disease/symptom. The results of our experiments demon- strate the impact of subject identification on the effective detection of an episode of a disease/symptom. Moreover, the results suggest that our task is independent of the type of disease/symptom. ",,,,ACL
161,2015,Weakly Supervised Role Identification in Teamwork Interactions,"Diyi Yang, Miaomiao Wen, Carolyn Rosé","In this paper, we model conversational roles in terms of distributions of turn level behaviors, including conversation acts and stylistic markers, as they occur over the whole interaction. This work presents a lightly supervised approach to inducing role definitions over sets of contributions within an extended interaction, where the supervision comes in the form of an out- come measure from the interaction. The identified role definitions enable a map- ping from behavior profiles of each par- ticipant in an interaction to limited sized feature vectors that can be used effectively to predict the teamwork outcome. An em- pirical evaluation applied to two Massive Open Online Course (MOOCs) datasets demonstrates that this approach yields su- perior performance in learning representa- tions for predicting the teamwork outcome over several baselines. ",,,,ACL
162,2015,Deep Unordered Composition Rivals Syntactic Methods for Text Classification,"Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, Hal Daumé III","Many existing deep learning models for natural language processing tasks focus on learning the compositionality of their in- puts, which requires many expensive com- putations. We present a simple deep neural network that competes with and, in some cases, outperforms such models on sen- timent analysis and factoid question an- swering tasks while taking only a fraction of the training time. While our model is syntactically-ignorant, we show significant improvements over previous bag-of-words models by deepening our network and ap- plying a novel variant of dropout. More- over, our model performs better than syn- tactic models on datasets with high syn- tactic variance. We show that our model makes similar errors to syntactically-aware models, indicating that for the tasks we con- sider, nonlinearly transforming the input is more important than tailoring a network to incorporate word order and syntax. ",,,,ACL
163,2015,SOLAR: Scalable Online Learning Algorithms for Ranking,"Jialei Wang, Ji Wan, Yongdong Zhang, Steven Hoi","Traditional learning to rank methods learn ranking models from training data in a batch and offline learning mode, which suffers from some critical limitations, e.g., poor scalability as the model has to be re- trained from scratch whenever new train- ing data arrives. This is clearly non- scalable for many real applications in practice where training data often arrives sequentially and frequently. To overcome the limitations, this paper presents SO- LAR — a new framework of Scalable On- line Learning Algorithms for Ranking, to tackle the challenge of scalable learning to rank. Specifically, we propose two novel SOLAR algorithms and analyze their IR measure bounds theoretically. We conduct extensive empirical studies by comparing our SOLAR algorithms with conventional learning to rank algorithms on benchmark testbeds, in which promising results vali- date the efficacy and scalability of the pro- posed novel SOLAR algorithms. ",,,,ACL
164,2015,Text Categorization as a Graph Classification Problem,"François Rousseau, Emmanouil Kiagias, Michalis Vazirgiannis","In this paper, we consider the task of text categorization as a graph classifica- tion problem. By representing textual doc- uments as graph-of-words instead of his- torical n-gram bag-of-words, we extract more discriminative features that corre- spond to long-distance n-grams through frequent subgraph mining. Moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its dens- est part – its main core – speeding up the feature extraction step for little to no cost in prediction performances. Experiments on four standard text classification datasets show statistically significant higher accu- racy and macro-averaged F1-score com- pared to baseline approaches. ",,,,ACL
165,2015,Inverted indexing for cross-lingual NLP,"Anders Søgaard, Željko Agić, Héctor Martínez Alonso, Barbara Plank","We present a novel, count-based approach to obtaining inter-lingual word represen- tations based on inverted indexing of Wikipedia. We present experiments ap- plying these representations to 17 datasets in document classification, POS tagging, dependency parsing, and word alignment. Our approach has the advantage that it is simple, computationally efficient and almost parameter-free, and, more im- portantly, it enables multi-source cross- lingual learning. In 14/17 cases, we im- prove over using state-of-the-art bilingual embeddings. ",,,,ACL
166,2015,Multi-Task Learning for Multiple Language Translation,"Daxiang Dong, Hua Wu, Wei He, Dianhai Yu","In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available. ",,,,ACL
167,2015,Accurate Linear-Time Chinese Word Segmentation via Embedding Matching,"Jianqiang Ma, Erhard Hinrichs","This paper proposes an embedding match- ing approach to Chinese word segmenta- tion, which generalizes the traditional se- quence labeling framework and takes ad- vantage of distributed representations. The training and prediction algorithms have linear-time complexity. Based on the pro- posed model, a greedy segmenter is de- veloped and evaluated on benchmark cor- pora. Experiments show that our greedy segmenter achieves improved results over previous neural network-based word seg- menters, and its performance is competi- tive with state-of-the-art methods, despite its simple feature set and the absence of ex- ternal resources for training. ",,,,ACL
168,2015,Gated Recursive Neural Network for Chinese Word Segmentation,"Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Xuanjing Huang","Recently, neural network models for natu- ral language processing tasks have been in- creasingly focused on for their ability of al- leviating the burden of manual feature en- gineering. However, the previous neural models cannot extract the complicated fea- ture compositions as the traditional meth- ods with discrete features. In this paper, we propose a gated recursive neural net- work (GRNN) for Chinese word segmen- tation, which contains reset and update gates to incorporate the complicated com- binations of the context characters. Since GRNN is relative deep, we also use a supervised layer-wise training method to avoid the problem of gradient diffusion. Experiments on the benchmark datasets show that our model outperforms the pre- vious neural network models as well as the state-of-the-art methods. ",,,,ACL
169,2015,An analysis of the user occupational class through Twitter content,"Daniel Preoţiuc-Pietro, Vasileios Lampos, Nikolaos Aletras","Social media content can be used as a complementary source to the traditional methods for extracting and studying col- lective social attributes. This study focuses on the prediction of the occupational class for a public user profile. Our analysis is conducted on a new annotated corpus of Twitter users, their respective job titles, posted textual content and platform-related attributes. We frame our task as classifi- cation using latent feature representations such as word clusters and embeddings. The employed linear and, especially, non-linear methods can predict a user’s occupational class with strong accuracy for the coars- est level of a standard occupation taxon- omy which includes nine classes. Com- bined with a qualitative assessment, the derived results confirm the feasibility of our approach in inferring a new user at- tribute that can be embedded in a multitude of downstream applications. ",,,,ACL
170,2015,Tracking unbounded Topic Streams,"Dominik Wurzer, Victor Lavrenko, Miles Osborne","Tracking topics on social media streams is non-trivial as the number of topics men- tioned grows without bound. This com- plexity is compounded when we want to track such topics against other fast mov- ing streams. We go beyond traditional small scale topic tracking and consider a stream of topics against another document stream. We introduce two tracking ap- proaches which are fully applicable to true streaming environments. When tracking 4.4 million topics against 52 million doc- uments in constant time and space, we demonstrate that counter to expectations, simple single-pass clustering can outper- form locality sensitive hashing for nearest neighbour search on streams. ",,,,ACL
171,2015,Inducing Word and Part-of-Speech with Pitman-Yor Hidden Semi-Markov Models,"Kei Uchiumi, Hiroshi Tsukahara, Daichi Mochihashi","We propose a nonparametric Bayesian model for joint unsupervised word seg- mentation and part-of-speech tagging from raw strings. Extending a previous model for word segmentation, our model is called a Pitman-Yor Hidden Semi- Markov Model (PYHSMM) and consid- ered as a method to build a class n-gram language model directly from strings, while integrating character and word level information. Experimental results on stan- dard datasets on Japanese, Chinese and Thai revealed it outperforms previous re- sults to yield the state-of-the-art accura- cies. This model will also serve to analyze a structure of a language whose words are not identified a priori. ",,,,ACL
172,2015,Coupled Sequence Labeling on Heterogeneous Annotations: POS Tagging as a Case Study,"Zhenghua Li, Jiayuan Chao, Min Zhang, Wenliang Chen","In order to effectively utilize multiple datasets with heterogeneous annotations, this paper proposes a coupled sequence labeling model that can directly learn and infer two heterogeneous annotations simultaneously, and to facilitate discussion we use Chinese part-of- speech (POS) tagging as our case study. The key idea is to bundle two sets of POS tags together (e.g. “ [NN, n ]”), and build a conditional random field (CRF) based tagging model in the enlarged space of bundled tags with the help of ambiguous labelings. To train our model on two non-overlapping datasets that each has only one-side tags, we transform a one-side tag into a set of bundled tags by considering all possible mappings at the missing side and derive an objective function based on ambiguous labelings. The key advantage of our coupled model is to provide us with the flexibility of 1) incorporating joint features on the bundled tags to implicitly learn the loose mapping between heterogeneous annotations, and 2) exploring separate features on one-side tags to overcome the data sparseness problem of using only bundled tags. Experiments on benchmark datasets show that our coupled model significantly outperforms the state-of- the-art baselines on both one-side POS tagging and annotation conversion tasks. The codes and newly annotated data are released for non-commercial usage. 1 ? Correspondence author. 1 http://hlt.suda.edu.cn/ ? zhli ",,,,ACL
173,2015,AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes,"Sascha Rothe, Hinrich Schütze","We present AutoExtend, a system to learn embeddings for synsets and lexemes. It is flexible in that it can take any word embed- dings as input and does not need an addi- tional training corpus. The synset/lexeme embeddings obtained live in the same vec- tor space as the word embeddings. A sparse tensor formalization guarantees ef- ficiency and parallelizability. We use WordNet as a lexical resource, but Auto- Extend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disam- biguation tasks. ",,,,ACL
174,2015,Improving Evaluation of Machine Translation Quality Estimation,Yvette Graham,"Quality estimation evaluation commonly takes the form of measurement of the error that exists between predictions and gold standard labels for a particular test set of translations. Issues can arise during com- parison of quality estimation prediction score distributions and gold label distribu- tions, however. In this paper, we provide an analysis of methods of comparison and identify areas of concern with respect to widely used measures, such as the ability to gain by prediction of aggregate statistics specific to gold label distributions or by optimally conservative variance in predic- tion score distributions. As an alternative, we propose the use of the unit-free Pear- son correlation, in addition to providing an appropriate method of significance testing improvements over a baseline. Compo- nents of W MT -13 and W MT -14 quality es- timation shared tasks are replicated to re- veal substantially increased conclusivity in system rankings, including identification of outright winners of tasks. ",,,,ACL
1,2016,Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing,"James Goodman, Andreas Vlachos, Jason Naradowsky","Semantic parsers map natural language statements into meaning representations, and must abstract over syntactic phenom- ena, resolve anaphora, and identify word senses to eliminate ambiguous interpre- tations. Abstract meaning representation (AMR) is a recent example of one such semantic formalism which, similar to a de- pendency parse, utilizes a graph to repre- sent relationships between concepts (Ba- narescu et al., 2013). As with dependency parsing, transition-based approaches are a common approach to this problem. How- ever, when trained in the traditional man- ner these systems are susceptible to the ac- cumulation of errors when they find un- desirable states during greedy decoding. Imitation learning algorithms have been shown to help these systems recover from such errors. To effectively use these meth- ods for AMR parsing we find it highly beneficial to introduce two novel exten- sions: noise reduction and targeted explo- ration. The former mitigates the noise in the feature representation, a result of the complexity of the task. The latter targets the exploration steps of imitation learning towards areas which are likely to provide the most information in the context of a large action-space. We achieve state-of- the art results, and improve upon standard transition-based parsing by 4.7 F 1 points. ",,,,ACL
2,2016,Data Recombination for Neural Semantic Parsing,"Robin Jia, Percy Liang","Modeling crisp logical regularities is cru- cial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recom- bination, a novel framework for inject- ing such prior knowledge into a model. From the training data, we induce a high- precision synchronous context-free gram- mar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sam- pled from this grammar, thereby teaching the model about these structural proper- ties. Data recombination improves the ac- curacy of our RNN model on three se- mantic parsing datasets, leading to new state-of-the-art performance on the stan- dard GeoQuery dataset for models with comparable supervision. ",,,,ACL
3,2016,Inferring Logical Forms From Denotations,"Panupong Pasupat, Percy Liang","A core problem in learning semantic parsers from denotations is picking out consistent logical forms—those that yield the correct denotation—from a combina- torially large space. To control the search space, previous work relied on restricted set of rules, which limits expressivity. In this paper, we consider a much more ex- pressive class of logical forms, and show how to use dynamic programming to effi- ciently represent the complete set of con- sistent logical forms. Expressivity also introduces many more spurious logical forms which are consistent with the cor- rect denotation but do not represent the meaning of the utterance. To address this, we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms. On the W IKI T ABLE Q UESTIONS dataset, we increase the coverage of answerable ques- tions from 53.5% to 76%, and the ad- ditional crowdsourced supervision lets us rule out 92.1% of spurious logical forms. ",,,,ACL
4,2016,Language to Logical Form with Neural Attention,"Li Dong, Mirella Lapata","Semantic parsing aims at mapping nat- ural language to machine interpretable meaning representations. Traditional ap- proaches rely on high-quality lexicons, manually-built templates, and linguis- tic features which are either domain- or representation-specific. In this pa- per we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vec- tors. Experimental results on four datasets show that our approach performs compet- itively without using hand-engineered fea- tures and is easy to adapt across domains and meaning representations. ",,,,ACL
5,2016,Unsupervised Person Slot Filling based on Graph Mining,"Dian Yu, Heng Ji","Slot filling aims to extract the values (slot fillers) of specific attributes (slots types) for a given entity (query) from a large- scale corpus. Slot filling remains very challenging over the past seven years. We propose a simple yet effective unsuper- vised approach to extract slot fillers based on the following two observations: (1) a trigger is usually a salient node relative to the query and filler nodes in the depen- dency graph of a context sentence; (2) a relation is likely to exist if the query and candidate filler nodes are strongly con- nected by a relation-specific trigger. Thus we design a graph-based algorithm to au- tomatically identify triggers based on per- sonalized PageRank and Affinity Prop- agation for a given (query, filler) pair and then label the slot type based on the identified triggers. Our approach achieves 11.6%-25% higher F-score over state-of- the-art English slot filling methods. Our experiments also demonstrate that as long as a few trigger seeds, name tagging and dependency parsing capabilities exist, this approach can be quickly adapted to any language and new slot types. Our promis- ing results on Chinese slot filling can serve as a new benchmark. ",,,,ACL
6,2016,A Multi-media Approach to Cross-lingual Entity Knowledge Transfer,"Di Lu, Xiaoman Pan, Nima Pourdamghani, Shih-Fu Chang","When a large-scale incident or disaster oc- curs, there is often a great demand for rapidly developing a system to extract detailed and new information from low- resource languages (LLs). We propose a novel approach to discover compara- ble documents in high-resource languages (HLs), and project Entity Discovery and Linking results from HLs documents back to LLs. We leverage a wide variety of language-independent forms from multi- ple data modalities, including image pro- cessing (image-to-image retrieval, visual similarity and face recognition) and sound matching. We also propose novel meth- ods to learn entity priors from a large-scale HL corpus and knowledge base. Using Hausa and Chinese as the LLs and En- glish as the HL, experiments show that our approach achieves 36.1% higher Hausa name tagging F-score over a costly super- vised model, and 9.4% higher Chinese- to-English Entity Linking accuracy over state-of-the-art. ",,,,ACL
7,2016,Models and Inference for Prefix-Constrained Machine Translation,"Joern Wuebker, Spence Green, John DeNero, Saša Hasan","We apply phrase-based and neural models to a core task in interactive machine trans- lation: suggesting how to complete a par- tial translation. For the phrase-based sys- tem, we demonstrate improvements in sug- gestion quality using novel objective func- tions, learning techniques, and inference algorithms tailored to this task. Our con- tributions include new tunable metrics, an improved beam search strategy, an n-best extraction method that increases sugges- tion diversity, and a tuning procedure for a hierarchical joint model of alignment and translation. The combination of these tech- niques improves next-word suggestion accu- racy dramatically from 28.5% to 41.2% in a large-scale English-German experiment. Our recurrent neural translation system in- creases accuracy yet further to 53.0%, but inference is two orders of magnitude slower. Manual error analysis shows the strengths and weaknesses of both approaches. ",,,,ACL
8,2016,Modeling Coverage for Neural Machine Translation,"Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu","Attention mechanism has enhanced state- of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vec- tor is fed to the attention model to help ad- just future attention, which lets NMT sys- tem to consider more about untranslated source words. Experiments show that the proposed approach significantly im- proves both translation quality and align- ment quality over standard attention-based NMT. 1 ",,,,ACL
9,2016,Improving Neural Machine Translation Models with Monolingual Data,"Rico Sennrich, Barry Haddow, Alexandra Birch","Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only us- ing parallel data for training. Target- side monolingual data plays an impor- tant role in boosting fluency for phrase- based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with sep- arately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolin- gual data without changing the neural net- work architecture. By pairing monolin- gual training data with an automatic back- translation, we can treat it as additional parallel training data, and we obtain sub- stantial improvements on the WMT 15 task English?German (+2.8–3.7 B LEU ), and for the low-resourced IWSLT 14 task Turkish→English (+2.1–3.4 B LEU ), ob- taining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives sub- stantial improvements for the IWSLT 15 task English→German. ",,,,ACL
10,2016,Graph-Based Translation Via Graph Segmentation,"Liangyou Li, Andy Way, Qun Liu","One major drawback of phrase-based translation is that it segments an input sen- tence into continuous phrases. To sup- port linguistically informed source discon- tinuity, in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based trans- lation model. The model segments an input graph into connected subgraphs, each of which may cover a discontinuous phrase. We use beam search to combine translations of each subgraph left-to-right to produce a complete translation. Experi- ments on Chinese–English and German– English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores. By explicitly modeling the graph segmen- tation, our system obtains further improve- ment, especially on German–English. ",,,,ACL
11,2016,Incremental Acquisition of Verb Hypothesis Space towards Physical World Interaction,"Lanbo She, Joyce Chai","As a new generation of cognitive robots start to enter our lives, it is important to enable robots to follow human commands and to learn new actions from human lan- guage instructions. To address this issue, this paper presents an approach that ex- plicitly represents verb semantics through hypothesis spaces of fluents and automat- ically acquires these hypothesis spaces by interacting with humans. The learned hy- pothesis spaces can be used to automati- cally plan for lower-level primitive actions towards physical world interaction. Our empirical results have shown that the rep- resentation of a hypothesis space of flu- ents, combined with the learned hypothe- sis selection algorithm, outperforms a pre- vious baseline. In addition, our approach applies incremental learning, which can contribute to life-long learning from hu- mans in the future. ",,,,ACL
12,2016,Language Transfer Learning for Supervised Lexical Substitution,"Gerold Hintz, Chris Biemann","We propose a framework for lexical sub- stitution that is able to perform transfer learning across languages. Datasets for this task are available in at least three languages (English, Italian, and German). Previous work has addressed each of these tasks in isolation. In contrast, we regard the union of three shared tasks as a com- bined multilingual dataset. We show that a supervised system can be trained effec- tively, even if training and evaluation data are from different languages. Successful transfer learning between languages sug- gests that the learned model is in fact in- dependent of the underlying language. We combine state-of-the-art unsupervised fea- tures obtained from syntactic word em- beddings and distributional thesauri in a supervised delexicalized ranking system. Our system improves over state of the art in the full lexical substitution task in all three languages. ",,,,ACL
13,2016,Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning,"Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Brian MacWhinney","We use Bayesian optimization to learn curricula for word representation learning, optimizing performance on downstream tasks that depend on the learned represen- tations as features. The curricula are mod- eled by a linear ranking function which is the scalar product of a learned weight vec- tor and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus. We show that learning the curricu- lum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order. ",,,,ACL
14,2016,Pointing the Unknown Words,"Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou","The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP sys- tems, including traditional count-based and deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models us- ing attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sen- tence, and the other predicts a word in the shortlist vocabulary. At each timestep, the decision of which softmax layer to use is adaptively made by an MLP which is con- ditioned on the context. We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the envi- ronment when the name of an object is not known. Using our proposed model, we ob- serve improvements on two tasks, neural machine translation on the Europarl En- glish to French parallel corpora and text summarization on the Gigaword dataset. ",,,,ACL
15,2016,Generalized Transition-based Dependency Parsing via Control Parameters,"Bernd Bohnet, Ryan McDonald, Emily Pitler, Ji Ma","In this paper, we present a generalized transition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain tran- sitions between parser states. This gener- alization provides a unified framework to describe and compare various transition- based parsing approaches from both a the- oretical and empirical perspective. This includes well-known transition systems, but also previously unstudied systems. ",,,,ACL
16,2016,A Transition-Based System for Joint Lexical and Syntactic Analysis,"Matthieu Constant, Joakim Nivre","We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words: a syntactic dependency tree and a forest of lexical units including multiword expres- sions (MWEs). This combined represen- tation allows us to capture both the syn- tactic and semantic structure of MWEs, which in turn enables deeper downstream semantic analysis, especially for semi- compositional MWEs. The proposed sys- tem extends the arc-standard transition system for dependency parsing with tran- sitions for building complex lexical units. Experiments on two different data sets show that the approach significantly im- proves MWE identification accuracy (and sometimes syntactic accuracy) compared to existing joint approaches. ",,,,ACL
17,2016,Neural Greedy Constituent Parsing with Dynamic Oracles,"Maximin Coavoux, Benoît Crabbé","Dynamic oracle training has shown sub- stantial improvements for dependency parsing in various settings, but has not been explored for constituent parsing. The present article introduces a dynamic ora- cle for transition-based constituent pars- ing. Experiments on the 9 languages of the SPMRL dataset show that a neu- ral greedy parser with morphological fea- tures, trained with a dynamic oracle, leads to accuracies comparable with the best non-reranking and non-ensemble parsers. ",,,,ACL
18,2016,Literal and Metaphorical Senses in Compositional Distributional Semantic Models,"E.Dario Gutiérrez, Ekaterina Shutova, Tyler Marghetis, Benjamin Bergen","Metaphorical expressions are pervasive in natural language and pose a substan- tial challenge for computational seman- tics. The inherent compositionality of metaphor makes it an important test case for compositional distributional semantic models (CDSMs). This paper is the first to investigate whether metaphorical compo- sition warrants a distinct treatment in the CDSM framework. We propose a method to learn metaphors as linear transforma- tions in a vector space and find that, across a variety of semantic domains, explicitly modeling metaphor improves the result- ing semantic representations. We then use these representations in a metaphor iden- tification task, achieving a high perfor- mance of 0.82 in terms of F-score. ",,,,ACL
19,2016,Idiom Token Classification using Sentential Distributed Semantics,"Giancarlo Salton, Robert Ross, John Kelleher","Idiom token classification is the task of deciding for a set of potentially idiomatic phrases whether each occurrence of a phrase is a literal or idiomatic usage of the phrase. In this work we explore the use of Skip-Thought Vectors to create dis- tributed representations that encode fea- tures that are predictive with respect to id- iom token classification. We show that classifiers using these representations have competitive performance compared with the state of the art in idiom token classifi- cation. Importantly, however, our models use only the sentence containing the tar- get phrase as input and are thus less de- pendent on a potentially inaccurate or in- complete model of discourse context. We further demonstrate the feasibility of using these representations to train a competitive general idiom token classifier. ",,,,ACL
20,2016,Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings,"Kazuma Hashimoto, Yoshimasa Tsuruoka","We present a novel method for jointly learning compositional and non- compositional phrase embeddings by adaptively weighting both types of em- beddings using a compositionality scoring function. The scoring function is used to quantify the level of compositionality of each phrase, and the parameters of the function are jointly optimized with the ob- jective for learning phrase embeddings. In experiments, we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases, and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality, substantially outperforming the previous state of the art. Moreover, our embeddings improve upon the previous best model on a transitive verb disambiguation task. We also show that a simple ensemble technique further improves the results for both tasks. ",,,,ACL
21,2016,"Metaphor Detection with Topic Transition, Emotion and Cognition in Context","Hyeju Jang, Yohan Jo, Qinlan Shen, Michael Miller","Metaphor is a common linguistic tool in communication, making its detection in discourse a crucial task for natural lan- guage understanding. One popular ap- proach to this challenge is to capture se- mantic incohesion between a metaphor and the dominant topic of the surrounding text. While these methods are effective, they tend to overclassify target words as metaphorical when they deviate in mean- ing from its context. We present a new approach that (1) distinguishes literal and non-literal use of target words by exam- ining sentence-level topic transitions and (2) captures the motivation of speakers to express emotions and abstract concepts metaphorically. Experiments on an on- line breast cancer discussion forum dataset demonstrate a significant improvement in metaphor detection over the state-of-the- art. These experimental results also re- veal a tendency toward metaphor usage in personal topics and certain emotional con- texts. ",,,,ACL
22,2016,Compressing Neural Language Models by Sparse Word Representations,"Yunchuan Chen, Lili Mou, Yan Xu, Ge Li","Neural networks are among the state-of- the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such ap- proaches are time- and memory-intensive because of the large numbers of parame- ters for word embeddings and the output layer. In this paper, we propose to com- press neural language models by sparse word representations. In the experiments, the number of parameters in our model in- creases very slowly with the growth of the vocabulary size, which is almost imper- ceptible. Moreover, our approach not only reduces the parameter space to a large ex- tent, but also improves the performance in terms of the perplexity measure. 1 ",,,,ACL
23,2016,Intrinsic Subspace Evaluation of Word Embedding Representations,"Yadollah Yaghoobzadeh, Hinrich Schütze","We introduce a new methodology for in- trinsic evaluation of word representations. Specifically, we identify four fundamen- tal criteria based on the characteristics of natural language that pose difficulties to NLP systems; and develop tests that di- rectly show whether or not representations contain the subspaces necessary to satisfy these criteria. Current intrinsic evalua- tions are mostly based on the overall simi- larity or full-space similarity of words and thus view vector representations as points. We show the limits of these point-based intrinsic evaluations. We apply our evalu- ation methodology to the comparison of a count vector model and several neural net- work models and demonstrate important properties of these models. ",,,,ACL
24,2016,On the Role of Seed Lexicons in Learning Bilingual Word Embeddings,"Ivan Vulić, Anna Korhonen","A shared bilingual word embedding space (SBWES) is an indispensable resource in a variety of cross-language NLP and IR tasks. A common approach to the SB- WES induction is to learn a mapping func- tion between monolingual semantic spaces, where the mapping critically relies on a seed word lexicon used in the learning pro- cess. In this work, we analyze the impor- tance and properties of seed lexicons for the SBWES induction across different di- mensions (i.e., lexicon source, lexicon size, translation method, translation pair relia- bility). On the basis of our analysis, we propose a simple but effective hybrid bilin- gual word embedding (BWE) model. This model (HYBWE) learns the mapping be- tween two monolingual embedding spaces using only highly reliable symmetric trans- lation pairs from a seed document-level embedding space. We perform bilingual lexicon learning (BLL) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models, all of which use more expensive bilingual signals. Effectively, we demonstrate that a SBWES may be in- duced by leveraging only a very weak bilin- gual signal (document alignments) along with monolingual data. ",,,,ACL
25,2016,Liberal Event Extraction and Event Schema Induction,"Lifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng Ji","Meaning Repre- sentation) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simulta- neously extract event types and argument roles and discover an event schema. Ex- periments on general and specific domains demonstrate that this framework can con- struct high-quality schemas with many event and argument role types, covering a high proportion of event types and argu- ment roles in manually defined schemas. We show that extraction performance us- ing discovered schemas is comparable to supervised models trained from a large amount of data labeled according to pre- defined event types. The extraction quality of new event types is also promising. ",,,,ACL
26,2016,Jointly Event Extraction and Visualization on Twitter via Probabilistic Modelling,"Deyu Zhou, Tianmeng Gao, Yulan He","Event extraction from texts aims to de- tect structured information such as what has happened, to whom, where and when. Event extraction and visualization are typ- ically considered as two different tasks. In this paper, we propose a novel approach based on probabilistic modelling to joint- ly extract and visualize events from tweet- s where both tasks benefit from each oth- er. We model each event as a joint dis- tribution over named entities, a date, a lo- cation and event-related keywords. More- over, both tweets and event instances are associated with coordinates in the visual- ization space. The manifold assumption that the intrinsic geometry of tweets is a low-rank, non-linear manifold within the high-dimensional space is incorporated in- to the learning framework using a regu- larization. Experimental results show that the proposed approach can effectively deal with both event extraction and visualiza- tion and performs remarkably better than both the state-of-the-art event extraction method and a pipeline approach for event extraction and visualization. ",,,,ACL
27,2016,Using Sentence-Level LSTM Language Models for Script Inference,"Karl Pichotta, Raymond J. Mooney","There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from docu- ments. These systems operate on struc- tured verb-argument events produced by an NLP pipeline. We compare these sys- tems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents. ",,,,ACL
28,2016,Two Discourse Driven Language Models for Semantics,"Haoruo Peng, Dan Roth","Natural language understanding often re- quires deep semantic knowledge. Ex- panding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of ab- straction. We develop two distinct mod- els that capture semantic frame chains and discourse information while abstract- ing over the specific mentions of predi- cates and entities. For each model, we in- vestigate four implementations: a “stan- dard” N-gram language model and three discriminatively trained “neural” language models that generate embeddings for se- mantic frames. The quality of the se- mantic language models (SemLM) is eval- uated both intrinsically, using perplexity and a narrative cloze test and extrinsically – we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference res- olution and discourse parsing. ",,,,ACL
29,2016,Sentiment Domain Adaptation with Multiple Sources,"Fangzhao Wu, Yongfeng Huang","Domain adaptation is an important re- search topic in sentiment analysis area. Existing domain adaptation methods usu- ally transfer sentiment knowledge from only one source domain to target do- main. In this paper, we propose a new domain adaptation approach which can exploit sentiment knowledge from mul- tiple source domains. We first extrac- t both global and domain-specific senti- ment knowledge from the data of multi- ple source domains using multi-task learn- ing. Then we transfer them to target do- main with the help of words’ sentimen- t polarity relations extracted from the un- labeled target domain data. The similar- ities between target domain and different source domains are also incorporated into the adaptation process. Experimental re- sults on benchmark dataset show the ef- fectiveness of our approach in improving cross-domain sentiment classification per- formance. ",,,,ACL
30,2016,Connotation Frames: A Data-Driven Investigation,"Hannah Rashkin, Sameer Singh, Yejin Choi","Through a particular choice of a predicate (e.g., “x violated y”), a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y: (1) writer’s perspective: projecting x as an “antagonist” and y as a “victim”, (2) entities’ perspective: y probably dislikes x, (3) effect: something bad happened to y, (4) value: y is something valuable, and (5) mental state: y is distressed by the event. We introduce connotation frames as a rep- resentation formalism to organize these rich dimensions of connotation using typed relations. First, we investigate the fea- sibility of obtaining connotative labels through crowdsourcing experiments. We then present models for predicting the con- notation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations. Empirical results confirm that connotation frames can be in- duced from various data sources that reflect how language is used in context. We con- clude with analytical results that show the potential use of connotation frames for an- alyzing subtle biases in online news media. ",,,,ACL
31,2016,Bi-Transferring Deep Neural Networks for Domain Adaptation,"Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang, Tingting He","Sentiment classification aims to automati- cally predict sentiment polarity (e.g., pos- itive or negative) of user generated sen- timent data (e.g., reviews, blogs). Due to the mismatch among different domains, a sentiment classifier trained in one do- main may not work well when directly applied to other domains. Thus, domain adaptation for sentiment classification al- gorithms are highly desirable to reduce the domain discrepancy and manual labeling costs. To address the above challenge, we propose a novel domain adaptation method, called Bi-Transferring Deep Neu- ral Networks (BTDNNs). The proposed BTDNNs attempts to transfer the source domain examples to the target domain, and also transfer the target domain examples to the source domain. The linear transfor- mation of BTDNNs ensures the feasibility of transferring between domains, and the distribution consistency between the trans- ferred domain and the desirable domain is constrained with a linear data reconstruc- tion manner. As a result, the transferred source domain is supervised and follows similar distribution as the target domain. Therefore, any supervised method can be used on the transferred source domain to train a classifier for sentiment classifica- tion in a target domain. We conduct ex- periments on a benchmark composed of reviews of 4 types of Amazon products. Experimental results show that our pro- posed approach significantly outperforms the several baseline methods, and achieves an accuracy which is competitive with the state-of-the-art method for domain adapta- tion. ",,,,ACL
32,2016,"Document-level Sentiment Inference with Social, Faction, and Discourse Context","Eunsol Choi, Hannah Rashkin, Luke Zettlemoyer, Yejin Choi","We present a new approach for document- level sentiment inference, where the goal is to predict directed opinions (who feels positively or negatively towards whom) for all entities mentioned in a text. To encour- age more complete and consistent predic- tions, we introduce an ILP that jointly models (1) sentence- and discourse-level sentiment cues, (2) factual evidence about entity factions, and (3) global constraints based on social science theories such as homophily, social balance, and reci- procity. Together, these cues allow for rich inference across groups of entities, includ- ing for example that CEOs and the com- panies they lead are likely to have simi- lar sentiment towards others. We evalu- ate performance on new, densely labeled data that provides supervision for all pairs, complementing previous work that only labeled pairs mentioned in the same sen- tence. Experiments demonstrate that the global model outperforms sentence-level baselines, by providing more coherent pre- dictions across sets of related entities. ",,,,ACL
33,2016,Active Learning for Dependency Parsing with Partial Annotation,"Zhenghua Li, Min Zhang, Yue Zhang, Zhanyi Liu","Different from traditional active learning based on sentence-wise full annotation (FA), this paper proposes active learning with dependency-wise partial annotation (PA) as a finer-grained unit for dependency parsing. At each iteration, we select a few most uncertain words from an unlabeled data pool, manually annotate their syntactic heads, and add the partial trees into labeled data for parser retraining. Compared with sentence-wise FA, dependency-wise PA gives us more flexibility in task selection and avoids wasting time on annotating trivial tasks in a sentence. Our work makes the following contributions. First, we are the first to apply a probabilistic model to active learning for dependency parsing, which can 1) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics, and 2) directly learn parameters from PA based on a forest-based training objective. Second, we propose and compare several uncertainty metrics through simulation experiments on both Chinese and English. Finally, we conduct human annotation experiments to compare FA and PA on real annotation time and quality. ",,,,ACL
34,2016,Dependency Parsing with Bounded Block Degree and Well-nestedness via Lagrangian Relaxation and Branch-and-Bound,"Caio Corro, Joseph Le Roux, Mathieu Lacroix, Antoine Rozenknop","We present a novel dependency pars- ing method which enforces two structural properties on dependency trees: bounded block degree and well-nestedness. These properties are useful to better represent the set of admissible dependency structures in treebanks and connect dependency pars- ing to context-sensitive grammatical for- malisms. We cast this problem as an Inte- ger Linear Program that we solve with La- grangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search. Experimen- tally, we see that these methods are effi- cient and competitive compared to a base- line unconstrained parser, while enforcing structural properties in all cases. ",,,,ACL
35,2016,Query Expansion with Locally-Trained Word Embeddings,"Fernando Diaz, Bhaskar Mitra, Nick Craswell","Continuous space word embeddings have received a great deal of atten- tion in the natural language processing and machine learning communities for their ability to model term similarity and other relationships. We study the use of term relatedness in the context of query expansion for ad hoc informa- tion retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when trained globally, under- perform corpus and query specific em- beddings for retrieval tasks. These re- sults suggest that other tasks benefit- ing from global embeddings may also benefit from local embeddings. ",,,,ACL
36,2016,Together we stand: Siamese Networks for Similar Question Retrieval,"Arpita Das, Harish Yenala, Manoj Chinnakotla, Manish Shrivastava","Community Question Answering (cQA) services like Yahoo! Answers 1 , Baidu Zhidao 2 , Quora 3 , StackOverflow 4 etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions. The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives. The main challenge in this task is the “lexico- syntactic” gap between the current and the previous questions. In this paper, we pro- pose a novel approach called “Siamese Convolutional Neural Network for cQA (SCQA)” to find the semantic similarity between the current and the archived ques- tions. SCQA consist of twin convolu- tional neural networks with shared param- eters and a contrastive loss function join- ing them. SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA fo- rum archives. The model projects semanti- cally similar question pairs nearer to each other and dissimilar question pairs far- ther away from each other in the seman- tic space. Experiments on large scale real- life “Yahoo! Answers” dataset reveals that SCQA outperforms current state-of-the- art approaches based on translation mod- els, topic models and deep neural network 1 https://answers.yahoo.com/ 2 http://zhidao.baidu.com/ 3 http://www.quora.com/ 4 http://stackoverflow.com/ based models which use non-shared pa- rameters. ",,,,ACL
37,2016,News Citation Recommendation with Implicit and Explicit Semantics,"Hao Peng, Jing Liu, Chin-Yew Lin","In this work, we focus on the problem of news citation recommendation. The task aims to recommend news citations for both authors and readers to create and search news references. Due to the sparsity issue of news citations and the engineering difficulty in obtaining infor- mation on authors, we focus on content similarity-based methods instead of col- laborative filtering-based approaches. In this paper, we explore word embedding (i.e., implicit semantics) and grounded en- tities (i.e., explicit semantics) to address the variety and ambiguity issues of lan- guage. We formulate the problem as a re- ranking task and integrate different simi- larity measures under the learning to rank framework. We evaluate our approach on a real-world dataset. The experimental re- sults show the efficacy of our method. ",,,,ACL
38,2016,Grapheme-to-Phoneme Models for (Almost) Any Language,"Aliya Deri, Kevin Knight","Grapheme-to-phoneme (g2p) models are rarely available in low-resource languages, as the creation of training and evaluation data is expensive and time-consuming. We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages. We then develop phoneme and language distance metrics based on phono- logical and linguistic knowledge; apply- ing those, we adapt g2p models for high- resource languages to create models for related low-resource languages. We pro- vide results for models for 229 adapted lan- guages. ",,,,ACL
39,2016,Neural Word Segmentation Learning for Chinese,"Deng Cai, Hai Zhao","Most previous approaches to Chinese word segmentation formalize this prob- lem as a character-based sequence label- ing task so that only contextual informa- tion within fixed sized local windows and simple interactions between adjacent tags can be captured. In this paper, we pro- pose a novel neural framework which thor- oughly eliminates context windows and can utilize complete segmentation history. Our model employs a gated combination neural network over characters to produce distributed representations of word candi- dates, which are then given to a long short- term memory (LSTM) language scoring model. Experiments on the benchmark datasets show that without the help of feature engineering as most existing ap- proaches, our models achieve competitive or better performances with previous state- of-the-art methods. ",,,,ACL
40,2016,Transition-Based Neural Word Segmentation,"Meishan Zhang, Yue Zhang, Guohong Fu","Character-based and word-based methods are two main types of statistical models for Chinese word segmentation, the for- mer exploiting sequence labeling models over characters and the latter typically ex- ploiting a transition-based model, with the advantages that word-level features can be easily utilized. Neural models have been exploited for character-based Chi- nese word segmentation, giving high accu- racies by making use of external character embeddings, yet requiring less feature en- gineering. In this paper, we study a neu- ral model for word-based Chinese word segmentation, by replacing the manually- designed discrete features with neural fea- tures in a word-based segmentation frame- work. Experimental results demonstrate that word features lead to comparable per- formances to the best systems in the litera- ture, and a further combination of discrete and neural features gives top accuracies. ",,,,ACL
41,2016,A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data,"Adam Trischler, Zheng Ye, Xingdi Yuan, Jing He","Understanding unstructured text is a ma- jor goal within natural language process- ing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we in- vestigate machine comprehension on the challenging MCTest benchmark. Partly because of its limited size, prior work on MCTest has focused mainly on engi- neering better features. We tackle the dataset with a neural approach, harness- ing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the pas- sage, question, and answer from a vari- ety of trainable perspectives, as opposed to using a manually designed, rigid fea- ture set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology de- signed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for MCTest, outper- forming previous feature-engineered ap- proaches slightly and previous neural ap- proaches by a significant margin (over 15 percentage points). ",,,,ACL
42,2016,Combining Natural Logic and Shallow Reasoning for Question Answering,"Gabor Angeli, Neha Nayak, Christopher D. Manning","Broad domain question answering is of- ten difficult in the absence of structured knowledge bases, and can benefit from shallow lexical methods (broad coverage) and logical reasoning (high precision). We propose an approach for incorporating both of these signals in a unified frame- work based on natural logic. We extend the breadth of inferences afforded by nat- ural logic to include relational entailment (e.g., buy → own) and meronymy (e.g., a person born in a city is born the city’s country). Furthermore, we train an eval- uation function – akin to gameplaying – to evaluate the expected truth of candidate premises on the fly. We evaluate our ap- proach on answering multiple choice sci- ence questions, achieving strong results on the dataset. ",,,,ACL
43,2016,Easy Questions First? A Case Study on Curriculum Learning for Question Answering,"Mrinmaya Sachan, Eric Xing","Cognitive science researchers have em- phasized the importance of ordering a complex task into a sequence of easy to hard problems. Such an ordering provides an easier path to learning and increases the speed of acquisition of the task com- pared to conventional learning. Recent works in machine learning have explored a curriculum learning approach called self- paced learning which orders data samples on the easiness scale so that easy sam- ples can be introduced to the learning algo- rithm first and harder samples can be intro- duced successively. We introduce a num- ber of heuristics that improve upon self- paced learning. Then, we argue that incor- porating easy, yet, a diverse set of samples can further improve learning. We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improve- ments in each of them. ",,,,ACL
44,2016,Improved Representation Learning for Question Answer Matching,"Ming Tan, Cicero dos Santos, Bing Xiang, Bowen Zhou","Passage-level question answer matching is a challenging task since it requires effec- tive representations that capture the com- plex semantic relations between questions and answers. In this work, we propose a series of deep learning models to address passage answer selection. To match pas- sage answers to questions accommodat- ing their complex semantic relations, un- like most previous work that utilizes a sin- gle deep learning structure, we develop hybrid models that process the text us- ing both convolutional and recurrent neu- ral networks, combining the merits on ex- tracting linguistic information from both structures. Additionally, we also develop a simple but effective attention mechanism for the purpose of constructing better an- swer representations according to the in- put question, which is imperative for bet- ter modeling long answer sequences. The results on two public benchmark datasets, InsuranceQA and TREC-QA, show that our proposed models outperform a variety of strong baselines. ",,,,ACL
45,2016,Tables as Semi-structured Knowledge for Question Answering,"Sujay Kumar Jauhar, Peter Turney, Eduard Hovy","Question answering requires access to a knowledge base to check facts and rea- son about information. Knowledge in the form of natural language text is easy to ac- quire, but difficult for automated reason- ing. Highly-structured knowledge bases can facilitate reasoning, but are difficult to acquire. In this paper we explore tables as a semi-structured formalism that pro- vides a balanced compromise to this trade- off. We first use the structure of tables to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations, easily and ef- ficiently via crowd-sourcing. We then use this annotated data to train a semi- structured feature-driven model for ques- tion answering that uses tables as a knowl- edge base. In benchmark evaluations, we significantly outperform both a strong un- structured retrieval baseline and a highly- structured Markov Logic Network model. ",,,,ACL
46,2016,Neural Summarization by Extracting Sentences and Words,"Jianpeng Cheng, Mirella Lapata",Traditional approaches to extractive summarization rely heavily on human- engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general frame- work for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs 1 . Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation. ,,,,ACL
47,2016,Neural Networks For Negation Scope Detection,"Federico Fancellu, Adam Lopez, Bonnie Webber","Automatic negation scope detection is a task that has been tackled using differ- ent classifiers and heuristics. Most sys- tems are however 1) highly-engineered, 2) English-specific, and 3) only tested on the same genre they were trained on. We start by addressing 1) and 2) using a neural network architecture. Results obtained on data from the *SEM2012 shared task on negation scope detection show that even a simple feed-forward neural network us- ing word-embedding features alone, per- forms on par with earlier classifiers, with a bi-directional LSTM outperforming all of them. We then address 3) by means of a specially-designed synthetic test set; in doing so, we explore the problem of de- tecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered. ",,,,ACL
48,2016,CSE: Conceptual Sentence Embeddings based on Attention Model,"Yashen Wang, Heyan Huang, Chong Feng, Qiang Zhou","Most sentence embedding models typical- ly represent each sentence only using word surface, which makes these models indis- criminative for ubiquitous homonymy and polysemy. In order to enhance represen- tation capability of sentence, we employ conceptualization model to assign associ- ated concepts for each sentence in the tex- t corpus, and then learn conceptual sen- tence embedding (CSE). Hence, this se- mantic representation is more expressive than some widely-used text representation models such as latent topic model, espe- cially for short-text. Moreover, we fur- ther extend CSE models by utilizing a lo- cal attention-based model that select rel- evant words within the context to make more efficient prediction. In the experi- ments, we evaluate the CSE models on two tasks, text classification and information retrieval. The experimental results show that the proposed models outperform typi- cal sentence embed-ding models. ",,,,ACL
49,2016,DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents,"Zhao Yan, Nan Duan, Junwei Bao, Peng Chen","Most current chatbot engines are designed to reply to user utterances based on exist- ing utterance-response (or Q-R) 1 pairs. In this paper, we present DocChat, a novel information retrieval approach for chat- bot engines that can leverage unstructured documents, instead of Q-R pairs, to re- spond to utterances. A learning to rank model with features designed at different levels of granularity is proposed to mea- sure the relevance between utterances and responses directly. We evaluate our pro- posed approach in both English and Chi- nese: (i) For English, we evaluate Doc- Chat on WikiQA and QASent, two answer sentence selection tasks, and compare it with state-of-the-art methods. Reasonable improvements and good adaptability are observed. (ii) For Chinese, we compare DocChat with XiaoIce 2 , a famous chitchat engine in China, and side-by-side evalua- tion shows that DocChat is a perfect com- plement for chatbot engines using Q-R pairs as main source of responses. ",,,,ACL
50,2016,Investigating the Sources of Linguistic Alignment in Conversation,"Gabriel Doyle, Michael C. Frank","In conversation, speakers tend to “ac- commodate” or “align” to their partners, changing the style and substance of their communications to be more similar to their partners’ utterances. We focus here on “linguistic alignment,” changes in word choice based on others’ choices. Although linguistic alignment is observed across many different contexts and its degree cor- relates with important social factors such as power and likability, its sources are still uncertain. We build on a recent probabilis- tic model of alignment, using it to separate out alignment attributable to words ver- sus word categories. We model alignment in two contexts: telephone conversations and microblog replies. Our results show evidence of alignment, but it is primarily lexical rather than categorical. Further- more, we find that discourse acts modu- late alignment substantially. This evidence supports the view that alignment is shaped by strategic communicative processes re- lated to the ongoing discourse. ",,,,ACL
51,2016,Entropy Converges Between Dialogue Participants: Explanations from an Information-Theoretic Perspective,"Yang Xu, David Reitter","The applicability of entropy rate constancy to dialogue is examined on two spoken di- alogue corpora. The principle is found to hold; however, new entropy change pat- terns within the topic episodes of dialogue are described, which are different from written text. Speaker’s dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy, respectively, which results in lo- cal convergence between these speakers in each topic episode. This implies that the sentence entropy in dialogue is con- ditioned on different contexts determined by the speaker’s roles. Explanations from the perspectives of grounding theory and interactive alignment are discussed, re- sulting in a novel, unified information- theoretic approach of dialogue. ",,,,ACL
52,2016,Finding the Middle Ground - A Model for Planning Satisficing Answers,"Sabine Janzen, Wolfgang Maaß, Tobias Kowatsch","To establish sophisticated dialogue sys- tems, text planning needs to cope with congruent as well as incongruent inter- locutor interests as given in everyday di- alogues. Little attention has been given to this topic in text planning in contrast to di- alogues that are fully aligned with antic- ipated user interests. When considering dialogues with congruent and incongru- ent interlocutor interests, dialogue part- ners are facing the constant challenge of finding a balance between cooperation and competition. We introduce the concept of fairness that operationalize an equal and adequate, i.e. equitable satisfaction of all interlocutors’ interests. Focusing on Question-Answering (QA) settings, we describe an answer planning approach that support fair dialogues under congruent and incongruent interlocutor interests. Due to the fact that fairness is subjective per se, we present promising results from an empirical study (N=107) in which human subjects interacted with a QA system im- plementing the proposed approach. ",,,,ACL
53,2016,A Sentence Interaction Network for Modeling Dependence between Sentences,"Biao Liu, Minlie Huang","Modeling interactions between two sen- tences is crucial for a number of natu- ral language processing tasks including Answer Selection, Dialogue Act Analy- sis, etc. While deep learning methods like Recurrent Neural Network or Convo- lutional Neural Network have been proved to be powerful for sentence modeling, prior studies paid less attention on inter- actions between sentences. In this work, we propose a Sentence Interaction Net- work (SIN) for modeling the complex in- teractions between two sentences. By in- troducing “interaction states” for word and phrase pairs, SIN is powerful and flexi- ble in capturing sentence interactions for different tasks. We obtain significant im- provements on Answer Selection and Dia- logue Act Analysis without any feature en- gineering. ",,,,ACL
54,2016,Towards more variation in text generation: Developing and evaluating variation models for choice of referential form,"Thiago Castro Ferreira, Emiel Krahmer, Sander Wubben","In this study, we introduce a non- deterministic method for referring expres- sion generation. We describe two models that account for individual variation in the choice of referential form in automatically generated text: a Naive Bayes model and a Recurrent Neural Network. Both are eval- uated using the VaREG corpus. Then we select the best performing model to gen- erate referential forms in texts from the GREC-2.0 corpus and conduct an evalu- ation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original references and those pro- duced by a random baseline model. ",,,,ACL
55,2016,How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions,"Arun Chaganty, Percy Liang","How much is 131 million US dollars? To help readers put such numbers in con- text, we propose a new task of automati- cally generating short descriptions known as perspectives, e.g. “$131 million is about the cost to employ everyone in Texas over a lunch period”. First, we collect a dataset of numeric mentions in news arti- cles, where each mention is labeled with a set of rated perspectives. We then pro- pose a system to generate these descrip- tions consisting of two steps: formula con- struction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on fa- miliarity, numeric proximity and seman- tic compatibility. In generation, we con- vert a formula into natural language us- ing a sequence-to-sequence recurrent neu- ral network. Our system obtains a 15.2% F 1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation. ",,,,ACL
56,2016,Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus,"Iulian Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn","Over the past decade, large-scale super- vised learning corpora have enabled ma- chine learning researchers to make sub- stantial advances. However, to this date, there are no large-scale question- answer corpora available. In this paper we present the 30M Factoid Question- Answer Corpus, an enormous question- answer pair corpus produced by apply- ing a novel neural network architecture on the knowledge base Freebase to trans- duce facts into natural language ques- tions. The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine trans- lation and sentence similarity metrics. Across all evaluation criteria the question- generation model outperforms the compet- ing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear to be compa- rable in quality to real human-generated questions. * First authors. ? Email: {iulian.vlad.serban,caglar.gulcehre, sungjin.ahn,sarath.chandar.anbil.parthipan, aaron.courville,yoshua.bengio}@umontreal.ca Email: alberto.garcia-duran@utc.fr ? CIFAR Senior Fellow ",,,,ACL
57,2016,Latent Predictor Networks for Code Generation,"Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz Hermann","Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architec- ture which generates an output sequence conditioned on an arbitrary number of in- put functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating pro- gramming code from a mixed natural lan- guage and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearth- stone. On these, and a third preexisting corpus, we demonstrate that marginalis- ing multiple predictors allows our model to outperform strong benchmarks. ",,,,ACL
58,2016,Easy Things First: Installments Improve Referring Expression Generation for Objects in Photographs,"Sina Zarrieß, David Schlangen","Research on generating referring expres- sions has so far mostly focussed on “one- shot reference”, where the aim is to gener- ate a single, discriminating expression. In interactive settings, however, it is not un- common for reference to be established in “installments”, where referring informa- tion is offered piecewise until success has been confirmed. We show that this strat- egy can also be advantageous in technical systems that only have uncertain access to object attributes and categories. We train a recently introduced model of grounded word meaning on a data set of REs for objects in images and learn to predict se- mantically appropriate expressions. In a human evaluation, we observe that users are sensitive to inadequate object names - which unfortunately are not unlikely to be generated from low-level visual input. We propose a solution inspired from hu- man task-oriented interaction and imple- ment strategies for avoiding and repair- ing semantically inaccurate words. We enhance a word-based REG with context- aware, referential installments and find that they substantially improve the refer- ential success of the system. ",,,,ACL
59,2016,Collective Entity Resolution with Multi-Focal Attention,"Amir Globerson, Nevena Lazic, Soumen Chakrabarti, Amarnag Subramanya","Entity resolution is the task of linking each mention of an entity in text to the cor- responding record in a knowledge base (KB). Coherence models for entity resolu- tion encourage all referring expressions in a document to resolve to entities that are related in the KB. We explore attention- like mechanisms for coherence, where the evidence for each candidate is based on a small set of strong relations, rather than relations to all other entities in the doc- ument. The rationale is that document- wide support may simply not exist for non-salient entities, or entities not densely connected in the KB. Our proposed sys- tem outperforms state-of-the-art systems on the CoNLL 2003, TAC KBP 2010, 2011 and 2012 tasks. ",,,,ACL
60,2016,Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric,"Nafise Sadat Moosavi, Michael Strube","Interpretability and discriminative power are the two most basic requirements for an evaluation metric. In this paper, we re- port the mention identification effect in the B 3 , CEAF, and BLANC coreference eval- uation metrics that makes it impossible to interpret their results properly. The only metric which is insensitive to this flaw is MUC, which, however, is known to be the least discriminative metric. It is a known fact that none of the current metrics are reliable. The common practice for rank- ing coreference resolvers is to use the av- erage of three different metrics. However, one cannot expect to obtain a reliable score by averaging three unreliable metrics. We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to over- come the shortcomings of the current eval- uation metrics. LEA is available as branch LEA-scorer in the reference implemen- tation of the official CoNLL scorer. ",,,,ACL
61,2016,Improving Coreference Resolution by Learning Entity-Level Distributed Representations,"Kevin Clark, Christopher D. Manning","A long-standing challenge in coreference resolution has been the incorporation of entity-level information – features defined over clusters of mentions instead of men- tion pairs. We present a neural net- work based coreference system that pro- duces high-dimensional vector represen- tations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is de- sirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final corefer- ence partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features. ",,,,ACL
62,2016,Effects of Creativity and Cluster Tightness on Short Text Clustering Performance,"Catherine Finegan-Dollak, Reed Coke, Rui Zhang, Xiangyi Ye","Properties of corpora, such as the diver- sity of vocabulary and how tightly related texts cluster together, impact the best way to cluster short texts. We examine several such properties in a variety of corpora and track their effects on various combinations of similarity metrics and clustering algo- rithms. We show that semantic similar- ity metrics outperform traditional n-gram and dependency similarity metrics for k- means clustering of a linguistically cre- ative dataset, but do not help with less creative texts. Yet the choice of simi- larity metric interacts with the choice of clustering method. We find that graph- based clustering methods perform well on tightly clustered data but poorly on loosely clustered data. Semantic similarity met- rics generate loosely clustered output even when applied to a tightly clustered dataset. Thus, the best performing clustering sys- tems could not use semantic metrics. ",,,,ACL
63,2016,Generative Topic Embedding: a Continuous Representation of Documents,"Shaohua Li, Tat-Seng Chua, Jun Zhu, Chunyan Miao","Word embedding maps words into a low- dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps docu- ments onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The proba- bility of each word is influenced by both its local context and its topic. A variational inference method yields the topic embed- dings as well as the topic mixing propor- tions for each document. Jointly they rep- resent the document in a low-dimensional continuous space. In two document clas- sification tasks, our method performs bet- ter than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate co- herent topics even based on only one doc- ument. ",,,,ACL
64,2016,Detecting Common Discussion Topics Across Culture From News Reader Comments,"Bei Shi, Wai Lam, Lidong Bing, Yinqing Xu","News reader comments found in many on-line news websites are typically massive in amount. We investigate the task of Cultural-common Topic Detection (CTD), which is aimed at discovering common discussion topics from news reader comments written in different languages. We propose a new probabilistic graphical model called MCTA which can cope with the language gap and capture the common semantics in different languages. We also develop a partially collapsed Gibbs sampler which effectively incorporates the term trans- lation relationship into the detection of cultural-common topics for model param- eter learning. Experimental results show improvements over the state-of-the-art model. ",,,,ACL
65,2016,A Discriminative Topic Model using Document Network Structure,"Weiwei Yang, Jordan Boyd-Graber, Philip Resnik","Document collections often have links be- tween documents—citations, hyperlinks, or revisions—and which links are added is often based on topical similarity. To model these intuitions, we introduce a new topic model for documents situated within a net- work structure, integrating latent blocks of documents with a max-margin learning criterion for link prediction using topic- and word-level features. Experiments on a scientific paper dataset and collection of webpages show that, by more robustly exploiting the rich link structure within a document network, our model improves link prediction, topic quality, and block distributions. ",,,,ACL
66,2016,AraSenTi: Large-Scale Twitter-Specific Arabic Sentiment Lexicons,"Nora Al-Twairesh, Hend Al-Khalifa, Abdulmalik Al-Salman","Sentiment Analysis (SA) is an active research area nowadays due to the tremendous interest in aggregating and evaluating opinions being disseminated by users on the Web. SA of English has been thoroughly researched; however research on SA of Arabic has just flourished. Twitter is considered a powerful tool for disseminating information and a rich resource for opinionated text containing views on many different topics. In this paper we attempt to bridge a gap in Arabic SA of Twitter which is the lack of sentiment lexi- cons that are tailored for the informal lan- guage of Twitter. We generate two lexicons extracted from a large dataset of tweets using two approaches and evaluate their use in a simple lexicon based method. The evaluation is performed on internal and external da- tasets. The performance of these automatical- ly generated lexicons was very promising, al- beit the simple method used for classification. The best F-score obtained was 89.58% on the internal dataset and 63.1-64.7% on the exter- nal datasets. ",,,,ACL
67,2016,Unsupervised Multi-Author Document Decomposition Based on Hidden Markov Model,"Khaled Aldebei, Xiangjian He, Wenjing Jia, Jie Yang","This paper proposes an unsupervised approach for segmenting a multi- author document into authorial com- ponents. The key novelty is that we utilize the sequential patterns hid- den among document elements when determining their authorships. For this purpose, we adopt Hidden Markov Model (HMM) and construct a sequen- tial probabilistic model to capture the dependencies of sequential sentences and their authorships. An unsuper- vised learning method is developed to initialize the HMM parameters. Exper- imental results on benchmark datasets have demonstrated the significant ben- efit of our idea and our approach has outperformed the state-of-the-arts on all tests. As an example of its applica- tions, the proposed approach is applied for attributing authorship of a docu- ment and has also shown promising re- sults. ",,,,ACL
68,2016,Automatic Text Scoring Using Neural Networks,"Dimitrios Alikaniotis, Helen Yannakoudakis, Marek Rei","Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the pre- dictive features of the system need to be manually engineered by human ex- perts. We introduce a model that forms word representations by learning the ex- tent to which specific words contribute to the text’s score. Using Long-Short Term Memory networks to represent the mean- ing of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent ad- vances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative. ",,,,ACL
69,2016,Improved Semantic Parsers For If-Then Statements,"I. Beltagy, Chris Quirk","Digital personal assistants are becoming both more common and more useful. The major NLP challenge for personal assis- tants is machine understanding: translat- ing natural language user commands into an executable representation. This paper focuses on understanding rules written as If-Then statements, though the techniques should be portable to other semantic pars- ing tasks. We view understanding as struc- ture prediction and show improved mod- els using both conventional techniques and neural network models. We also discuss various ways to improve generalization and reduce overfitting: synthetic training data from paraphrase, grammar combina- tions, feature selection and ensembles of multiple systems. An ensemble of these techniques achieves a new state of the art result with 8% accuracy improvement. ",,,,ACL
70,2016,Universal Dependencies for Learner English,"Yevgeni Berzak, Jessica Kenney, Carolyn Spadine, Jing Xian Wang","We introduce the Treebank of Learner En- glish (TLE), the first publicly available syntactic treebank for English as a Sec- ond Language (ESL). The TLE provides manually annotated POS tags and Univer- sal Dependency (UD) trees for 5,124 sen- tences from the Cambridge First Certifi- cate in English (FCE) corpus. The UD annotations are tied to a pre-existing er- ror annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungram- matical English. Finally, we benchmark POS tagging and dependency parsing per- formance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to sup- port a wide range of linguistic and compu- tational research on second language ac- quisition as well as automatic processing of ungrammatical language 1 . ",,,,ACL
71,2016,Extracting token-level signals of syntactic processing from fMRI - with an application to PoS induction,"Joachim Bingel, Maria Barrett, Anders Søgaard","Neuro-imaging studies on reading differ- ent parts of speech (PoS) report somewhat mixed results, yet some of them indicate different activations with different PoS. This paper addresses the difficulty of using fMRI to discriminate between linguistic tokens in reading of running text because of low temporal resolution. We show that once we solve this problem, fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4%. ",,,,ACL
72,2016,Bidirectional Recurrent Convolutional Neural Network for Relation Classification,"Rui Cai, Xiaodong Zhang, Houfeng Wang","Relation classification is an important se- mantic processing task in the field of natu- ral language processing (NLP). In this pa- per, we present a novel model BRCNN to classify the relation of two entities in a sentence. Some state-of-the-art systems concentrate on modeling the shortest de- pendency path (SDP) between two entities leveraging convolutional or recurrent neu- ral networks. We further explore how to make full use of the dependency relations information in the SDP, by combining convolutional neural networks and two- channel recurrent neural networks with long short term memory (LSTM) units. We propose a bidirectional architecture to learn relation representations with di- rectional information along the SDP for- wards and backwards at the same time, which benefits classifying the direction of relations. Experimental results show that our method outperforms the state-of-the- art approaches on the SemEval-2010 Task 8 dataset. ",,,,ACL
73,2016,Sentence Rewriting for Semantic Parsing,"Bo Chen, Le Sun, Xianpei Han, Bo An","A major challenge of semantic parsing is the vocabulary mismatch problem be- tween natural language and target ontol- ogy. In this paper, we propose a sen- tence rewriting based semantic parsing method, which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same struc- ture with its target logical form. Specifi- cally, we propose two sentence-rewriting methods for two common types of mis- match: a dictionary-based method for 1- N mismatch and a template-based method for N-1 mismatch. We evaluate our sen- tence rewriting based semantic parser on the benchmark semantic parsing dataset – WEBQUESTIONS. Experimental results show that our system outperforms the base system with a 3.4% gain in F1, and gen- erates logical forms more accurately and parses sentences more robustly. ",,,,ACL
74,2016,Chinese Zero Pronoun Resolution with Deep Neural Networks,"Chen Chen, Vincent Ng","While unsupervised anaphoric zero pro- noun (AZP) resolvers have recently been shown to rival their supervised counter- parts in performance, it is relatively diffi- cult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features. To address these weaknesses, we propose a supervised approach to AZP resolution based on deep neural networks, taking advantage of their ability to learn useful task-specific representations and ef- fectively exploit lexical features via word embeddings. Our approach achieves state- of-the-art performance when resolving the Chinese AZPs in the OntoNotes corpus. ",,,,ACL
75,2016,Constrained Multi-Task Learning for Automated Essay Scoring,"Ronan Cummins, Meng Zhang, Ted Briscoe","Supervised machine learning models for automated essay scoring (AES) usually re- quire substantial task-specific training data in order to make accurate predictions for a particular writing task. This limita- tion hinders their utility, and consequently their deployment in real-world settings. In this paper, we overcome this shortcoming using a constrained multi-task pairwise- preference learning approach that enables the data from multiple tasks to be com- bined effectively. Furthermore, contrary to some recent re- search, we show that high performance AES systems can be built with little or no task-specific training data. We perform a detailed study of our approach on a pub- licly available dataset in scenarios where we have varying amounts of task-specific training data and in scenarios where the number of tasks increases. ",,,,ACL
76,2016,CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases,"Zihang Dai, Lei Li, Wei Xu","How can we enable computers to automat- ically answer questions like “Who created the character Harry Potter”? Carefully built knowledge bases provide rich sources of facts. However, it remains a chal- lenge to answer factoid questions raised in natural language due to numerous ex- pressions of one question. In particular, we focus on the most common questions — ones that can be answered with a sin- gle fact in the knowledge base. We pro- pose CFO, a Conditional Focused neural- network-based approach to answering fac- toid questions with knowledge bases. Our approach first zooms in a question to find more probable candidate subject men- tions, and infers the final answers with a unified conditional probabilistic frame- work. Powered by deep recurrent neural networks and neural embeddings, our pro- posed CFO achieves an accuracy of 75.7% on a dataset of 108k questions – the largest public one to date. It outperforms the cur- rent state of the art by an absolute margin of 11.8%. ",,,,ACL
77,2016,Verbs Taking Clausal and Non-Finite Arguments as Signals of Modality – Revisiting the Issue of Meaning Grounded in Syntax,Judith Eckle-Kohler,"We revisit Levin’s theory about the correspondence of verb meaning and syntax and infer semantic classes from a large syntactic classification of more than 600 German verbs taking clausal and non-finite arguments. Grasping the meaning components of Levin-classes is known to be hard. We address this chal- lenge by setting up a multi-perspective semantic characterization of the inferred classes. To this end, we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons – the German wordnet GermaNet, VerbNet and FrameNet – and perform a detailed analysis and evaluation of the resulting German–English classification (avail- able at www.ukp.tu-darmstadt. de/modality-verbclasses/). ",,,,ACL
78,2016,Tree-to-Sequence Attentional Neural Machine Translation,"Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka","Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntac- tic NMT model, extending a sequence- to-sequence model with the source-side phrase structure. Our model has an at- tention mechanism that enables the de- coder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experi- mental results on the WAT’15 English- to-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system. ",,,,ACL
79,2016,Coordination Annotation Extension in the Penn Tree Bank,"Jessica Ficler, Yoav Goldberg","Coordination is an important and common syntactic construction which is not han- dled well by state of the art parsers. Co- ordinations in the Penn Treebank are miss- ing internal structure in many cases, do not include explicit marking of the conjuncts and contain various errors and inconsisten- cies. In this work, we initiated manual an- notation process for solving these issues. We identify the different elements in a co- ordination phrase and label each element with its function. We add phrase bound- aries when these are missing, unify incon- sistencies, and fix errors. The outcome is an extension of the PTB that includes con- sistent and detailed structures for coordi- nations. We make the coordination anno- tation publicly available, in hope that they will facilitate further research into coordi- nation disambiguation. 1 ",,,,ACL
80,2016,Analyzing Biases in Human Perception of User Age and Gender from Text,"Lucie Flekova, Jordan Carpenter, Salvatore Giorgi, Lyle Ungar","User traits disclosed through written text, such as age and gender, can be used to per- sonalize applications such as recommender systems or conversational agents. However, human perception of these traits is not per- fectly aligned with reality. In this paper, we conduct a large-scale crowdsourcing ex- periment on guessing age and gender from tweets. We systematically analyze the qual- ity and possible biases of these predictions. We identify the textual cues which lead to miss-assessments of traits or make annota- tors more or less confident in their choice. Our study demonstrates that differences be- tween real and perceived traits are notewor- thy and elucidates inaccurately used stereo- types in human perception. ",,,,ACL
81,2016,Modeling Social Norms Evolution for Personalized Sentiment Classification,"Lin Gong, Mohammad Al Boni, Hongning Wang","Motivated by the findings in social sci- ence that people’s opinions are diverse and variable while together they are shaped by evolving social norms, we perform person- alized sentiment classification via shared model adaptation over time. In our pro- posed solution, a global sentiment model is constantly updated to capture the ho- mogeneity in which users express opin- ions, while personalized models are simul- taneously adapted from the global model to recognize the heterogeneity of opin- ions from individuals. Global model shar- ing alleviates data sparsity issue, and in- dividualized model adaptation enables ef- ficient online model learning. Extensive experimentations are performed on two large review collections from Amazon and Yelp, and encouraging performance gain is achieved against several state-of-the-art transfer learning and multi-task learning based sentiment classification solutions. ",,,,ACL
82,2016,Modeling Concept Dependencies in a Scientific Corpus,"Jonathan Gordon, Linhong Zhu, Aram Galstyan, Prem Natarajan","Our goal is to generate reading lists for stu- dents that help them optimally learn techni- cal material. Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query. This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered. Here we formulate an information-theoretic view of concept de- pendency and present methods to construct a “concept graph” automatically from a text corpus. We perform the first human evalu- ation of concept dependency edges (to be published as open data), and the results ver- ify the feasibility of automatic approaches for inferring concepts and their dependency relations. This result can support search ca- pabilities that may be tuned to help users learn a subject rather than retrieve docu- ments based on a single query. ",,,,ACL
83,2016,Normalized Log-Linear Interpolation of Backoff Language Models is Efficient,"Kenneth Heafield, Chase Geigle, Sean Massung, Lane Schwartz","We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single nor- malized backoff model, contradicting Hsu (2007). While prior work reported that log-linear interpolation yields lower per- plexity than linear interpolation, normaliz- ing at query time was impractical. We nor- malize the model offline in advance, which is efficient due to a recurrence relationship between the normalizing factors. To tune interpolation weights, we apply Newton’s method to this convex problem and show that the derivatives can be computed ef- ficiently in a batch process. These find- ings are combined in new open-source in- terpolation tool, which is distributed with KenLM. With 21 out-of-domain corpora, log-linear interpolation yields 72.58 per- plexity on TED talks, compared to 75.91 for linear interpolation. ",,,,ACL
84,2016,How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation,"Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin","Recently a few systems for automatically solving math word problems have reported promising results. However, the datasets used for evaluation have limitations in both scale and diversity. In this paper, we build a large-scale dataset which is more than 9 times the size of previous ones, and contains many more problem types. Problems in the dataset are semi- automatically obtained from community question-answering (CQA) web pages. A ranking SVM model is trained to auto- matically extract problem answers from the answer text provided by CQA users, which significantly reduces human anno- tation cost. Experiments conducted on the new dataset lead to interesting and surpris- ing results. ",,,,ACL
85,2016,Embeddings for Word Sense Disambiguation: An Evaluation Study,"Ignacio Iacobacci, Mohammad Taher Pilehvar, Roberto Navigli","Recent years have seen a dramatic growth in the popularity of word embeddings mainly owing to their ability to capture se- mantic information from massive amounts of textual content. As a result, many tasks in Natural Language Processing have tried to take advantage of the potential of these distributional models. In this work, we study how word embeddings can be used in Word Sense Disambiguation, one of the oldest tasks in Natural Language Processing and Artificial Intelligence. We propose different methods through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture, and perform a deep analysis of how different parameters affect perfor- mance. We show how a WSD system that makes use of word embeddings alone, if designed properly, can provide significant performance improvement over a state-of- the-art WSD system that incorporates sev- eral standard WSD features. ",,,,ACL
86,2016,Text Understanding with the Attention Sum Reader Network,"Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, Jan Kleindienst","Several large cloze-style context-question- answer datasets have been introduced re- cently: the CNN and Daily Mail news data and the Children’s Book Test. Thanks to the size of these datasets, the asso- ciated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alterna- tive approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended rep- resentation of words in the document as is usual in similar models. This makes the model particularly suitable for question- answering problems where the answer is a single word from the document. Ensem- ble of our models sets new state of the art on all evaluated datasets. ",,,,ACL
87,2016,Investigating LSTMs for Joint Extraction of Opinion Entities and Relations,"Arzoo Katiyar, Claire Cardie","We investigate the use of deep bi- directional LSTMs for joint extraction of opinion entities and the IS - FROM and IS - ABOUT relations that connect them — the first such attempt using a deep learning approach. Perhaps surprisingly, we find that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint in- ference approach (Yang and Cardie, 2013) to opinion entities extraction, perform- ing below even the standalone sequence- tagging CRF. Incorporating sentence-level and a novel relation-level optimization, however, allows the LSTM to identify opinion relations and to perform within 1– 3% of the state-of-the-art joint model for opinion entities and the IS - FROM relation; and to perform as well as the state-of-the- art for the IS - ABOUT relation — all with- out access to opinion lexicons, parsers and other preprocessing components required for the feature-rich CRF+ILP approach. ",,,,ACL
88,2016,Transition-Based Left-Corner Parsing for Identifying PTB-Style Nonlocal Dependencies,"Yoshihide Kato, Shigeki Matsubara",This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based sys- tem. We use a structured perceptron which enables our parser to utilize global features captured by nonlocal dependencies. An experimental result demonstrates that our parser achieves a good balance between constituent parsing and nonlocal depen- dency identification. ,,,,ACL
89,2016,Siamese CBOW: Optimizing Word Embeddings for Sentence Representations,"Tom Kenter, Alexey Borisov, Maarten de Rijke","We present the Siamese Continuous Bag of Words (Siamese CBOW) model, a neural network for efficient estimation of high- quality sentence embeddings. Averaging the embeddings of words in a sentence has proven to be a surprisingly success- ful and efficient way of obtaining sen- tence embeddings. However, word em- beddings trained with the methods cur- rently available are not optimized for the task of sentence representation, and, thus, likely to be suboptimal. Siamese CBOW handles this problem by training word em- beddings directly for the purpose of be- ing averaged. The underlying neural net- work learns word embeddings by predict- ing, from a sentence representation, its surrounding sentences. We show the ro- bustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources. ",,,,ACL
90,2016,Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings,"Fereshte Khani, Martin Rinard, Percy Liang","Can we train a system that, on any new input, either says “don’t know” or makes a prediction that is guaranteed to be cor- rect? We answer the question in the affir- mative provided our model family is well- specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operational- ize this principle for semantic parsing, the task of mapping utterances to logi- cal forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only check- ing two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empiri- cally, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset. ",,,,ACL
91,2016,Exploring Convolutional and Recurrent Neural Networks in Sequential Labelling for Dialogue Topic Tracking,"Seokhwan Kim, Rafael Banchs, Haizhou Li","Dialogue topic tracking is a sequential la- belling problem of recognizing the topic state at each time step in given dialogue sequences. This paper presents various ar- tificial neural network models for dialogue topic tracking, including convolutional neural networks to account for semantics at each individual utterance, and recurrent neural networks to account for conversa- tional contexts along multiple turns in the dialogue history. The experimental results demonstrate that our proposed models can significantly improve the tracking perfor- mances in human-human conversations. ",,,,ACL
92,2016,Cross-Lingual Lexico-Semantic Transfer in Language Learning,"Ekaterina Kochmar, Ekaterina Shutova","Lexico-semantic knowledge of our native language provides an initial foundation for second language learning. In this paper, we investigate whether and to what extent the lexico-semantic models of the native language (L1) are transferred to the sec- ond language (L2). Specifically, we focus on the problem of lexical choice and in- vestigate it in the context of three typolog- ically diverse languages: Russian, Span- ish and English. We show that a statistical semantic model learned from L1 data im- proves automatic error detection in L2 for the speakers of the respective L1. Finally, we investigate whether the semantic model learned from a particular L1 is portable to other, typologically related languages. ",,,,ACL
93,2016,A CALL System for Learning Preposition Usage,"John Lee, Donald Sturgeon, Mengqi Luo","Fill-in-the-blank items are commonly fea- tured in computer-assisted language learn- ing (CALL) systems. An item displays a sentence with a blank, and often proposes a number of choices for filling it. These choices should include one correct answer and several plausible distractors. We de- scribe a system that, given an English cor- pus, automatically generates distractors to produce items for preposition usage. We report a comprehensive evaluation on this system, involving both experts and learners. First, we analyze the diffi- culty levels of machine-generated carrier sentences and distractors, comparing sev- eral methods that exploit learner error and learner revision patterns. We show that the quality of machine-generated items ap- proaches that of human-crafted ones. Fur- ther, we investigate the extent to which mismatched L1 between the user and the learner corpora affects the quality of dis- tractors. Finally, we measure the system’s impact on the user’s language proficiency in both the short and the long term. ",,,,ACL
94,2016,A Persona-Based Neural Conversation Model,"Jiwei Li, Michel Galley, Chris Brockett, Georgios Spithourakis","We present persona-based models for han- dling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed em- beddings that capture individual charac- teristics such as background information and speaking style. A dyadic speaker- addressee model captures properties of in- teractions between two interlocutors. Our models yield qualitative performance im- provements in both perplexity and B LEU scores over baseline sequence-to-sequence models, with similar gains in speaker con- sistency as measured by human judges. ",,,,ACL
95,2016,Discriminative Deep Random Walk for Network Classification,"Juzheng Li, Jun Zhu, Bo Zhang","Deep Random Walk (DeepWalk) can learn a latent space representation for describ- ing the topological structure of a network. However, for relational network classifi- cation, DeepWalk can be suboptimal as it lacks a mechanism to optimize the ob- jective of the target task. In this paper, we present Discriminative Deep Random Walk (DDRW), a novel method for re- lational network classification. By solv- ing a joint optimization problem, DDRW can learn the latent space representations that well capture the topological struc- ture and meanwhile are discriminative for the network classification task. Our ex- perimental results on several real social networks demonstrate that DDRW signif- icantly outperforms DeepWalk on multi- label network classification tasks, while retaining the topological structure in the latent space. DDRW is stable and con- sistently outperforms the baseline meth- ods by various percentages of labeled data. DDRW is also an online method that is scalable and can be naturally parallelized. ",,,,ACL
96,2016,Normalising Medical Concepts in Social Media Texts by Learning Semantic Representation,"Nut Limsopatham, Nigel Collier","Automatically recognising medical con- cepts mentioned in social media messages (e.g. tweets) enables several applications for enhancing health quality of people in a community, e.g. real-time monitoring of infectious diseases in population. How- ever, the discrepancy between the type of language used in social media and med- ical ontologies poses a major challenge. Existing studies deal with this challenge by employing techniques, such as lexi- cal term matching and statistical machine translation. In this work, we handle the medical concept normalisation at the se- mantic level. We investigate the use of neural networks to learn the transition be- tween layman’s language used in social media messages and formal medical lan- guage used in the descriptions of medi- cal concepts in a standard ontology. We evaluate our approaches using three differ- ent datasets, where social media texts are extracted from Twitter messages and blog posts. Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines, which achieved state-of-the-art performance on several medical concept normalisation tasks, by up to 44%. ",,,,ACL
97,2016,Agreement-based Learning of Parallel Lexicons and Phrases from Non-Parallel Corpora,"Chunyang Liu, Yang Liu, Maosong Sun, Huanbo Luan","We introduce an agreement-based ap- proach to learning parallel lexicons and phrases from non-parallel corpora. The basic idea is to encourage two asym- metric latent-variable translation models (i.e., source-to-target and target-to-source) to agree on identifying latent phrase and word alignments. The agreement is de- fined at both word and phrase levels. We develop a Viterbi EM algorithm for jointly training the two unidirectional models ef- ficiently. Experiments on the Chinese- English dataset show that agreement- based learning significantly improves both alignment and translation performance. ",,,,ACL
98,2016,Deep Fusion LSTMs for Text Semantic Matching,"Pengfei Liu, Xipeng Qiu, Jifan Chen, Xuanjing Huang","Recently, there is rising interest in mod- elling the interactions of text pair with deep neural networks. In this paper, we propose a model of deep fusion LSTMs (DF-LSTMs) to model the strong inter- action of text pair in a recursive match- ing way. Specifically, DF-LSTMs con- sist of two interdependent LSTMs, each of which models a sequence under the in- fluence of another. We also use exter- nal memory to increase the capacity of LSTMs, thereby possibly capturing more complicated matching patterns. Experi- ments on two very large datasets demon- strate the efficacy of our proposed archi- tecture. Furthermore, we present an elab- orate qualitative analysis of our models, giving an intuitive understanding how our model worked. ",,,,ACL
99,2016,Understanding Discourse on Work and Job-Related Well-Being in Public Social Media,"Tong Liu, Christopher Homan, Cecilia Ovesdotter Alm, Megan Lytle","We construct a humans-in-the-loop super- vised learning framework that integrates crowdsourcing feedback and local knowl- edge to detect job-related tweets from in- dividual and business accounts. Using data-driven ethnography, we examine dis- course about work by fusing language- based analysis with temporal, geospa- tional, and labor statistics information. ",,,,ACL
100,2016,Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models,"Minh-Thang Luong, Christopher D. Manning","Nearly all previous work on neural ma- chine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word- character solution to achieving open vo- cabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recur- rent neural networks compute source word representations and recover unknown tar- get words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT’15 English to Czech translation task, this hybrid approach offers an ad- dition boost of +2.1?11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words. ",,,,ACL
101,2016,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,"Xuezhe Ma, Eduard Hovy","State-of-the-art sequence labeling systems traditionally require large amounts of task- specific knowledge in the form of hand- crafted features and data pre-processing. In this paper, we introduce a novel neu- tral network architecture that benefits from both word- and character-level representa- tions automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requir- ing no feature engineering or data pre- processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks — Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 cor- pus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets — 97.55% accuracy for POS tagging and 91.21% F1 for NER. ",,,,ACL
102,2016,Off-topic Response Detection for Spontaneous Spoken English Assessment,"Andrey Malinin, Rogier Van Dalen, Kate Knill, Yu Wang","Automatic spoken language assessment systems are becoming increasingly impor- tant to meet the demand for English sec- ond language learning. This is a challeng- ing task due to the high error rates of, even state-of-the-art, non-native speech recog- nition. Consequently current systems pri- marily assess fluency and pronunciation. However, content assessment is essential for full automation. As a first stage it is important to judge whether the speaker re- sponds on topic to test questions designed to elicit spontaneous speech. Standard ap- proaches to off-topic response detection assess similarity between the response and question based on bag-of-words represen- tations. An alternative framework based on Recurrent Neural Network Language Models (RNNLM) is proposed in this pa- per. The RNNLM is adapted to the topic of each test question. It learns to asso- ciate example responses to questions with points in a topic space constructed using these example responses. Classification is done by ranking the topic-conditional posterior probabilities of a response. The RNNLMs associate a broad range of re- sponses with each topic, incorporate se- quence information and scale better with additional training data, unlike standard methods. On experiments conducted on data from the Business Language Testing Service (BULATS) this approach outper- forms standard approaches. ",,,,ACL
103,2016,Synthesizing Compound Words for Machine Translation,"Austin Matthews, Eva Schlinger, Alon Lavie, Chris Dyer","Most machine translation systems con- struct translations from a closed vocabu- lary of target word forms, posing problems for translating into languages that have productive compounding processes. We present a simple and effective approach that deals with this problem in two phases. First, we build a classifier that identifies spans of the input text that can be trans- lated into a single compound word in the target language. Then, for each identi- fied span, we generate a pool of possible compounds which are added to the trans- lation model as “synthetic” phrase trans- lations. Experiments reveal that (i) we can effectively predict what spans can be compounded; (ii) our compound gener- ation model produces good compounds; and (iii) modest improvements are pos- sible in end-to-end English–German and English–Finnish translation tasks. We ad- ditionally introduce KomposEval, a new multi-reference dataset of English phrases and their translations into German com- pounds. ",,,,ACL
104,2016,Harnessing Cognitive Features for Sarcasm Detection,"Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal Dey","In this paper, we propose a novel mecha- nism for enriching the feature vector, for the task of sarcasm detection, with cogni- tive features extracted from eye-movement patterns of human readers. Sarcasm detec- tion has been a challenging research prob- lem, and its importance for NLP applica- tions such as review summarization, dia- log systems and sentiment analysis is well recognized. Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds. This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text. We observe the difference in the be- haviour of the eye, while reading sarcastic and non sarcastic sentences. Motivated by this observation, we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features ob- tained from readers eye movement data. We perform statistical classification using the enhanced feature set so obtained. The augmented cognitive features improve sar- casm detection by 3.7% (in terms of F- score), over the performance of the best reported system. ",,,,ACL
105,2016,End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures,"Makoto Miwa, Mohit Bansal","We present a novel end-to-end neural model to extract entities and relations be- tween them. Our recurrent neural net- work based model captures both word se- quence and dependency tree substructure information by stacking bidirectional tree- structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a sin- gle model. We further encourage detec- tion of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sam- pling. Our model improves over the state- of-the-art feature-based model on end-to- end relation extraction, achieving 12.1% and 5.7% relative error reductions in F1- score on ACE2005 and ACE2004, respec- tively. We also show that our LSTM- RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classifica- tion (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components. ",,,,ACL
106,2016,A short proof that O_2 is an MCFL,Mark-Jan Nederhof,"We present a new proof that O 2 is a mul- tiple context-free language. It contrasts with a recent proof by Salvati (2015) in its avoidance of concepts that seem specific to two-dimensional geometry, such as the complex exponential function. Our simple proof creates realistic prospects of widen- ing the results to higher dimensions. This finding is of central importance to the rela- tion between extreme free word order and classes of grammars used to describe the syntax of natural language. ",,,,ACL
107,2016,Context-aware Argumentative Relation Mining,"Huy Nguyen, Diane Litman","Context is crucial for identifying argumen- tative relations in text, but many argument mining methods make little use of contex- tual features. This paper presents context- aware argumentative relation mining that uses features extracted from writing top- ics as well as from windows of context sentences. Experiments on student essays demonstrate that the proposed features im- prove predictive performance in two argu- mentative relation classification tasks. ",,,,ACL
108,2016,Leveraging Inflection Tables for Stemming and Lemmatization.,"Garrett Nicolai, Grzegorz Kondrak","We present several methods for stemming and lemmatization based on discrimina- tive string transduction. We exploit the paradigmatic regularity of semi-structured inflection tables to identify stems in an un- supervised manner with over 85% accu- racy. Experiments on English, Dutch and German show that our stemmers substan- tially outperform Snowball and Morfes- sor, and approach the accuracy of a super- vised model. Furthermore, the generated stems are more consistent than those an- notated by experts. Our direct lemmatiza- tion model is more accurate than Morfette and Lemming on most datasets. Finally, we test our methods on the data from the shared task on morphological reinflection. ",,,,ACL
109,2016,Scaling a Natural Language Generation System,"Jonathan Pfeil, Soumya Ray","A key goal in natural language genera- tion (NLG) is to enable fast generation even with large vocabularies, grammars and worlds. In this work, we build upon a recently proposed NLG system, Sentence Tree Realization with UCT (STRUCT). We describe four enhancements to this system: (i) pruning the grammar based on the world and the communicative goal, (ii) intelligently caching and pruning the com- binatorial space of semantic bindings, (iii) reusing the lookahead search tree at differ- ent search depths, and (iv) learning and us- ing a search control heuristic. We evaluate the resulting system on three datasets of increasing size and complexity, the largest of which has a vocabulary of about 10K words, a grammar of about 32K lexical- ized trees and a world with about 11K enti- ties and 23K relations between them. Our results show that the system has a median generation time of 8.5s and finds the best sentence on average within 25s. These re- sults are based on a sequential, interpreted implementation and are significantly bet- ter than the state of the art for planning- based NLG systems. ",,,,ACL
110,2016,ALTO: Active Learning with Topic Overviews for Speeding Label Induction and Document Labeling,"Forough Poursabzi-Sangdeh, Jordan Boyd-Graber, Leah Findlater, Kevin Seppi","Effective text classification requires experts to annotate data with labels; these training data are time-consuming and expensive to obtain. If you know what labels you want, active learning can reduce the number of labeled documents needed. However, estab- lishing the label set remains difficult. An- notators often lack the global knowledge needed to induce a label set. We intro- duce ALTO : Active Learning with Topic Overviews, an interactive system to help humans annotate documents: topic mod- els provide a global overview of what la- bels to create and active learning directs them to the right documents to label. Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions, topic models (even by themselves) lead to better label sets, and ALTO ’s combination is best over- all. ",,,,ACL
111,2016,Predicting the Rise and Fall of Scientific Topics from Trends in their Rhetorical Framing,"Vinodkumar Prabhakaran, William L. Hamilton, Dan McFarland, Dan Jurafsky","Computationally modeling the evolution of science by tracking how scientific top- ics rise and fall over time has important implications for research funding and pub- lic policy. However, little is known about the mechanisms underlying topic growth and decline. We investigate the role of rhetorical framing: whether the rhetori- cal role or function that authors ascribe to topics (as methods, as goals, as results, etc.) relates to the historical trajectory of the topics. We train topic models and a rhetorical function classifier to map topic models onto their rhetorical roles in 2.4 million abstracts from the Web of Science from 1991-2010. We find that a topic’s rhetorical function is highly predictive of its eventual growth or decline. For exam- ple, topics that are rhetorically described as results tend to be in decline, while top- ics that function as methods tend to be in early phases of growth. ",,,,ACL
112,2016,Compositional Sequence Labeling Models for Error Detection in Learner Writing,"Marek Rei, Helen Yannakoudakis","In this paper, we present the first exper- iments using neural network models for the task of error detection in learner writ- ing. We perform a systematic comparison of alternative compositional architectures and propose a framework for error detec- tion based on bidirectional LSTMs. Ex- periments on the CoNLL-14 shared task dataset show the model is able to outper- form other participants on detecting er- rors in learner writing. Finally, the model is integrated with a publicly deployed self-assessment system, leading to perfor- mance comparable to human annotators. ",,,,ACL
113,2016,Neural Semantic Role Labeling with Dependency Path Embeddings,"Michael Roth, Mirella Lapata","This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the obser- vation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as sub- sequences of lexicalized dependency paths and learns suitable embedding representa- tions. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role la- belers, and showcase qualitative improve- ments obtained by our method. ",,,,ACL
114,2016,Prediction of Prospective User Engagement with Intelligent Assistants,"Shumpei Sano, Nobuhiro Kaji, Manabu Sassano","Intelligent assistants on mobile devices, such as Siri, have recently gained con- siderable attention as novel applications of dialogue technologies. A tremendous amount of real users of intelligent assis- tants provide us with an opportunity to ex- plore a novel task of predicting whether users will continually use their intelligent assistants in the future. We developed pre- diction models of prospective user engage- ment by using large-scale user logs ob- tained from a commercial intelligent as- sistant. Experiments demonstrated that our models can predict prospective user engagement reasonably well, and outper- forms a strong baseline that makes predic- tion based past utterance frequency. ",,,,ACL
115,2016,Resolving References to Objects in Photographs using the Words-As-Classifiers Model,"David Schlangen, Sina Zarrieß, Casey Kennington","A common use of language is to refer to visually present objects. Modelling it in computers requires modelling the link between language and perception. The “words as classifiers” model of grounded semantics views words as classifiers of perceptual contexts, and composes the meaning of a phrase through composition of the denotations of its component words. It was recently shown to perform well in a game-playing scenario with a small num- ber of object types. We apply it to two large sets of real-world photographs that contain a much larger variety of object types and for which referring expressions are available. Using a pre-trained convolu- tional neural network to extract image re- gion features, and augmenting these with positional information, we show that the model achieves performance competitive with the state of the art in a reference res- olution task (given expression, find bound- ing box of its referent), while, as we argue, being conceptually simpler and more flex- ible. ",,,,ACL
116,2016,RBPB: Regularization-Based Pattern Balancing Method for Event Extraction,"Lei Sha, Jing Liu, Chin-Yew Lin, Sujian Li","Event extraction is a particularly chal- lenging information extraction task, which intends to identify and classify event triggers and arguments from raw text. In recent works, when determining event types (trigger classification), most of the works are either pattern-only or feature-only. However, although patterns cannot cover all representations of an event, it is still a very important feature. In addition, when identifying and classifying arguments, previous works consider each candidate argument separately while ignoring the relationship between arguments. This paper proposes a Regularization-Based Pattern Balancing Method (RBPB). Inspired by the progress in representation learning, we use trigger embedding, sentence-level embedding and pattern features together as our features for trigger classification so that the effect of patterns and other useful features can be balanced. In addition, RBPB uses a regularization method to take advantage of the relationship between arguments. Experiments show that we achieve results better than current state-of-art equivalents. ",,,,ACL
117,2016,Neural Network-Based Model for Japanese Predicate Argument Structure Analysis,"Tomohide Shibata, Daisuke Kawahara, Sadao Kurohashi","This paper presents a novel model for Japanese predicate argument structure (PAS) analysis based on a neural network framework. Japanese PAS analysis is chal- lenging due to the tangled characteristics of the Japanese language, such as case dis- appearance and argument omission. To unravel this problem, we learn selectional preferences from a large raw corpus, and incorporate them into a SOTA PAS anal- ysis model, which considers the consis- tency of all PASs in a given sentence. We demonstrate that the proposed PAS anal- ysis model significantly outperforms the base SOTA system. ",,,,ACL
118,2016,Addressing Limited Data for Textual Entailment Across Domains,"Chaitanya Shivade, Preethi Raghavan, Siddharth Patwardhan","We seek to address the lack of labeled data (and high cost of annotation) for textual entailment in some domains. To that end, we first create (for experimental purposes) an entailment dataset for the clinical do- main, and a highly competitive supervised entailment system, E NT , that is effective (out of the box) on two domains. We then explore self-training and active learn- ing strategies to address the lack of la- beled data. With self-training, we success- fully exploit unlabeled data to improve over E NT by 15% F-score on the newswire domain, and 13% F-score on clinical data. On the other hand, our active learning ex- periments demonstrate that we can match (and even beat) E NT using only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain. ",,,,ACL
119,2016,Annotating and Predicting Non-Restrictive Noun Phrase Modifications,"Gabriel Stanovsky, Ido Dagan","The distinction between restrictive and non-restrictive modification in noun phrases is a well studied subject in linguistics. Automatically identifying non-restrictive modifiers can provide NLP applications with shorter, more salient arguments, which were found beneficial by several recent works. While previous work showed that restrictiveness can be annotated with high agreement, no large scale corpus was created, hindering the development of suitable classification algorithms. In this work we devise a novel crowdsourcing annotation methodology, and an accompanying large scale corpus. Then, we present a robust automated system which identifies non-restrictive modifiers, notably improving over prior methods. ",,,,ACL
120,2016,Bilingual Segmented Topic Model,"Akihiro Tamura, Eiichiro Sumita","This study proposes the bilingual seg- mented topic model (BiSTM), which hi- erarchically models documents by treat- ing each document as a set of segments, e.g., sections. While previous bilingual topic models, such as bilingual latent Dirichlet allocation (BiLDA) (Mimno et al., 2009; Ni et al., 2009), consider only cross-lingual alignments between entire documents, the proposed model consid- ers cross-lingual alignments between seg- ments in addition to document-level align- ments and assigns the same topic distri- bution to aligned segments. This study also presents a method for simultane- ously inferring latent topics and segmen- tation boundaries, incorporating unsuper- vised topic segmentation (Du et al., 2013) into BiSTM. Experimental results show that the proposed model significantly out- performs BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction (up to +0.083 extraction accuracy). ",,,,ACL
121,2016,Learning Semantically and Additively Compositional Distributional Representations,"Ran Tian, Naoaki Okazaki, Kentaro Inui","This paper connects a vector-based com- position model to a formal semantics, the Dependency-based Compositional Se- mantics (DCS). We show theoretical evi- dence that the vector compositions in our model conform to the logic of DCS. Ex- perimentally, we show that vector-based composition brings a strong ability to calculate similar phrases as similar vec- tors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence comple- tion task and report a new state-of-the-art. ",,,,ACL
122,2016,Inner Attention based Recurrent Neural Networks for Answer Selection,"Bingning Wang, Kang Liu, Jun Zhao","Attention based recurrent neural networks have shown advantages in representing natural language sentences (Hermann et al., 2015; Rockt¨aschel et al., 2015; Tan et al., 2015). Based on recurrent neural networks (RNN), external attention infor- mation was added to hidden representa- tions to get an attentive sentence represen- tation. Despite the improvement over non- attentive models, the attention mechanism under RNN is not well studied. In this work, we analyze the deficiency of tradi- tional attention based RNN models quanti- tatively and qualitatively. Then we present three new RNN models that add attention information before RNN hidden represen- tation, which shows advantage in repre- senting sentence and achieves new state- of-art results in answer selection task. ",,,,ACL
123,2016,Relation Classification via Multi-Level Attention CNNs,"Linlin Wang, Zhu Cao, Gerard de Melo, Zhiyuan Liu","Relation classification is a crucial ingredi- ent in numerous information extraction sys- tems seeking to mine structured facts from text. We propose a novel convolutional neural network architecture for this task, relying on two levels of attention in order to better discern patterns in heterogeneous contexts. This architecture enables end- to-end learning from task-specific labeled data, forgoing the need for external knowl- edge such as explicit dependency structures. Experiments show that our model outper- forms previous state-of-the-art methods, in- cluding those relying on much richer forms of prior knowledge. ",,,,ACL
124,2016,Knowledge Base Completion via Coupled Path Ranking,"Quan Wang, Jing Liu, Yuanfei Luo, Bin Wang","Knowledge bases (KBs) are often greatly incomplete, necessitating a demand for K- B completion. The path ranking algorith- m (PRA) is one of the most promising ap- proaches to this task. Previous work on PRA usually follows a single-task learn- ing paradigm, building a prediction mod- el for each relation independently with its own training data. It ignores meaningful associations among certain relations, and might not get enough training data for less frequent relations. This paper proposes a novel multi-task learning framework for PRA, referred to as coupled PRA (CPRA). It first devises an agglomerative clustering strategy to automatically discover relation- s that are highly correlated to each other, and then employs a multi-task learning s- trategy to effectively couple the prediction of such relations. As such, CPRA takes in- to account relation association and enables implicit data sharing among them. We empirically evaluate CPRA on benchmark data created from Freebase. Experimen- tal results show that CPRA can effective- ly identify coherent clusters in which rela- tions are highly correlated. By further cou- pling such relations, CPRA significantly outperforms PRA, in terms of both predic- tive accuracy and model interpretability. ",,,,ACL
125,2016,Larger-Context Language Modelling with Recurrent Neural Network,"Tian Wang, Kyunghyun Cho","In this work, we propose a novel method to incorporate corpus-level discourse infor- mation into language modelling. We call this larger-context language model. We in- troduce a late fusion approach to a recur- rent language model based on long short- term memory units (LSTM), which helps the LSTM unit keep intra-sentence depen- dencies and inter-sentence dependencies separate from each other. Through the evaluation on four corpora (IMDB, BBC, Penn TreeBank, and Fil9), we demonstrate that the proposed model improves per- plexity significantly. In the experiments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporat- ing additional inputs to the LSTM. By an- alyzing the trained larger-context language model, we discover that content words, in- cluding nouns, adjectives and verbs, bene- fit most from an increasing number of con- text sentences. This analysis suggests that larger-context language model improves the unconditional language model by cap- turing the theme of a document better and more easily. ",,,,ACL
126,2016,The Creation and Analysis of a Website Privacy Policy Corpus,"Shomir Wilson, Florian Schaub, Aswarth Abhilash Dara, Frederick Liu","Website privacy policies are often ignored by Internet users, because these docu- ments tend to be long and difficult to un- derstand. However, the significance of pri- vacy policies greatly exceeds the attention paid to them: these documents are binding legal agreements between website opera- tors and their users, and their opaqueness is a challenge not only to Internet users but also to policy regulators. One proposed al- ternative to the status quo is to automate or semi-automate the extraction of salient de- tails from privacy policy text, using a com- bination of crowdsourcing, natural lan- guage processing, and machine learning. However, there has been a relative dearth of datasets appropriate for identifying data practices in privacy policies. To remedy this problem, we introduce a corpus of 115 privacy policies (267K words) with man- ual annotations for 23K fine-grained data practices. We describe the process of us- ing skilled annotators and a purpose-built annotation tool to produce the data. We provide findings based on a census of the annotations and show results toward au- tomating the annotation procedure. Fi- nally, we describe challenges and oppor- tunities for the research community to use this corpus to advance research in both pri- vacy and language technologies. ",,,,ACL
127,2016,Sequence-based Structured Prediction for Semantic Parsing,"Chunyang Xiao, Marc Dymetman, Claire Gardent","We propose an approach for semantic parsing that uses a recurrent neural net- work to map a natural language question into a logical form representation of a KB query. Building on recent work by (Wang et al., 2015), the interpretable log- ical forms, which are structured objects obeying certain constraints, are enumer- ated by an underlying grammar and are paired with their canonical realizations. In order to use sequence prediction, we need to sequentialize these logical forms. We compare three sequentializations: a direct linearization of the logical form, a linearization of the associated canonical realization, and a sequence consisting of derivation steps relative to the underlying grammar. We also show how grammati- cal constraints on the derivation sequence can easily be integrated inside the RNN- based sequential predictor. Our experi- ments show important improvements over previous results for the same dataset, and also demonstrate the advantage of incor- porating the grammatical constraints. ",,,,ACL
128,2016,Learning Word Meta-Embeddings,"Wenpeng Yin, Hinrich Schütze","Word embeddings – distributed represen- tations of words – in deep learning are beneficial for many tasks in NLP. How- ever, different embedding sets vary greatly in quality and characteristics of the cap- tured information. Instead of relying on a more advanced algorithm for embed- ding learning, this paper proposes an en- semble approach of combining different public embedding sets with the aim of learning metaembeddings. Experiments on word similarity and analogy tasks and on part-of-speech tagging show better per- formance of metaembeddings compared to individual embedding sets. One advan- tage of metaembeddings is the increased vocabulary coverage. We release our metaembeddings publicly at http:// cistern.cis.lmu.de/meta-emb. ",,,,ACL
129,2016,Towards Constructing Sports News from Live Text Commentary,"Jianmin Zhang, Jin-ge Yao, Xiaojun Wan","In this paper, we investigate the possibil- ity to automatically generate sports news from live text commentary scripts. As a preliminary study, we treat this task as a special kind of document summarization based on sentence extraction. We for- mulate the task in a supervised learning to rank framework, utilizing both tradi- tional sentence features for generic docu- ment summarization and novelly designed task-specific features. To tackle the prob- lem of local redundancy, we also propose a probabilistic sentence selection algorithm. Experiments on our collected data from football live commentary scripts and cor- responding sports news demonstrate the feasibility of this task. Evaluation results show that our methods are indeed appro- priate for this task, outperforming several baseline methods in different aspects. ",,,,ACL
130,2016,A Continuous Space Rule Selection Model for Syntax-based Statistical Machine Translation,"Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Graham Neubig","One of the major challenges for statisti- cal machine translation (SMT) is to choose the appropriate translation rules based on the sentence context. This paper pro- poses a continuous space rule selection (CSRS) model for syntax-based SMT to perform this context-dependent rule selec- tion. In contrast to existing maximum en- tropy based rule selection (MERS) mod- els, which use discrete representations of words as features, the CSRS model is learned by a feed-forward neural network and uses real-valued vector representa- tions of words, allowing for better gen- eralization. In addition, we propose a method to train the rule selection models only on minimal rules, which are more fre- quent and have richer training data com- pared to non-minimal rules. We tested our model on different translation tasks and the CSRS model outperformed a base- line without rule selection and the previ- ous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. ",,,,ACL
131,2016,Probabilistic Graph-based Dependency Parsing with Convolutional Neural Network,"Zhisong Zhang, Hai Zhao, Lianhui Qin","This paper presents neural probabilistic parsing models which explore up to third- order graph-based parsing with maximum likelihood training criteria. Two neural network extensions are exploited for per- formance improvement. Firstly, a convo- lutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effec- tively captured. Secondly, a linear layer is added to integrate different order neu- ral models and trained with perceptron method. The proposed parsers are evalu- ated on English and Chinese Penn Tree- banks and obtain competitive accuracies. ",,,,ACL
132,2016,A Search-Based Dynamic Reranking Model for Dependency Parsing,"Hao Zhou, Yue Zhang, Shujian Huang, Junsheng Zhou","We propose a novel reranking method to extend a deterministic neural dependency parser. Different to conventional k-best reranking, the proposed model integrates search and learning by utilizing a dynamic action revising process, using the rerank- ing model to guide modification for the base outputs and to rerank the candidates. The dynamic reranking model achieves an absolute 1.78% accuracy improvement over the deterministic baseline parser on PTB, which is the highest improvement by neural rerankers in the literature. ",,,,ACL
133,2016,Cross-Lingual Sentiment Classification with Bilingual Document Representation Learning,"Xinjie Zhou, Xiaojun Wan, Jianguo Xiao","Cross-lingual sentiment classification aims to adapt the sentiment resource in a resource-rich language to a resource-poor language. In this study, we propose a representation learning approach which simultaneously learns vector representa- tions for the texts in both the source and the target languages. Different from pre- vious research which only gets bilingual word embedding, our Bilingual Document Representation Learning model BiDRL directly learns document representations. Both semantic and sentiment correlations are utilized to map the bilingual texts into the same embedding space. The experiments are based on the multilingual multi-domain Amazon review dataset. We use English as the source language and use Japanese, German and French as the target languages. The experimental results show that BiDRL outperforms the state-of-the-art methods for all the target languages. ",,,,ACL
134,2016,Segment-Level Sequence Modeling using Gated Recursive Semi-Markov Conditional Random Fields,"Jingwei Zhuo, Yong Cao, Jun Zhu, Bo Zhang","Most of the sequence tagging tasks in nat- ural language processing require to recog- nize segments with certain syntactic role or semantic meaning in a sentence. They are usually tackled with Conditional Ran- dom Fields (CRFs), which do indirect word-level modeling over word-level fea- tures and thus cannot make full use of segment-level information. Semi-Markov Conditional Random Fields (Semi-CRFs) model segments directly but extracting segment-level features for Semi-CRFs is still a very challenging problem. This pa- per presents Gated Recursive Semi-CRFs (grSemi-CRFs), which model segments directly and automatically learn segment- level features through a gated recursive convolutional neural network. Our exper- iments on text chunking and named en- tity recognition (NER) demonstrate that grSemi-CRFs generally outperform other neural models. ",,,,ACL
135,2016,Identifying Causal Relations Using Parallel Wikipedia Articles,"Christopher Hidey, Kathy McKeown","The automatic detection of causal relation- ships in text is important for natural lan- guage understanding. This task has proven to be difficult, however, due to the need for world knowledge and inference. We fo- cus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality. Unlike the explicit markers, a closed class, these markers vary signifi- cantly in their linguistic forms. We lever- age parallel Wikipedia corpora to identify new markers that are variations on known causal phrases, creating a training set via distant supervision. We also train a causal classifier using features from the open class markers and semantic features pro- viding contextual information. The results show that our features provide an 11.05 point absolute increase over the baseline on the task of identifying causality in text. ",,,,ACL
136,2016,Compositional Learning of Embeddings for Relation Paths in Knowledge Base and Text,"Kristina Toutanova, Victoria Lin, Wen-tau Yih, Hoifung Poon","Modeling relation paths has offered sig- nificant gains in embedding models for knowledge base (KB) completion. How- ever, enumerating paths between two en- tities is very expensive, and existing ap- proaches typically resort to approxima- tion with a sampled subset. This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts men- tioned in it. In this paper, we propose the first exact dynamic programming al- gorithm which enables efficient incorpo- ration of all relation paths of bounded length, while modeling both relation types and intermediate nodes in the composi- tional path representations. We conduct a theoretical analysis of the efficiency gain from the approach. Experiments on two datasets show that it addresses representa- tional limitations in prior approaches and improves accuracy in KB completion. ",,,,ACL
137,2016,Commonsense Knowledge Base Completion,"Xiang Li, Aynaz Taheri, Lifu Tu, Kevin Gimpel","We enrich a curated resource of common- sense knowledge by formulating the prob- lem as one of knowledge base comple- tion (KBC). Most work in KBC focuses on knowledge bases like Freebase that re- late entities drawn from a fixed set. How- ever, the tuples in ConceptNet (Speer and Havasi, 2012) define relations between an unbounded set of phrases. We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones. We find strong performance from a bilinear model using a simple additive architecture to model phrases. We manually evaluate our trained model’s ability to assign quality scores to novel tuples, finding that it can propose tu- ples at the same quality level as medium- confidence tuples from ConceptNet. ",,,,ACL
138,2016,Simpler Context-Dependent Logical Forms via Model Projections,"Reginald Long, Panupong Pasupat, Percy Liang","We consider the task of learning a context- dependent mapping from utterances to de- notations. With only denotations at train- ing time, we must search over a combina- torially large space of logical forms, which is even larger with context-dependent ut- terances. To cope with this challenge, we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms. Though less expressive, we find that these simpler models are much faster and can be surprisingly effective. Moreover, they can be used to bootstrap the full model. Finally, we collected three new context- dependent semantic parsing datasets, and develop a new left-to-right parser. ",,,,ACL
139,2016,A Fast Unified Model for Parsing and Sentence Understanding,"Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta","Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they su?er from two key techni- cal problems that make them slow and un- wieldy for large-scale NLP tasks: they usu- ally operate on parsed sentences and they do not directly support batched computa- tion. We address these issues by introduc- ing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift- reduce parser. Our model supports batched computation for a speedup of up to 25× over other tree-structured models, and its integrated parser can operate on unparsed data with little loss in accuracy. We evalu- ate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models. ",,,,ACL
140,2016,Investigating Language Universal and Specific Properties in Word Embeddings,"Peng Qian, Xipeng Qiu, Xuanjing Huang","Recently, many NLP tasks have benefit- ed from distributed word representation. However, it remains unknown whether embedding models are really immune to the typological diversity of languages, despite the language-independent archi- tecture. Here we investigate three repre- sentative models on a large set of language samples by mapping dense embedding to sparse linguistic property space. Experi- ment results reveal the language universal and specific properties encoded in various word representation. Additionally, strong evidence supports the utility of word form, especially for inflectional languages. ",,,,ACL
141,2016,Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change,"William L. Hamilton, Jure Leskovec, Dan Jurafsky","Understanding how words change their meanings over time is key to models of language and cultural evolution, but his- torical data on meaning is scarce, mak- ing theories hard to develop and test. Word embeddings show promise as a di- achronic tool, but have not been carefully evaluated. We develop a robust method- ology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evo- lution. Using six historical corpora span- ning four languages and two centuries, we propose two quantitative laws of seman- tic change: (i) the law of conformity—the rate of semantic change scales with an in- verse power-law of word frequency; (ii) the law of innovation—independent of fre- quency, words that are more polysemous have higher rates of semantic change. ",,,,ACL
142,2016,"Beyond Plain Spatial Knowledge: Determining Where Entities Are and Are Not Located, and For How Long","Alakananda Vempala, Eduardo Blanco","This paper complements semantic role representations with spatial knowledge be- yond indicating plain locations. Namely, we extract where entities are (and are not) located, and for how long (seconds, hours, days, etc.). Crowdsourced annotations show that this additional knowledge is in- tuitive to humans and can be annotated by non-experts. Experimental results show that the task can be automated. ",,,,ACL
143,2016,LexSemTm: A Semantic Dataset Based on All-words Unsupervised Sense Distribution Learning,"Andrew Bennett, Timothy Baldwin, Jey Han Lau, Diana McCarthy","There has recently been a lot of interest in unsupervised methods for learning sense distributions, particularly in applications where sense distinctions are needed. This paper analyses a state-of-the-art method for sense distribution learning, and op- timises it for application to the entire vocabulary of a given language. The optimised method is then used to pro- duce L EX S EM TM: a sense frequency and semantic dataset of unprecedented size, spanning approximately 88% of polyse- mous, English simplex lemmas, which is released as a public resource to the com- munity. Finally, the quality of this data is investigated, and the L EX S EM TM sense distributions are shown to be superior to those based on the W ORD N ET first sense for lemmas missing from S EM C OR , and at least on par with S EM C OR -based distribu- tions otherwise. ",,,,ACL
144,2016,The LAMBADA dataset: Word prediction requiring a broad discourse context,"Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham","We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages shar- ing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preced- ing the target word. To succeed on LAM- BADA, computational models cannot sim- ply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA ex- emplifies a wide range of linguistic phe- nomena, and that none of several state-of- the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the develop- ment of new models capable of genuine understanding of broad context in natural language text. ",,,,ACL
145,2016,WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia,"Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin","We present W IKI R EADING , a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the cor- responding Wikipedia articles. The task contains a rich variety of challenging clas- sification and extraction sub-tasks, mak- ing it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNN- based architectures for document classifi- cation, information extraction, and ques- tion answering. We find that models sup- porting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level se- quence to sequence model with a mecha- nism to copy out-of-vocabulary words, ob- tains an accuracy of 71.8%. ",,,,ACL
146,2016,Optimizing Spectral Learning for Parsing,"Shashi Narayan, Shay B. Cohen","We describe a search algorithm for opti- mizing the number of latent states when estimating latent-variable PCFGs with spectral methods. Our results show that contrary to the common belief that the number of latent states for each nontermi- nal in an L-PCFG can be decided in isola- tion with spectral methods, parsing results significantly improve if the number of la- tent states for each nonterminal is globally optimized, while taking into account in- teractions between the different nontermi- nals. In addition, we contribute an empiri- cal analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Ko- rean, Polish and Swedish. Our results show that our estimation consistently per- forms better or close to coarse-to-fine expectation-maximization techniques for these languages. ",,,,ACL
147,2016,Stack-propagation: Improved Representation Learning for Syntax,"Yuan Zhang, David Weiss","Traditional syntax models typically lever- age part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a reg- ularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call “stack-propagation”. We apply this to de- pendency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 lan- guages from the Universal Dependencies, our method is 1.3% (absolute) more accu- rate than a state-of-the-art graph-based ap- proach and 2.7% more accurate than the most comparable greedy model. ",,,,ACL
148,2016,Inferring Perceived Demographics from User Emotional Tone and User-Environment Emotional Contrast,"Svitlana Volkova, Yoram Bachrach","We examine communications in a social network to study user emotional contrast – the propensity of users to express dif- ferent emotions than those expressed by their neighbors. Our analysis is based on a large Twitter dataset, consisting of the tweets of 123,513 users from the USA and Canada. Focusing on Ekman’s basic emo- tions, we analyze differences between the emotional tone expressed by these users and their neighbors of different types, and correlate these differences with perceived user demographics. We demonstrate that many perceived demographic traits corre- late with the emotional contrast between users and their neighbors. Unlike other ap- proaches on inferring user attributes that rely solely on user communications, we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors. ",,,,ACL
149,2016,Prototype Synthesis for Model Laws,"Matthew Burgess, Eugenia Giraudy, Eytan Adar","State legislatures often rely on existing text when drafting new bills. Resource and expertise constraints, which often drive this copying behavior, can be taken ad- vantage of by lobbyists and special inter- est groups. These groups provide model bills, which encode policy agendas, with the intent that the models become actual law. Unfortunately, model legislation is often opaque to the public–both in source and content. In this paper we present L OBBY B ACK , a system that reverse en- gineers model legislation from observed text. L OBBY B ACK identifies clusters of bills which have text reuse and gener- ates “prototypes” that represent a canon- ical version of the text shared between the documents. We demonstrate that L OBBY - B ACK accurately reconstructs model leg- islation and apply it to a dataset of over 550k bills. ",,,,ACL
150,2016,Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM,"Ivan Habernal, Iryna Gurevych","We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments, namely their convincingness. We cast the problem as relation classifica- tion, where a pair of arguments having the same stance to the same prompt is judged. We annotate a large datasets of 16k pairs of arguments over 32 topics and investi- gate whether the relation “A is more con- vincing than B” exhibits properties of total ordering; these findings are used as global constraints for cleaning the crowdsourced data. We propose two tasks: (1) predicting which argument from an argument pair is more convincing and (2) ranking all argu- ments to the topic based on their convinc- ingness. We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spear- man’s correlation in a cross-topic evalua- tion. We release the newly created corpus UKPConvArg1 and the experimental soft- ware under open licenses. ",,,,ACL
151,2016,Discovery of Treatments from Text Corpora,"Christian Fong, Justin Grimmer","An extensive literature in computational social science examines how features of messages, advertisements, and other cor- pora affect individuals’ decisions, but these analyses must specify the relevant features of the text before the experiment. Automated text analysis methods are able to discover features of text, but these meth- ods cannot be used to obtain the estimates of causal effects—the quantity of inter- est for applied researchers. We introduce a new experimental design and statistical model to simultaneously discover treat- ments in a corpora and estimate causal ef- fects for these discovered treatments. We prove the conditions to identify the treat- ment effects of texts and introduce the su- pervised Indian Buffet process to discover those treatments. Our method enables us to discover treatments in a training set us- ing a collection of texts and individuals’ responses to those texts, and then esti- mate the effects of these interventions in a test set of new texts and survey respon- dents. We apply the model to an exper- iment about candidate biographies, recov- ering intuitive features of voters’ decisions and revealing a penalty for lawyers and a bonus for military service. ",,,,ACL
152,2016,Learning Structured Predictors from Bandit Feedback for Interactive NLP,"Artem Sokolov, Julia Kreutzer, Christopher Lo, Stefan Riezler","Structured prediction from bandit feed- back describes a learning scenario where instead of having access to a gold standard structure, a learner only receives partial feedback in form of the loss value of a pre- dicted structure. We present new learning objectives and algorithms for this inter- active scenario, focusing on convergence speed and ease of elicitability of feed- back. We present supervised-to-bandit simulation experiments for several NLP tasks (machine translation, sequence la- beling, text classification), showing that bandit learning from relative preferences eases feedback strength and yields im- proved empirical convergence. ",,,,ACL
153,2016,Deep Reinforcement Learning with a Natural Language Action Space,"Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao","This paper introduces a novel architec- ture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture repre- sents action and state spaces with sepa- rate embedding vectors, which are com- bined with an interaction function to ap- proximate the Q-function in reinforce- ment learning. We evaluate the DRRN on two popular text games, showing su- perior performance over other deep Q- learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text. ",,,,ACL
154,2016,Incorporating Copying Mechanism in Sequence-to-Sequence Learning,"Jiatao Gu, Zhengdong Lu, Hang Li, Victor O.K. Li","We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which cer- tain segments in the input sequence are selectively replicated in the output se- quence. A similar phenomenon is ob- servable in human language communica- tion. For example, humans tend to re- peat entity names or even long phrases in conversation. The challenge with re- gard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network- based Seq2Seq learning and propose a new model called C OPY N ET with encoder- decoder structure. C OPY N ET can nicely integrate the regular way of word gener- ation in the decoder with the new copy- ing mechanism which can choose sub- sequences in the input sequence and put them at proper places in the output se- quence. Our empirical study on both syn- thetic data sets and real world data sets demonstrates the efficacy of C OPY N ET . For example, C OPY N ET can outperform regular RNN-based model with remark- able margins on text summarization tasks. ",,,,ACL
155,2016,Cross-domain Text Classification with Multiple Domains and Disparate Label Sets,"Himanshu Sharad Bhatt, Manjira Sinha, Shourya Roy","Advances in transfer learning have let go the limitations of traditional supervised machine learning algorithms for being de- pendent on annotated training data for training new models for every new do- main. However, several applications en- counter scenarios where models need to transfer/adapt across domains when the la- bel sets vary both in terms of count of la- bels as well as their connotations. This pa- per presents first-of-its-kind transfer learn- ing algorithm for cross-domain classifica- tion with multiple source domains and dis- parate label sets. It starts with identifying transferable knowledge from across multi- ple domains that can be useful for learning the target domain task. This knowledge in the form of selective labeled instances from different domains is congregated to form an auxiliary training set which is used for learning the target domain task. Experimental results validate the efficacy of the proposed algorithm against strong baselines on a real world social media and the 20 Newsgroups datasets. ",,,,ACL
156,2016,Morphological Smoothing and Extrapolation of Word Embeddings,"Ryan Cotterell, Hinrich Schütze, Jason Eisner","Languages with rich inflectional morphol- ogy exhibit lexical data sparsity, since the word used to express a given concept will vary with the syntactic context. For in- stance, each count noun in Czech has 12 forms (where English uses only singular and plural). Even in large corpora, we are un- likely to observe all inflections of a given lemma. This reduces the vocabulary cover- age of methods that induce continuous rep- resentations for words from distributional corpus information. We solve this prob- lem by exploiting existing morphological resources that can enumerate a word’s com- ponent morphemes. We present a latent- variable Gaussian graphical model that al- lows us to extrapolate continuous represen- tations for words not observed in the train- ing corpus, as well as smoothing the repre- sentations provided for the observed words. The latent variables represent embeddings of morphemes, which combine to create em- beddings of words. Over several languages and training sizes, our model improves the embeddings for words, when evaluated on an analogy task, skip-gram predictive accu- racy, and word similarity. ",,,,ACL
157,2016,Cross-lingual Models of Word Embeddings: An Empirical Comparison,"Shyam Upadhyay, Manaal Faruqui, Chris Dyer, Dan Roth","Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive eval- uation of four popular approaches of in- ducing cross-lingual embeddings, each re- quiring a different form of supervision, on four typologically different language pairs. Our evaluation setup spans four dif- ferent tasks, including intrinsic evaluation on mono-lingual and cross-lingual simi- larity, and extrinsic evaluation on down- stream semantic and syntactic applica- tions. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply super- vised models often prove competitive on certain tasks. ",,,,ACL
158,2016,"Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning","Ekaterina Vylomova, Laura Rimell, Trevor Cohn, Timothy Baldwin","Recent work has shown that simple vector subtraction over word embeddings is surpris- ingly effective at capturing different lexical relations, despite lacking explicit supervision. Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated. In this paper, we carry out such an evaluation in two learning settings: (1) spectral clustering to induce word rela- tions, and (2) supervised learning to classify vector differences into relation types. We find that word embeddings capture a surprising amount of information, and that, under suit- able supervised training, vector subtraction generalises well to a broad range of relations, including over unseen lexical items. ",,,,ACL
159,2016,Minimum Risk Training for Neural Machine Translation,"Shiqi Shen, Yong Cheng, Zhongjun He, Wei He","We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is ca- pable of optimizing model parameters di- rectly with respect to arbitrary evaluation metrics, which are not necessarily differ- entiable. Experiments show that our ap- proach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks. ",,,,ACL
160,2016,A Character-level Decoder without Explicit Segmentation for Neural Machine Translation,"Junyoung Chung, Kyunghyun Cho, Yoshua Bengio","The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder– decoder with a subword-level encoder and a character-level decoder on four language pairs–En-Cs, En-De, En-Ru and En-Fi– using the parallel corpora from WMT’15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Further- more, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine trans- lation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru. ",,,,ACL
161,2016,Target-Side Context for Discriminative Models in Statistical Machine Translation,"Aleš Tamchyna, Alexander Fraser, Ondřej Bojar, Marcin Junczys-Dowmunt","Discriminative translation models utiliz- ing source context have been shown to help statistical machine translation perfor- mance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the de- coding process. Our approach scales to large training data sizes and results in con- sistent improvements in translation qual- ity on four language pairs. We also pro- vide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target- context model and we show that our ex- tension allows us to better capture mor- phological coherence. Our work is freely available as part of Moses. ",,,,ACL
162,2016,Neural Machine Translation of Rare Words with Subword Units,"Rico Sennrich, Barry Haddow, Alexandra Birch","Neural machine translation (NMT) mod- els typically operate with a fixed vocabu- lary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this pa- per, we introduce a simpler and more ef- fective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as se- quences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via com- positional translation), and cognates and loanwords (via phonological and morpho- logical transformations). We discuss the suitability of different word segmentation techniques, including simple character n- gram models and a segmentation based on the byte pair encoding compression algo- rithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 B LEU , respectively. ",,,,ACL
163,2016,Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network,"Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu","Word pairs, which are one of the most eas- ily accessible features between two text segments, have been proven to be very useful for detecting the discourse relations held between text segments. However, be- cause of the data sparsity problem, the performance achieved by using word pair features is limited. In this paper, in or- der to overcome the data sparsity problem, we propose the use of word embeddings to replace the original words. Moreover, we adopt a gated relevance network to capture the semantic interaction between word pairs, and then aggregate those se- mantic interactions using a pooling layer to select the most informative interactions. Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recog- nizing the discourse level relations in all of the relations. ",,,,ACL
164,2016,Model Architectures for Quotation Detection,"Christian Scheible, Roman Klinger, Sebastian Padó","Quotation detection is the task of locating spans of quoted speech in text. The state of the art treats this problem as a sequence labeling task and employs linear-chain con- ditional random fields. We question the ef- ficacy of this choice: The Markov assump- tion in the model prohibits it from making joint decisions about the begin, end, and internal context of a quotation. We per- form an extensive analysis with two new model architectures. We find that (a), sim- ple boundary classification combined with a greedy prediction strategy is competitive with the state of the art; (b), a semi-Markov model significantly outperforms all others, by relaxing the Markov assumption. ",,,,ACL
165,2016,Speech Act Modeling of Written Asynchronous Conversations with Task-Specific Embeddings and Conditional Structured Models,"Shafiq Joty, Enamul Hoque","This paper addresses the problem of speech act recognition in written asyn- chronous conversations (e.g., fora, emails). We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversa- tional dependencies between sentences. Our models use sentence representations encoded by a long short term memory (LSTM) recurrent neural model. Empir- ical evaluation shows the effectiveness of our approach over existing ones: (i) LSTMs provide better task-specific representations, and (ii) the global joint model improves over local models. ",,,,ACL
166,2016,Situation entity types: automatic classification of clause-level aspect,"Annemarie Friedrich, Alexis Palmer, Manfred Pinkal","This paper describes the first robust ap- proach to automatically labeling clauses with their situation entity type (Smith, 2003), capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure. Previous work on this task used a small data set from a limited domain, and relied mainly on words as features, an approach which is impractical in larger settings. We pro- vide a new corpus of texts from 13 genres (40,000 clauses) annotated with situation entity types. We show that our sequence labeling approach using distributional in- formation in the form of Brown clusters, as well as syntactic-semantic features tar- geted to the task, is robust across genres, reaching accuracies of up to 76%. ",,,,ACL
167,2016,Learning Prototypical Event Structure from Photo Albums,"Antoine Bosselut, Jianfu Chen, David Warren, Hannaneh Hajishirzi","Activities and events in our lives are struc- tural, be it a vacation, a camping trip, or a wedding. While individual details vary, there are characteristic patterns that are specific to each of these scenarios. For ex- ample, a wedding typically consists of a sequence of events such as walking down the aisle, exchanging vows, and dancing. In this paper, we present a data-driven ap- proach to learning event knowledge from a large collection of photo albums. We for- mulate the task as constrained optimiza- tion to induce the prototypical temporal structure of an event, integrating both vi- sual and textual cues. Comprehensive evaluation demonstrates that it is possible to learn multimodal knowledge of event structure from noisy web content. ",,,,ACL
168,2016,Cross-Lingual Image Caption Generation,"Takashi Miyazaki, Nobuyuki Shimizu","Automatically generating a natural lan- guage description of an image is a fun- damental problem in artificial intelligence. This task involves both computer vision and natural language processing and is called “image caption generation.” Re- search on image caption generation has typically focused on taking in an image and generating a caption in English as ex- isting image caption corpora are mostly in English. The lack of corpora in languages other than English is an issue, especially for morphologically rich languages such as Japanese. There is thus a need for cor- pora sufficiently large for image caption- ing in other languages. We have developed a Japanese version of the MS COCO cap- tion dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese ver- sion of the dataset to generate a caption in Japanese. As the Japanese portion of the corpus is small, our model was de- signed to transfer the knowledge represen- tation obtained from the English portion into the Japanese portion. Experiments showed that the resulting bilingual compa- rable corpus has better performance than a monolingual corpus, indicating that image understanding using a resource-rich lan- guage benefits a resource-poor language. ",,,,ACL
169,2016,Learning Concept Taxonomies from Multi-modal Data,"Hao Zhang, Zhiting Hu, Yuntian Deng, Mrinmaya Sachan","We study the problem of automatically building hypernym taxonomies from tex- tual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leverag- ing text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representa- tions of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evalu- ate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap. ",,,,ACL
170,2016,Generating Natural Questions About an Image,"Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell","There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription, and answering ques- tions about images. These tasks have fo- cused on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image. In this paper, we introduce the novel task of Visual Question Gener- ation (VQG), where the system is tasked with asking a natural and engaging ques- tion when shown an image. We provide three datasets which cover a variety of im- ages from object-centric to event-centric, with considerably more abstract training data than provided to state-of-the-art cap- tioning systems thus far. We train and test several generative and retrieval mod- els to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions for a variety of im- ages, there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the commu- nity which we hope furthers interest in ex- ploring deeper connections between vision & language. ",,,,ACL
171,2016,Physical Causality of Action Verbs in Grounded Language Understanding,"Qiaozi Gao, Malcolm Doering, Shaohua Yang, Joyce Chai","Linguistics studies have shown that action verbs often denote some Change of State (CoS) as the result of an action. However, the causality of action verbs and its poten- tial connection with the physical world has not been systematically explored. To ad- dress this limitation, this paper presents a study on physical causality of action verbs and their implied changes in the physi- cal world. We first conducted a crowd- sourcing experiment and identified eigh- teen categories of physical causality for action verbs. For a subset of these cat- egories, we then defined a set of detec- tors that detect the corresponding change from visual perception of the physical en- vironment. We further incorporated phys- ical causality modeling and state detec- tion in grounded language understanding. Our empirical studies have demonstrated the effectiveness of causality modeling in grounding language to perception. ",,,,ACL
172,2016,Optimizing an Approximation of ROUGE - a Problem-Reduction Approach to Extractive Multi-Document Summarization,"Maxime Peyrard, Judith Eckle-Kohler","This paper presents a problem-reduction approach to extractive multi-document summarization: we propose a reduction to the problem of scoring individual sen- tences with their ROUGE scores based on supervised learning. For the summariza- tion, we solve an optimization problem where the ROUGE score of the selected summary sentences is maximized. To this end, we derive an approximation of the ROUGE-N score of a set of sentences, and define a principled discrete optimization problem for sentence selection. Mathe- matical and empirical evidence suggests that the sentence selection step is solved almost exactly, thus reducing the problem to the sentence scoring task. We perform a detailed experimental evaluation on two DUC datasets to demonstrate the validity of our approach. ",,,,ACL
173,2016,Phrase Structure Annotation and Parsing for Learner English,"Ryo Nagata, Keisuke Sakaguchi","There has been almost no work on phrase structure annotation and parsing specially designed for learner English despite the fact that they are useful for representing the structural characteristics of learner En- glish. To address this problem, in this pa- per, we first propose a phrase structure an- notation scheme for learner English and annotate two different learner corpora us- ing it. Second, we show their usefulness, reporting on (a) inter-annotator agreement rate, (b) characteristic CFG rules in the corpora, and (c) parsing performance on them. In addition, we explore methods to improve phrase structure parsing for learner English (achieving an F -measure of 0.878). Finally, we release the full annotation guidelines, the annotated data, and the improved parser model for learner English to the public. ",,,,ACL
174,2016,A Trainable Spaced Repetition Model for Language Learning,"Burr Settles, Brendan Meeder","We present half-life regression (HLR), a novel model for spaced repetition practice with applications to second language ac- quisition. HLR combines psycholinguis- tic theory with modern machine learning techniques, indirectly estimating the “half- life” of a word or concept in a student’s long-term memory. We use data from Duolingo — a popular online language learning application — to fit HLR models, reducing error by 45%+ compared to sev- eral baselines at predicting student recall rates. HLR model weights also shed light on which linguistic concepts are system- atically challenging for second language learners. Finally, HLR was able to im- prove Duolingo daily student engagement by 12% in an operational user study. ",,,,ACL
175,2016,User Modeling in Language Learning with Macaronic Texts,"Adithya Renduchintala, Rebecca Knowles, Philipp Koehn, Jason Eisner","Foreign language learners can acquire new vocabulary by using cognate and con- text clues when reading. To measure such incidental comprehension, we devise an experimental framework that involves reading mixed-language “macaronic” sen- tences. Using data collected via Ama- zon Mechanical Turk, we train a graphi- cal model to simulate a human subject’s comprehension of foreign words, based on cognate clues (edit distance to an English word), context clues (pointwise mutual in- formation), and prior exposure. Our model does a reasonable job at predicting which words a user will be able to understand, which should facilitate the automatic con- struction of comprehensible text for per- sonalized foreign language education. ",,,,ACL
176,2016,"On the Similarities Between Native, Non-native and Translated Texts","Ella Rabinovich, Sergiu Nisioi, Noam Ordan, Shuly Wintner","We present a computational analysis of three language varieties: native, advanced non-native, and translation. Our goal is to investigate the similarities and differ- ences between non-native language pro- ductions and translations, contrasting both with native language. Using a collec- tion of computational methods we estab- lish three main results: (1) the three types of texts are easily distinguishable; (2) non- native language and translations are closer to each other than each of them is to native language; and (3) some of these character- istics depend on the source or native lan- guage, while others do not, reflecting, per- haps, unified principles that similarly af- fect translations and non-native language. ",,,,ACL
177,2016,Learning Text Pair Similarity with Context-sensitive Autoencoders,"Hadi Amiri, Philip Resnik, Jordan Boyd-Graber, Hal Daumé III","We present a pairwise context-sensitive Autoencoder for computing text pair sim- ilarity. Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs. Our model outperforms the state-of-the-art models in two semantic re- trieval tasks and a contextual word simi- larity task. For retrieval, our unsupervised approach that merely ranks inputs with re- spect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art su- pervised models and in some cases outper- forms them. ",,,,ACL
178,2016,Linguistic Benchmarks of Online News Article Quality,"Ioannis Arapakis, Filipa Peleja, Barla Berkant, Joao Magalhaes","Online news editors ask themselves the same question many times: what is miss- ing in this news article to go online? This is not an easy question to be answered by computational linguistic methods. In this work, we address this important question and characterise the constituents of news article editorial quality. More specifically, we identify 14 aspects related to the con- tent of news articles. Through a correla- tion analysis, we quantify their indepen- dence and relation to assessing an article’s editorial quality. We also demonstrate that the identified aspects, when combined to- gether, can be used effectively in quality control methods for online news. ",,,,ACL
179,2016,Alleviating Poor Context with Background Knowledge for Named Entity Disambiguation,"Ander Barrena, Aitor Soroa, Eneko Agirre","Named Entity Disambiguation (NED) al- gorithms disambiguate mentions of named entities with respect to a knowledge-base, but sometimes the context might be poor or misleading. In this paper we introduce the acquisition of two kinds of background information to alleviate that problem: en- tity similarity and selectional preferences for syntactic positions. We show, using a generative N¨aive Bayes model for NED, that the additional sources of context are complementary, and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets, yielding the third best and the best results, respectively. We provide ex- amples and analysis which show the value of the acquired background information. ",,,,ACL
180,2016,Mining Paraphrasal Typed Templates from a Plain Text Corpus,"Or Biran, Terra Blevins, Kathleen McKeown","Finding paraphrases in text is an impor- tant task with implications for genera- tion, summarization and question answer- ing, among other applications. Of par- ticular interest to those applications is the specific formulation of the task where the paraphrases are templated, which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities. Previous work has fo- cused on mining paraphrases from parallel and comparable corpora, or mining very short sub-sentence synonyms and para- phrases. In this paper we present an ap- proach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates, utilizing a rich type system for the slots, from a plain text corpus. ",,,,ACL
181,2016,How to Train Dependency Parsers with Inexact Search for Joint Sentence Boundary Detection and Parsing of Entire Documents,"Anders Björkelund, Agnieszka Faleńska, Wolfgang Seeker, Jonas Kuhn","We cast sentence boundary detection and syntactic parsing as a joint problem, so an entire text document forms a training instance for transition-based dependency parsing. When trained with an early up- date or max-violation strategy for inexact search, we observe that only a tiny part of these very long training instances is ever exploited. We demonstrate this effect by extending the ArcStandard transition sys- tem with swap for the joint prediction task. When we use an alternative update strat- egy, our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation. A comparison between a standard pipeline and our joint model furthermore empirically shows the usefulness of syntactic information on the task of sentence boundary detection. ",,,,ACL
182,2016,MUTT: Metric Unit TesTing for Language Generation Tasks,"William Boag, Renan Campos, Kate Saenko, Anna Rumshisky","Precise evaluation metrics are important for assessing progress in high-level lan- guage generation tasks such as machine translation or image captioning. Histor- ically, these metrics have been evaluated using correlation with human judgment. However, human-derived scores are often alarmingly inconsistent and are also limited in their ability to identify precise areas of weakness. In this paper, we perform a case study for metric evaluation by measuring the effect that systematic sentence trans- formations (e.g. active to passive voice) have on the automatic metric scores. These sentence “corruptions” serve as unit tests for precisely measuring the strengths and weaknesses of a given metric. We find that not only are human annotations heavily in- consistent in this study, but that the Met- ric Unit TesT analysis is able to capture precise shortcomings of particular metrics (e.g. comparing passive and active sen- tences) better than a simple correlation with human judgment can. ",,,,ACL
183,2016,N-gram language models for massively parallel devices,"Nikolay Bogoychev, Adam Lopez","For many applications, the query speed of N -gram language models is a computa- tional bottleneck. Although massively par- allel hardware like GPUs offer a poten- tial solution to this bottleneck, exploiting this hardware requires a careful rethink- ing of basic algorithms and data structures. We present the first language model de- signed for such hardware, using B-trees to maximize data parallelism and minimize memory footprint and latency. Compared with a single-threaded instance of KenLM (Heafield, 2011), a highly optimized CPU- based language model, our GPU imple- mentation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task. When we saturate both devices, the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures. Our implementation is freely available at https://github.com/XapaJIaMnu/gLM ",,,,ACL
184,2016,Cross-Lingual Morphological Tagging for Low-Resource Languages,"Jan Buys, Jan A. Botha","Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language pro- cessing tools. We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision. Our approach extends existing approaches of projecting part-of-speech tags across lan- guages, using bitext to infer constraints on the possible tags for a given word type or token. We propose a tagging model us- ing Wsabie, a discriminative embedding- based model with rank-based learning. In our evaluation on 11 languages, on av- erage this model performs on par with a baseline weakly-supervised HMM, while being more scalable. Multilingual experi- ments show that the method performs best when projecting between related language pairs. Despite the inherently lossy pro- jection, we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average. ",,,,ACL
185,2016,Semi-Supervised Learning for Neural Machine Translation,"Yong Cheng, Wei Xu, Zhongjun He, Wei He","While end-to-end neural machine transla- tion (NMT) has made remarkable progress recently, NMT systems only rely on par- allel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, espe- cially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi- supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (mono- lingual corpora) data. The central idea is to reconstruct the monolingual corpora us- ing an autoencoder, in which the source- to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese- English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. ",,,,ACL
186,2016,Strategies for Training Large Vocabulary Neural Language Models,"Wenlin Chen, David Grangier, Michael Auli","Training neural network language mod- els over large vocabularies is computa- tionally costly compared to count-based models such as Kneser-Ney. We present a systematic comparison of neural strate- gies to represent and train large vocabular- ies, including softmax, hierarchical soft- max, target sampling, noise contrastive es- timation and self normalization. We ex- tend self normalization to be a proper esti- mator of likelihood and introduce an effi- cient variant of softmax. We evaluate each method on three popular benchmarks, ex- amining performance on rare words, the speed/accuracy trade-off and complemen- tarity to Kneser-Ney. ",,,,ACL
187,2016,Predicting the Compositionality of Nominal Compounds: Giving Word Embeddings a Hard Time,"Silvio Cordeiro, Carlos Ramisch, Marco Idiart, Aline Villavicencio","Distributional semantic models (DSMs) are often evaluated on artificial simi- larity datasets containing single words or fully compositional phrases. We present a large-scale multilingual eval- uation of DSMs for predicting the de- gree of semantic compositionality of nom- inal compounds on 4 datasets for En- glish and French. We build a total of 816 DSMs and perform 2,856 evaluations using word2vec, GloVe, and PPMI-based models. In addition to the DSMs, we com- pare the impact of different parameters, such as level of corpus preprocessing, con- text window size and number of dimen- sions. The results obtained have a high correlation with human judgments, being comparable to or outperforming the state of the art for some datasets (Spearman’s ρ=.82 for the Reddy dataset). ",,,,ACL
188,2016,Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints,"Greg Durrett, Taylor Berg-Kirkpatrick, Dan Klein","We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final sys- tem 1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. ",,,,ACL
189,2016,Set-Theoretic Alignment for Comparable Corpora,"Thierry Etchegoyhen, Andoni Azpeitia","We describe and evaluate a simple method to extract parallel sentences from com- parable corpora. The approach, termed STACC , is based on expanded lexical sets and the Jaccard similarity coefficient. We evaluate our system against state-of-the- art methods on a large range of datasets in different domains, for ten language pairs, showing that it either matches or outper- forms current methods across the board and gives significantly better results on the noisiest datasets. STACC is a portable method, requiring no particular adaptation for new domains or language pairs, thus enabling the efficient mining of parallel sentences in comparable corpora. ",,,,ACL
190,2016,Jointly Learning to Embed and Predict with Multiple Languages,"Daniel C. Ferreira, André F. T. Martins, Mariana S. C. Almeida","We propose a joint formulation for learn- ing task-specific cross-lingual word em- beddings, along with classifiers for that task. Unlike prior work, which first learns the embeddings from parallel data and then plugs them in a supervised learning problem, our approach is one- shot: a single optimization problem com- bines a co-regularizer for the multilin- gual embeddings with a task-specific loss. We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension, a limitation which does not exist for other co-regularizers (such as the 1 - distance). Despite its simplicity, our method achieves state-of-the-art accura- cies on the RCV1/RCV2 dataset when transferring from English to German, with training times below 1 minute. On the TED Corpus, we obtain the highest re- ported scores on 10 out of 11 languages. ",,,,ACL
191,2016,"Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization","Lucie Flekova, Iryna Gurevych","Coarse-grained semantic categories such as supersenses have proven useful for a range of downstream tasks such as question an- swering or machine translation. To date, no effort has been put into integrating the supersenses into distributional word rep- resentations. We present a novel joint em- bedding model of words and supersenses, providing insights into the relationship be- tween words and supersenses in the same vector space. Using these embeddings in a deep neural network model, we demon- strate that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks. ",,,,ACL
192,2016,Efficient techniques for parsing with tree automata,"Jonas Groschwitz, Alexander Koller, Mark Johnson","Parsing for a wide variety of grammar for- malisms can be performed by intersecting finite tree automata. However, naive im- plementations of parsing by intersection are very inefficient. We present techniques that speed up tree-automata-based pars- ing, to the point that it becomes practically feasible on realistic data when applied to context-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature. ",,,,ACL
193,2016,A Vector Space for Distributional Semantics for Entailment,"James Henderson, Diana Popa","Distributional semantics creates vector- space representations that capture many forms of semantic similarity, but their re- lation to semantic entailment has been less clear. We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment. Us- ing a mean-field approximation, we de- velop approximate inference procedures and entailment operators over vectors of probabilities of features being known (ver- sus unknown). We use this framework to reinterpret an existing distributional- semantic model (Word2Vec) as approxi- mating an entailment-based model of the distributions of words in contexts, thereby predicting lexical entailment relations. In both unsupervised and semi-supervised experiments on hyponymy detection, we get substantial improvements over previ- ous results. ",,,,ACL
194,2016,Hidden Softmax Sequence Model for Dialogue Structure Analysis,"Zhiyang He, Xien Liu, Ping Lv, Ji Wu","We propose a new unsupervised learning model, hidden softmax sequence model (HSSM), based on Boltzmann machine for dialogue structure analysis. The model employs three types of units in the hidden layer to discovery dialogue latent struc- tures: softmax units which represent latent states of utterances; binary units which represent latent topics specified by dia- logues; and a binary unit that represents the global general topic shared across the whole dialogue corpus. In addition, the model contains extra connections between adjacent hidden softmax units to formu- late the dependency between latent states. Two different kinds of real world dialogue corpora, Twitter-Post and AirTicketBook- ing, are utilized for extensive comparing experiments, and the results illustrate that the proposed model outperforms sate-of- the-art popular approaches. ",,,,ACL
195,2016,Summarizing Source Code using a Neural Attention Model,"Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer","High quality source code is often paired with high level summaries of the compu- tation it performs, for example in code documentation or in descriptions posted in online forums. Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done for a small frac- tion of all code that is produced. In this paper, we present the first completely data- driven approach for generating high level summaries of source code. Our model, CODE-NN , uses Long Short Term Mem- ory (LSTM) networks with attention to produce sentences that describe C# code snippets and SQL queries. CODE-NN is trained on a new corpus that is auto- matically collected from StackOverflow, which we release. Experiments demon- strate strong performance on two tasks: (1) code summarization, where we estab- lish the first end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C# benchmark by a large mar- gin. ",,,,ACL
196,2016,Continuous Profile Models in ASL Syntactic Facial Expression Synthesis,"Hernisa Kacorri, Matt Huenerfauth","To create accessible content for deaf users, we investigate automatically synthesizing animations of American Sign Language (ASL), including grammatically important facial expressions and head movements. Based on recordings of humans perform- ing various types of syntactic face and head movements (which include idiosyn- cratic variation), we evaluate the efficacy of Continuous Profile Models (CPMs) at identifying an essential “latent trace” of the performance, for use in producing ASL animations. A metric-based evalua- tion and a study with deaf users indicated that this approach was more effective than a prior method for producing animations. ",,,,ACL
197,2016,Evaluating Sentiment Analysis in the Context of Securities Trading,"Siavash Kazemian, Shunan Zhao, Gerald Penn","There are numerous studies suggesting that published news stories have an im- portant effect on the direction of the stock market, its volatility, the volume of trades, and the value of individual stocks men- tioned in the news. There is even some published research suggesting that auto- mated sentiment analysis of news docu- ments, quarterly reports, blogs and/or twit- ter data can be productively used as part of a trading strategy. This paper presents just such a family of trading strategies, and then uses this application to re-examine some of the tacit assumptions behind how sentiment analyzers are generally evalu- ated, in spite of the contexts of their appli- cation. This discrepancy comes at a cost. ",,,,ACL
198,2016,Edge-Linear First-Order Dependency Parsing with Undirected Minimum Spanning Tree Inference,"Effi Levi, Roi Reichart, Ari Rappoport","The run time complexity of state-of-the- art inference algorithms in graph-based dependency parsing is super-linear in the number of input words (n). Recently, pruning algorithms for these models have shown to cut a large portion of the graph edges, with minimal damage to the re- sulting parse trees. Solving the infer- ence problem in run time complexity de- termined solely by the number of edges (m) is hence of obvious importance. We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running in O(n) time. In experiments with 18 languages, a variant of the first-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very sim- ilarly to the original parser that runs an O(n 2 ) directed MST inference. ",,,,ACL
199,2016,Topic Extraction from Microblog Posts Using Conversation Structures,"Jing Li, Ming Liao, Wei Gao, Yulan He","Conventional topic models are ineffec- tive for topic extraction from microblog messages since the lack of structure and context among the posts renders poor message-level word co-occurrence pat- terns. In this work, we organize microblog posts as conversation trees based on re- posting and replying relations, which en- rich context information to alleviate data sparseness. Our model generates words according to topic dependencies derived from the conversation structures. In spe- cific, we differentiate messages as leader messages, which initiate key aspects of previously focused topics or shift the focus to different topics, and follower messages that do not introduce any new information but simply echo topics from the messages that they repost or reply. Our model cap- tures the different extents that leader and follower messages may contain the key topical words, thus further enhances the quality of the induced topics. The results of thorough experiments demonstrate the effectiveness of our proposed model. ",,,,ACL
200,2016,Neural Relation Extraction with Selective Attention over Instances,"Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan","Distant supervised relation extraction has been widely used to find novel relational facts from text. However, distant su- pervision inevitably accompanies with the wrong labelling problem, and these noisy data will substantially hurt the perfor- mance of relation extraction. To allevi- ate this issue, we propose a sentence-level attention-based model for relation extrac- tion. In this model, we employ convolu- tional neural networks to embed the se- mantics of sentences. Afterwards, we build sentence-level attention over multi- ple instances, which is expected to dy- namically reduce the weights of those noisy instances. Experimental results on real-world datasets show that, our model can make full use of all informative sen- tences and effectively reduce the influence of wrong labelled instances. Our model achieves significant and consistent im- provements on relation extraction as com- pared with baselines. The source code of this paper can be obtained from https: //github.com/thunlp/NRE. ",,,,ACL
201,2016,Leveraging FrameNet to Improve Automatic Event Detection,"Shulin Liu, Yubo Chen, Shizhu He, Kang Liu","Frames defined in FrameNet (FN) share highly similar structures with events in ACE event extraction program. An even- t in ACE is composed of an event trig- ger and a set of arguments. Analogously, a frame in FN is composed of a lexical u- nit and a set of frame elements, which play similar roles as triggers and arguments of ACE events respectively. Besides having similar structures, many frames in FN ac- tually express certain types of events. The above observations motivate us to explore whether there exists a good mapping from frames to event-types and if it is possible to improve event detection by using FN. In this paper, we propose a global infer- ence approach to detect events in FN. Fur- ther, based on the detected results, we an- alyze possible mappings from frames to event-types. Finally, we improve the per- formance of event detection and achieve a new state-of-the-art result by using the events automatically detected from FN. ",,,,ACL
202,2016,Learning To Use Formulas To Solve Simple Arithmetic Problems,"Arindam Mitra, Chitta Baral","Solving simple arithmetic word problems is one of the challenges in Natural Lan- guage Understanding. This paper presents a novel method to learn to use formulas to solve simple arithmetic word problems. Our system, analyzes each of the sen- tences to identify the variables and their attributes; and automatically maps this in- formation into a higher level representa- tion. It then uses that representation to recognize the presence of a formula along with its associated variables. An equa- tion is then generated from the formal de- scription of the formula. In the training phase, it learns to score the <formula, variables> pair from the systematically generated higher level representation. It is able to solve 86.07% of the problems in a corpus of standard primary school test questions and beats the state-of-the-art by a margin of 8.07%. ",,,,ACL
203,2016,Unravelling Names of Fictional Characters,"Katerina Papantoniou, Stasinos Konstantopoulos","In this paper we explore the correlation be- tween the sound of words and their mean- ing, by testing if the polarity (‘good guy’ or ‘bad guy’) of a character’s role in a work of fiction can be predicted by the name of the character in the absence of any other context. Our approach is based on phonological and other features pro- posed in prior theoretical studies of fic- tional names. These features are used to construct a predictive model over a man- ually annotated corpus of characters from motion pictures. By experimenting with different mixtures of features, we identify phonological features as being the most discriminative by comparison to social and other types of features, and we delve into a discussion of specific phonological and phonotactic indicators of a character’s role’s polarity. ",,,,ACL
204,2016,Most “babies” are “little” and most “problems” are “huge”: Compositional Entailment in Adjective-Nouns,"Ellie Pavlick, Chris Callison-Burch","We examine adjective-noun (AN) composi- tion in the task of recognizing textual entail- ment (RTE). We analyze behavior of ANs in large corpora and show that, despite conven- tional wisdom, adjectives do not always re- strict the denotation of the nouns they mod- ify. We use natural logic to characterize the variety of entailment relations that can result from AN composition. Predicting these re- lations depends on context and on common- sense knowledge, making AN composition es- pecially challenging for current RTE systems. We demonstrate the inability of current state- of-the-art systems to handle AN composition in a simplified RTE task which involves the in- sertion of only a single word. ",,,,ACL
205,2016,Modeling Stance in Student Essays,"Isaac Persing, Vincent Ng","Essay stance classification, the task of de- termining how much an essay’s author agrees with a given proposition, is an important yet under-investigated subtask in understanding an argumentative essay’s overall content. We introduce a new cor- pus of argumentative student essays an- notated with stance information and pro- pose a computational model for automati- cally predicting essay stance. In an evalu- ation on 826 essays, our approach signif- icantly outperforms four baselines, one of which relies on features previously devel- oped specifically for stance classification in student essays, yielding relative error reductions of at least 11.3% and 5.3%, in micro and macro F-score, respectively. ",,,,ACL
206,2016,A New Psychometric-inspired Evaluation Metric for Chinese Word Segmentation,"Peng Qian, Xipeng Qiu, Xuanjing Huang","Word segmentation is a fundamental task for Chinese language processing. Howev- er, with the successive improvements, the standard metric is becoming hard to distin- guish state-of-the-art word segmentation systems. In this paper, we propose a new psychometric-inspired evaluation metric for Chinese word segmentation, which addresses to balance the very skewed word distribution at different levels of difficulty 1 . The performance on a real evaluation shows that the proposed metric gives more reasonable and distinguishable scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. ",,,,ACL
207,2016,Temporal Anchoring of Events for the TimeBank Corpus,"Nils Reimers, Nazanin Dehghani, Iryna Gurevych","Today’s extraction of temporal informa- tion for events heavily depends on an- notated temporal links. These so called TLINKs capture the relation between pairs of event mentions and time expressions. One problem is that the number of possible TLINKs grows quadratic with the num- ber of event mentions, therefore most an- notation studies concentrate on links for mentions in the same or in adjacent sen- tences. However, as our annotation study shows, this restriction results for 58% of the event mentions in a less precise infor- mation when the event took place. This paper proposes a new annotation scheme to anchor events in time. Not only is the annotation effort much lower as it scales linear with the number of events, it also gives a more precise anchoring when the events have happened as the complete document can be taken into account. Us- ing this scheme, we annotated a subset of the TimeBank Corpus and compare our re- sults to other annotation schemes. Addi- tionally, we present some baseline exper- iments to automatically anchor events in time. Our annotation scheme, the auto- mated system and the annotated corpus are publicly available. 1 ",,,,ACL
208,2016,Grammatical Error Correction: Machine Translation and Classifiers,"Alla Rozovskaya, Dan Roth","We focus on two leading state-of-the-art approaches to grammatical error correc- tion – machine learning classification and machine translation. Based on the com- parative study of the two learning frame- works and through error analysis of the output of the state-of-the-art systems, we identify key strengths and weaknesses of each of these approaches and demonstrate their complementarity. In particular, the machine translation method learns from parallel data without requiring further lin- guistic input and is better at correcting complex mistakes. The classification ap- proach possesses other desirable charac- teristics, such as the ability to easily gener- alize beyond what was seen in training, the ability to train without human-annotated data, and the flexibility to adjust knowl- edge sources for individual error types. Based on this analysis, we develop an algorithmic approach that combines the strengths of both methods. We present several systems based on resources used in previous work with a relative improve- ment of over 20% (and 7.4 F score points) over the previous state-of-the-art. ",,,,ACL
209,2016,Recurrent neural network models for disease name recognition using domain invariant features,"Sunil Sahu, Ashish Anand","Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and pro- pose a domain-invariant framework for the disease name recognition task. In particu- lar, we propose various end-to-end recur- rent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined cate- gories. We also utilize convolution neu- ral network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded fea- tures in our model. We compare our mod- els with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recogni- tion task indicate that state-of-the-art per- formance can be obtained without relying on feature engineering. Further the pro- posed models obtained improved perfor- mance on the classification task of disease names. ",,,,ACL
210,2016,Domain Adaptation for Authorship Attribution: Improved Structural Correspondence Learning,"Upendra Sapkota, Thamar Solorio, Manuel Montes, Steven Bethard","We present the first domain adaptation model for authorship attribution to leverage unlabeled data. The model includes exten- sions to structural correspondence learning needed to make it appropriate for the task. For example, we propose a median-based classification instead of the standard binary classification used in previous work. Our results show that punctuation-based charac- ter n-grams form excellent pivot features. We also show how singular value decom- position plays a critical role in achieving domain adaptation, and that replacing (in- stead of concatenating) non-pivot features with correspondence features yields better performance. ",,,,ACL
211,2016,A Corpus-Based Analysis of Canonical Word Order of Japanese Double Object Constructions,"Ryohei Sasano, Manabu Okumura","The canonical word order of Japanese double object constructions has attracted considerable attention among linguists and has been a topic of many studies. How- ever, most of these studies require either manual analyses or measurements of hu- man characteristics such as brain activities or reading times for each example. Thus, while these analyses are reliable for the ex- amples they focus on, they cannot be gen- eralized to other examples. On the other hand, the trend of actual usage can be col- lected automatically from a large corpus. Thus, in this paper, we assume that there is a relationship between the canonical word order and the proportion of each word or- der in a large corpus and present a corpus- based analysis of canonical word order of Japanese double object constructions. ",,,,ACL
212,2016,Knowledge-Based Semantic Embedding for Machine Translation,"Chen Shi, Shujie Liu, Shuo Ren, Shi Feng","In this paper, with the help of knowl- edge base, we build and formulate a se- mantic space to connect the source and target languages, and apply it to the sequence-to-sequence framework to pro- pose a Knowledge-Based Semantic Em- bedding (KBSE) method. In our KB- SE method, the source sentence is firstly mapped into a knowledge based seman- tic space, and the target sentence is gen- erated using a recurrent neural network with the internal meaning preserved. Ex- periments are conducted on two transla- tion tasks, the electric business data and movie data, and the results show that our proposed method can achieve outstanding performance, compared with both the tra- ditional SMT methods and the existing encoder-decoder models. ",,,,ACL
213,2016,One for All: Towards Language Independent Named Entity Linking,"Avirup Sil, Radu Florian","Entity linking (EL) is the task of dis- ambiguating mentions in text by associ- ating them with entries in a predefined database of mentions (persons, organiza- tions, etc). Most previous EL research has focused mainly on one language, English, with less attention being paid to other lan- guages, such as Spanish or Chinese. In this paper, we introduce L IE L, a Lan- guage Independent Entity Linking system, which provides an EL framework which, once trained on one language, works re- markably well on a number of different languages without change. L IE L makes a joint global prediction over the entire document, employing a discriminative re- ranking framework with many domain and language-independent feature func- tions. Experiments on numerous bench- mark datasets, show that the proposed sys- tem, once trained on one language, En- glish, outperforms several state-of-the-art systems in English (by 4 points) and the trained model also works very well on Spanish (14 points better than a competi- tor system), demonstrating the viability of the approach. ",,,,ACL
214,2016,On Approximately Searching for Similar Word Embeddings,"Kohei Sugawara, Hayato Kobayashi, Masajiro Iwasaki","We discuss an approximate similarity search for word embeddings, which is an operation to approximately find em- beddings close to a given vector. We compared several metric-based search al- gorithms with hash-, tree-, and graph- based indexing from different aspects. Our experimental results showed that a graph-based indexing exhibits robust per- formance and additionally provided use- ful information, e.g., vector normalization achieves an efficient search with cosine similarity. ",,,,ACL
215,2016,Composing Distributed Representations of Relational Patterns,"Sho Takase, Naoaki Okazaki, Kentaro Inui","Learning distributed representations for relation instances is a central technique in downstream NLP applications. In or- der to address semantic modeling of rela- tional patterns, this paper constructs a new dataset that provides multiple similarity ratings for every pair of relational patterns on the existing dataset (Zeichner et al., 2012). In addition, we conduct a compar- ative study of different encoders includ- ing additive composition, RNN, LSTM, and GRU for composing distributed rep- resentations of relational patterns. We also present Gated Additive Composition, which is an enhancement of additive com- position with the gating mechanism. Ex- periments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in the relation classification task. ",,,,ACL
216,2016,"The More Antecedents, the Merrier: Resolving Multi-Antecedent Anaphors","Hardik Vala, Andrew Piper, Derek Ruths","Anaphor resolution is an important task in NLP with many applications. De- spite much research effort, it remains an open problem. The difficulty of the prob- lem varies substantially across different sub-problems. One sub-problem, in par- ticular, has been largely untouched by prior work despite occurring frequently throughout corpora: the anaphor that has multiple antecedents, which here we call multi-antecedent anaphors or m- anaphors. Current coreference resolvers restrict anaphors to at most a single an- tecedent. As we show in this paper, re- laxing this constraint poses serious prob- lems in coreference chain-building, where each chain is intended to refer to a single entity. This work provides a formaliza- tion of the new task with preliminary in- sights into multi-antecedent noun-phrase anaphors, and offers a method for resolv- ing such cases that outperforms a number of baseline methods by a significant mar- gin. Our system uses local agglomerative clustering on candidate antecedents and an existing coreference system to score clus- ters to determine which cluster of men- tions is antecedent for a given anaphor. When we augment an existing coreference system with our proposed method, we ob- serve a substantial increase in performance (0.6 absolute CoNLL F1) on an annotated corpus. ",,,,ACL
217,2016,Automatic Labeling of Topic Models Using Text Summaries,"Xiaojun Wan, Tianming Wang","Labeling topics learned by topic models is a challenging problem. Previous studies have used words, phrases and images to label topics. In this paper, we propose to use text summaries for topic labeling. Several sentences are extracted from the most related documents to form the sum- mary for each topic. In order to obtain summaries with both high relevance, cov- erage and discrimination for all the topics, we propose an algorithm based on sub- modular optimization. Both automatic and manual analysis have been conducted on two real document collections, and we find 1) the summaries extracted by our proposed algorithm are superior over the summaries extracted by existing popular summarization methods; 2) the use of summaries as labels has obvious ad- vantages over the use of words and phrases. ",,,,ACL
218,2016,Graph-based Dependency Parsing with Bidirectional LSTM,"Wenhui Wang, Baobao Chang","In this paper, we propose a neural network model for graph-based dependency pars- ing which utilizes Bidirectional LSTM (BLSTM) to capture richer contextual in- formation instead of using high-order fac- torization, and enable our model to use much fewer features than previous work. In addition, we propose an effective way to learn sentence segment embedding on sentence-level based on an extra forward LSTM network. Although our model uses only first-order factorization, experiments on English Peen Treebank and Chinese Penn Treebank show that our model could be competitive with previous higher-order graph-based dependency parsing models and state-of-the-art models. ",,,,ACL
219,2016,TransG : A Generative Model for Knowledge Graph Embedding,"Han Xiao, Minlie Huang, Xiaoyan Zhu","Recently, knowledge graph embedding, which projects symbolic entities and rela- tions into continuous vector space, has be- come a new, hot topic in artificial intelli- gence. This paper proposes a novel gen- erative model (TransG) to address the is- sue of multiple relation semantics that a relation may have multiple meanings re- vealed by the entity pairs associated with the corresponding triples. The new model can discover latent semantics for a rela- tion and leverage a mixture of relation- specific component vectors to embed a fact triple. To the best of our knowledge, this is the first generative model for knowl- edge graph embedding, and at the first time, the issue of multiple relation seman- tics is formally discussed. Extensive ex- periments show that the proposed model achieves substantial improvements against the state-of-the-art baselines. ",,,,ACL
220,2016,Question Answering on Freebase via Relation Extraction and Textual Evidence,"Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang","Existing knowledge-based question an- swering systems often rely on small an- notated training data. While shallow meth- ods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at an- swering questions involving multiple con- straints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a neural network based re- lation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers. Ex- periments on the WebQuestions question answering dataset show that our method achieves an F 1 of 53.3%, a substantial im- provement over the state-of-the-art. ",,,,ACL
221,2016,Vector-space topic models for detecting Alzheimer’s disease,"Maria Yancheva, Frank Rudzicz","Semantic deficit is a symptom of language impairment in Alzheimer’s disease (AD). We present a generalizable method for au- tomatic generation of information content units (ICUs) for a picture used in a stan- dard clinical task, achieving high recall, 96.8%, of human-supplied ICUs. We use the automatically generated topic model to extract semantic features, and train a ran- dom forest classifier to achieve an F-score of 0.74 in binary classification of controls versus people with AD using a set of only 12 features. This is comparable to re- sults (0.72 F-score) with a set of 85 man- ual features. Adding semantic informa- tion to a set of standard lexicosyntactic and acoustic features improves F-score to 0.80. While control and dementia subjects dis- cuss the same topics in the same contexts, controls are more informative per second of speech. ",,,,ACL
222,2016,Chinese Couplet Generation with Neural Network Structures,"Rui Yan, Cheng-Te Li, Xiaohua Hu, Ming Zhang","Part of the unique cultural heritage of Chi- na is the Chinese couplet. Given a sen- tence (namely an antecedent clause), peo- ple reply with another sentence (namely a subsequent clause) equal in length. More- over, a special phenomenon is that corre- sponding characters from the same posi- tion in the two clauses match each oth- er by following certain constraints on se- mantic and/or syntactic relatedness. Au- tomatic couplet generation by computer is viewed as a difficult problem and has not been fully explored. In this paper, we formulate the task as a natural language generation problem using neural network structures. Given the issued anteceden- t clause, the system generates the subse- quent clause via sequential language mod- eling. To satisfy special characteristics of couplets, we incorporate the attention mechanism and polishing schema into the encoding-decoding process. The couplet is generated incrementally and iterative- ly. A comprehensive evaluation, using per- plexity and BLEU measurements as well as human judgments, has demonstrated the effectiveness of our proposed approach. ",,,,ACL
223,2016,A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task,"Danqi Chen, Jason Bolton, Christopher D. Manning","Enabling a computer to understand a docu- ment so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solu- tion by machine learned systems is the lim- ited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language un- derstanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding cur- rent state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.1 ",,,,ACL
224,2016,Learning Language Games through Interaction,"Sida I. Wang, Percy Liang, Christopher D. Manning","We introduce a new language learning setting relevant to building adaptive nat- ural language interfaces. It is inspired by Wittgenstein’s language games: a hu- man wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The com- puter initially knows nothing about lan- guage and therefore must learn it from scratch through interaction, while the hu- man adapts to the computer’s capabilities. We created a game called SHRDLURN in a blocks world and collected interactions from 100 people playing it. First, we an- alyze the humans’ strategies, showing that using compositionality and avoiding syn- onyms correlates positively with task per- formance. Second, we compare computer strategies, showing that modeling prag- matics on a semantic parsing model accel- erates learning for more strategic players. ",,,,ACL
225,2016,Finding Non-Arbitrary Form-Meaning Systematicity Using String-Metric Learning for Kernel Regression,"E.Dario Gutiérrez, Roger Levy, Benjamin Bergen","Arbitrariness of the sign—the notion that the forms of words are unrelated to their meanings—is an underlying assumption of many linguistic theories. Two lines of research have recently challenged this as- sumption, but they produce differing char- acterizations of non-arbitrariness in lan- guage. Behavioral and corpus studies have confirmed the validity of localized form-meaning patterns manifested in lim- ited subsets of the lexicon. Meanwhile, global (lexicon-wide) statistical analyses instead find diffuse form-meaning system- aticity across the lexicon as a whole. We bridge the gap with an approach that can detect both local and global form- meaning systematicity in language. In the kernel regression formulation we in- troduce, form-meaning relationships can be used to predict words’ distributional semantic vectors from their forms. Fur- thermore, we introduce a novel metric learning algorithm that can learn weighted edit distances that minimize kernel regres- sion error. Our results suggest that the English lexicon exhibits far more global form-meaning systematicity than previ- ously discovered, and that much of this systematicity is focused in localized form- meaning patterns. ",,,,ACL
226,2016,Improving Hypernymy Detection with an Integrated Path-based and Distributional Method,"Vered Shwartz, Yoav Goldberg, Ido Dagan","Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complemen- tary approaches. Distributional methods, whose supervised variants are the cur- rent best performers, and path-based meth- ods, which received less research atten- tion. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural net- work, that achieves results comparable to distributional methods. We then ex- tend the approach to integrate both path- based and distributional signals, signifi- cantly improving upon the state-of-the-art on this task. ",,,,ACL
227,2016,Multimodal Pivots for Image Caption Translation,"Julian Hitschler, Shigehiko Schamoni, Stefan Riezler","We present an approach to improve sta- tistical machine translation of image de- scriptions by multimodal pivots defined in visual space. The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of trans- lation outputs. Our approach does not de- pend on the availability of large amounts of in-domain parallel data, but only re- lies on available large datasets of monolin- gually captioned images, and on state-of- the-art convolutional neural networks to compute image similarities. Our experi- mental evaluation shows improvements of ",,,,ACL
228,2016,Harnessing Deep Neural Networks with Logic Rules,"Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy","Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the struc- tured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment anal- ysis, and an RNN for named entity recog- nition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable re- sults to previous best-performing systems. ",,,,ACL
229,2016,Case and Cause in Icelandic: Reconstructing Causal Networks of Cascaded Language Changes,"Fermín Moscoso del Prado Martín, Christian Brendel","Linguistic drift is a process that produces slow irreversible changes in the grammar and function of a language’s construc- tions. Importantly, changes in a part of a language can have trickle down effects, triggering changes elsewhere in that lan- guage. Although such causally triggered chains of changes have long been hypoth- esized by historical linguists, no explicit demonstration of the actual causality has been provided. In this study, we use co- occurrence statistics and machine learning to demonstrate that the functions of mor- phological cases experience a slow, irre- versible drift along history, even in a lan- guage as conservative as is Icelandic. Cru- cially, we then move on to demonstrate –using the notion of Granger-causality– that there are explicit causal connections between the changes in the functions of the different cases, which are consistent with documented processes in the history of Icelandic. Our technique provides a means for the quantitative reconstruction of connected networks of subtle linguistic changes. ",,,,ACL
230,2016,On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems,"Pei-Hao Su, Milica Gašić, Nikola Mrkšić, Lina M. Rojas-Barahona","The ability to compute an accurate re- ward function is essential for optimising a dialogue policy via reinforcement learn- ing. In real-world applications, using ex- plicit user feedback as the reward sig- nal is often unreliable and costly to col- lect. This problem can be mitigated if the user’s intent is known in advance or data is available to pre-train a task suc- cess predictor off-line. In practice neither of these apply for most real world applica- tions. Here we propose an on-line learn- ing framework whereby the dialogue pol- icy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process op- erates on a continuous space dialogue rep- resentation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annota- tion costs and mitigate noisy user feedback in dialogue policy learning. ",,,,ACL
231,2016,Globally Normalized Transition-Based Neural Networks,"Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn","We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of- speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We dis- cuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models. ",,,,ACL
1,2017,Adversarial Multi-task Learning for Text Classification,"Pengfei Liu, Xipeng Qiu, Xuanjing Huang","Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at http://nlp.fudan.edu.cn/data/.",,,,ACL
2,2017,Neural End-to-End Learning for Computational Argumentation Mining,"Steffen Eger, Johannes Daxenberger, Iryna Gurevych","We investigate neural techniques for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning ‘natural’ subtasks, in a multi-task learning setup, improves performance.",,,,ACL
3,2017,Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision,"Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus","Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural “programmer”, i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic “computer”, i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.",,,,ACL
4,2017,Neural Relation Extraction with Multi-lingual Attention,"Yankai Lin, Zhiyuan Liu, Maosong Sun","Relation extraction has been widely used for finding unknown relational facts from plain text. Most existing methods focus on exploiting mono-lingual data for relation extraction, ignoring massive information from the texts in various languages. To address this issue, we introduce a multi-lingual neural relation extraction framework, which employs mono-lingual attention to utilize the information within mono-lingual texts and further proposes cross-lingual attention to consider the information consistency and complementarity among cross-lingual texts. Experimental results on real-world datasets show that, our model can take advantage of multi-lingual texts and consistently achieve significant improvements on relation extraction as compared with baselines.",,,,ACL
5,2017,Learning Structured Natural Language Representations for Semantic Parsing,"Jianpeng Cheng, Siva Reddy, Vijay Saraswat, Mirella Lapata","We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEOQUERY and WEBQUESTIONS. The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones.",,,,ACL
6,2017,Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules,"Ivan Vulić, Nikola Mrkšić, Roi Reichart, Diarmuid Ó Séaghdha","Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for low-frequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for language understanding systems, which may infer that ‘inexpensive’ is a rephrasing for ‘expensive’ or may not associate ‘acquire’ with ‘acquires’. In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces. Instead, our method injects morphological constraints generated using simple language-specific rules, pulling inflectional forms of the same word close together and pushing derivational antonyms far apart. In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection. Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks.",,,,ACL
7,2017,Skip-Gram − Zipf + Uniform = Vector Additivity,"Alex Gittens, Dimitris Achlioptas, Michael W. Mahoney","In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected “side-effect” of such models is that their vectors often exhibit compositionality, i.e., addingtwo word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., “man” + “royal” = “king”. This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.",,,,ACL
8,2017,The State of the Art in Semantic Representation,"Omri Abend, Ari Rappoport","Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.",,,,ACL
9,2017,Joint Learning for Event Coreference Resolution,"Jing Lu, Vincent Ng","While joint models have been developed for many NLP tasks, the vast majority of event coreference resolvers, including the top-performing resolvers competing in the recent TAC KBP 2016 Event Nugget Detection and Coreference task, are pipeline-based, where the propagation of errors from the trigger detection component to the event coreference component is a major performance limiting factor. To address this problem, we propose a model for jointly learning event coreference, trigger detection, and event anaphoricity. Our joint model is novel in its choice of tasks and its features for capturing cross-task interactions. To our knowledge, this is the first attempt to train a mention-ranking model and employ event anaphoricity for event coreference. Our model achieves the best results to date on the KBP 2016 English and Chinese datasets.",,,,ACL
10,2017,Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution,"Ting Liu, Yiming Cui, Qingyu Yin, Wei-Nan Zhang","Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.",,,,ACL
11,2017,Discourse Mode Identification in Essays,"Wei Song, Dong Wang, Ruiji Fu, Lizhen Liu","Discourse modes play an important role in writing composition and evaluation. This paper presents a study on the manual and automatic identification of narration,exposition, description, argument and emotion expressing sentences in narrative essays. We annotate a corpus to study the characteristics of discourse modes and describe a neural sequence labeling model for identification. Evaluation results show that discourse modes can be identified automatically with an average F1-score of 0.7. We further demonstrate that discourse modes can be used as features that improve automatic essay scoring (AES). The impacts of discourse modes for AES are also discussed.",,,,ACL
12,2017,A Convolutional Encoder Model for Neural Machine Translation,"Jonas Gehring, Michael Auli, David Grangier, Yann Dauphin",The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. We present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT’16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and on WMT’15 English-German we outperform several recently published results. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT’14 English-French translation. We speed up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM.,,,,ACL
13,2017,Deep Neural Machine Translation with Linear Associative Unit,"Mingxuan Wang, Zhengdong Lu, Jie Zhou, Qun Liu","Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with its capability in modeling complex functions and capturing complex linguistic structures. However NMT with deep architecture in its encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often makes the optimization much more difficult. To address this problem we propose a novel linear associative units (LAU) to reduce the gradient propagation path inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs uses linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported on results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art.",,,,ACL
14,2017,Neural AMR: Sequence-to-Sequence Models for Parsing and Generation,"Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi","Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.",,,,ACL
15,2017,Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems,"Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom","Solving algebraic word problems requires executing a series of arithmetic operations—a program—to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.",,,,ACL
16,2017,Automatically Generating Rhythmic Verse with Neural Networks,"Jack Hopkins, Douwe Kiela","We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machine-generated poem to be the best amongst all evaluated.",,,,ACL
17,2017,Creating Training Corpora for NLG Micro-Planners,"Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini","In this paper, we present a novel framework for semi-automatically creating linguistically challenging micro-planning data-to-text corpora from existing Knowledge Bases. Because our method pairs data of varying size and shape with texts ranging from simple clauses to short texts, a dataset created using this framework provides a challenging benchmark for microplanning. Another feature of this framework is that it can be applied to any large scale knowledge base and can therefore be used to train and learn KB verbalisers. We apply our framework to DBpedia data and compare the resulting dataset with Wen et al. 2016’s. We show that while Wen et al.’s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of handling the complex interactions occurring during in micro-planning between lexicalisation, aggregation, surface realisation, referring expression generation and sentence segmentation. To encourage researchers to take up this challenge, we made available a dataset of 21,855 data/text pairs created using this framework in the context of the WebNLG shared task.",,,,ACL
18,2017,Gated Self-Matching Networks for Reading Comprehension and Question Answering,"Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang","In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.",,,,ACL
19,2017,Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning,"Shizhu He, Cao Liu, Kang Liu, Jun Zhao","Generating answer with natural language sentence is very important in real-world question answering systems, which needs to obtain a right answer as well as a coherent natural response. In this paper, we propose an end-to-end question answering system called COREQA in sequence-to-sequence learning, which incorporates copying and retrieving mechanisms to generate natural answers within an encoder-decoder framework. Specifically, in COREQA, the semantic units (words, phrases and entities) in a natural answer are dynamically predicted from the vocabulary, copied from the given question and/or retrieved from the corresponding knowledge base jointly. Our empirical study on both synthetic and real-world datasets demonstrates the efficiency of COREQA, which is able to generate correct, coherent and natural answers for knowledge inquired questions.",,,,ACL
20,2017,Coarse-to-Fine Question Answering for Long Documents,"Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin","We present a framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance of state-of-the-art models. While most successful approaches for reading comprehension rely on recurrent neural networks (RNNs), running them over long documents is prohibitively slow because it is difficult to parallelize over sequences. Inspired by how people first skim the document, identify relevant parts, and carefully read these parts to produce an answer, we combine a coarse, fast model for selecting relevant sentences and a more expensive RNN for producing the answer from those sentences. We treat sentence selection as a latent variable trained jointly from the answer only using reinforcement learning. Experiments demonstrate state-of-the-art performance on a challenging subset of the WikiReading dataset and on a new dataset, while speeding up the model by 3.5x-6.7x.",,,,ACL
21,2017,An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge,"Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He","With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Question answering over knowledge base (KB-QA) is one of the promising approaches to access the substantial knowledge. Meanwhile, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put more emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is not easy to express the proper information in the question. Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. As a result, it could alleviates the out-of-vocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely. The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach.",,,,ACL
22,2017,Translating Neuralese,"Jacob Andreas, Anca Dragan, Dan Klein","Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents’ messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.",,,,ACL
23,2017,Obtaining referential word meanings from visual and distributional information: Experiments on object naming,"Sina Zarrieß, David Schlangen","We investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings. We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.",,,,ACL
24,2017,FOIL it! Find One mismatch between Image and Language caption,"Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurélie Herbelot","In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and ‘foil’ captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake (‘foil word’). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.",,,,ACL
25,2017,Verb Physics: Relative Physical Knowledge of Actions and Objects,"Maxwell Forbes, Yejin Choi","Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., “My house is bigger than me.” However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, “Tyler entered his house” implies that his house is bigger than Tyler. In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance.",,,,ACL
26,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,"Masashi Yoshikawa, Hiroshi Noji, Yuji Matsumoto","We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the state-of-the-art results on English and Japanese CCG parsing.",,,,ACL
27,2017,A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing,"Daniel Fernández-González, Carlos Gómez-Rodríguez","Restricted non-monotonicity has been shown beneficial for the projective arc-eager dependency parser in previous research, as posterior decisions can repair mistakes made in previous states due to the lack of information. In this paper, we propose a novel, fully non-monotonic transition system based on the non-projective Covington algorithm. As a non-monotonic system requires exploration of erroneous actions during the training process, we develop several non-monotonic variants of the recently defined dynamic oracle for the Covington parser, based on tight approximations of the loss. Experiments on datasets from the CoNLL-X and CoNLL-XI shared tasks show that a non-monotonic dynamic oracle outperforms the monotonic version in the majority of languages.",,,,ACL
28,2017,Aggregating and Predicting Sequence Labels from Crowd Annotations,"An Thanh Nguyen, Byron Wallace, Junyi Jessy Li, Ani Nenkova","Despite sequences being core to NLP, scant work has considered how to handle noisy sequence labels from multiple annotators for the same text. Given such annotations, we consider two complementary tasks: (1) aggregating sequential crowd labels to infer a best single set of consensus annotations; and (2) using crowd annotations as training data for a model that can predict sequences in unannotated text. For aggregation, we propose a novel Hidden Markov Model variant. To predict sequences in unannotated text, we propose a neural approach using Long Short Term Memory. We evaluate a suite of methods across two different applications and text genres: Named-Entity Recognition in news articles and Information Extraction from biomedical abstracts. Results show improvement over strong baselines. Our source code and data are available online.",,,,ACL
29,2017,Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction,"Chunting Zhou, Graham Neubig","Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.",,,,ACL
30,2017,Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling,"Zhe Gan, Chunyuan Li, Changyou Chen, Yunchen Pu","Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.",,,,ACL
31,2017,Learning attention for historical text normalization by learning to pronounce,"Marcel Bollmann, Joachim Bingel, Anders Søgaard","Automated processing of historical texts often relies on pre-normalization to modern word forms. Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task. We address this problem by using several novel encoder-decoder architectures, including a multi-task learning (MTL) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-the-art by an absolute 2% increase in performance. We analyze the induced models across 44 different texts from Early New High German. Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms. This, we believe, is an important step toward understanding how MTL works.",,,,ACL
32,2017,Deep Learning in Semantic Kernel Spaces,"Danilo Croce, Simone Filice, Giuseppe Castellucci, Roberto Basili","Kernel methods enable the direct usage of structured representations of textual data during language learning and inference tasks. Expressive kernels, such as Tree Kernels, achieve excellent performance in NLP. On the other side, deep neural networks have been demonstrated effective in automatically learning feature representations during training. However, their input is tensor data, i.e., they can not manage rich structured information. In this paper, we show that expressive kernels and deep neural networks can be combined in a common framework in order to (i) explicitly model structured information and (ii) learn non-linear decision functions. We show that the input layer of a deep architecture can be pre-trained through the application of the Nystrom low-rank approximation of kernel spaces. The resulting “kernelized” neural network achieves state-of-the-art accuracy in three different tasks.",,,,ACL
33,2017,Topically Driven Neural Language Model,"Jey Han Lau, Timothy Baldwin, Trevor Cohn","Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.",,,,ACL
34,2017,Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors,"Xuepeng Wang, Kang Liu, Jun Zhao","Solving cold-start problem in review spam detection is an urgent and significant task. It can help the on-line review websites to relieve the damage of spammers in time, but has never been investigated by previous work. This paper proposes a novel neural network model to detect review spam for cold-start problem, by learning to represent the new reviewers’ review with jointly embedded textual and behavioral information. Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability. It is also applicable to a large scale dataset in an unsupervised way.",,,,ACL
35,2017,Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network,"Abhijit Mishra, Kuntal Dey, Pushpak Bhattacharyya","Cognitive NLP systems- i.e., NLP systems that make use of behavioral data - augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc. Such extraction of features is typically manual. We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like Sentiment Analysis and Sarcasm Detection, and that even the extraction and choice of features should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement/gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",,,,ACL
36,2017,An Unsupervised Neural Attention Model for Aspect Extraction,"Ruidan He, Wee Sun Lee, Hwee Tou Ng, Daniel Dahlmeier","Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.",,,,ACL
37,2017,Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization,"Akira Sasaki, Kazuaki Hanawa, Naoaki Okazaki, Kentaro Inui","We presents in this paper our approach for modeling inter-topic preferences of Twitter users: for example, “those who agree with the Trans-Pacific Partnership (TPP) also agree with free trade”. This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion survey, electoral prediction, electoral campaigns, and online debates. In order to extract users’ preferences on Twitter, we design linguistic patterns in which people agree and disagree about specific topics (e.g., “A is completely wrong”). By applying these linguistic patterns to a collection of tweets, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization: representing users’ preference as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our presented approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences.",,,,ACL
38,2017,Automatically Labeled Data Generation for Large Scale Event Extraction,"Yubo Chen, Shulin Liu, Xiang Zhang, Kang Liu","Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",,,,ACL
39,2017,Time Expression Analysis and Recognition Using Syntactic Token Types and General Heuristic Rules,"Xiaoshi Zhong, Aixin Sun, Erik Cambria","Extracting time expressions from free text is a fundamental task for many applications. We analyze the time expressions from four datasets and find that only a small group of words are used to express time information, and the words in time expressions demonstrate similar syntactic behaviour. Based on the findings, we propose a type-based approach, named SynTime, to recognize time expressions. Specifically, we define three main syntactic token types, namely time token, modifier, and numeral, to group time-related regular expressions over tokens. On the types we design general heuristic rules to recognize time expressions. In recognition, SynTime first identifies the time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions. As a light-weight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text of different types and of different domains. Experiment on benchmark datasets and tweets data shows that SynTime outperforms state-of-the-art methods.",,,,ACL
40,2017,Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix,"Bingfeng Luo, Yansong Feng, Zheng Wang, Zhanxing Zhu","Distant supervision significantly reduces human efforts in building training data for many classification tasks. While promising, this technique often introduces noise to the generated training data, which can severely affect the model performance. In this paper, we take a deep look at the application of distant supervision in relation extraction. We show that the dynamic transition matrix can effectively characterize the noise in the training data built by distant supervision. The transition matrix can be effectively trained using a novel curriculum learning based method without any direct supervision about the noise. We thoroughly evaluate our approach under a wide range of extraction scenarios. Experimental results show that our approach consistently improves the extraction results and outperforms the state-of-the-art in various evaluation scenarios.",,,,ACL
41,2017,A Syntactic Neural Model for General-Purpose Code Generation,"Pengcheng Yin, Graham Neubig","We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.",,,,ACL
42,2017,Learning bilingual word embeddings with (almost) no bilingual data,"Mikel Artetxe, Gorka Labaka, Eneko Agirre","Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs. This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead. In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.",,,,ACL
43,2017,Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks,"William Foland, James H. Martin","We present a system which parses sentences into Abstract Meaning Representations, improving state-of-the-art results for this task by more than 5%. AMR graphs represent semantic content using linguistic properties such as semantic roles, coreference, negation, and more. The AMR parser does not rely on a syntactic pre-parse, or heavily engineered features, and uses five recurrent neural networks as the key architectural components for inferring AMR graphs.",,,,ACL
44,2017,Deep Semantic Role Labeling: What Works and What’s Next,"Luheng He, Kenton Lee, Mike Lewis, Luke Zettlemoyer","We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10% relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.",,,,ACL
45,2017,Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access,"Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao","This paper proposes KB-InfoBot - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced “soft” posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents.",,,,ACL
46,2017,Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots,"Yu Wu, Wei Wu, Chen Xing, Ming Zhou","We study response selection for multi-turn conversation in retrieval based chatbots. Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among the utterances or important information in the context. We propose a sequential matching network (SMN) to address both problems. SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among the utterances. The final matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.",,,,ACL
47,2017,Learning Word-Like Units from Joint Audio-Visual Analysis,"David Harwath, James Glass","Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the word ‘lighthouse’ within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.",,,,ACL
48,2017,Joint CTC/attention decoding for end-to-end speech recognition,"Takaaki Hori, Shinji Watanabe, John Hershey","End-to-end automatic speech recognition (ASR) has become a popular alternative to conventional DNN/HMM systems because it avoids the need for linguistic resources such as pronunciation dictionary, tokenization, and context-dependency trees, leading to a greatly simplified model-building process. There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes joint decoding algorithm for end-to-end ASR with a hybrid CTC/attention architecture, which effectively utilizes both advantages in decoding. We have applied the proposed method to two ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and showing the comparable performance to conventional state-of-the-art DNN/HMM ASR systems without linguistic resources.",,,,ACL
49,2017,Found in Translation: Reconstructing Phylogenetic Language Trees from Translations,"Ella Rabinovich, Noam Ordan, Shuly Wintner","Translation has played an important role in trade, law, commerce, politics, and literature for thousands of years. Translators have always tried to be invisible; ideal translations should look as if they were written originally in the target language. We show that traces of the source language remain in the translation product to the extent that it is possible to uncover the history of the source language by looking only at the translation. Specifically, we automatically reconstruct phylogenetic language trees from monolingual texts (translated from several source languages). The signal of the source language is so powerful that it is retained even after two phases of translation. This strongly indicates that source language interference is the most dominant characteristic of translated texts, overshadowing the more subtle signals of universal properties of translation.",,,,ACL
50,2017,Predicting Native Language from Gaze,"Yevgeni Berzak, Chie Nakamura, Suzanne Flynn, Boris Katz","A fundamental question in language learning concerns the role of a speaker’s first language in second language acquisition. We present a novel methodology for studying this question: analysis of eye-movement patterns in second language reading of free-form text. Using this methodology, we demonstrate for the first time that the native language of English learners can be predicted from their gaze fixations when reading English. We provide analysis of classifier uncertainty and learned features, which indicates that differences in English reading are likely to be rooted in linguistic divergences across native languages. The presented framework complements production studies and offers new ground for advancing research on multilingualism.",,,,ACL
51,2017,MORSE: Semantic-ally Drive-n MORpheme SEgment-er,"Tarek Sakakini, Suma Bhat, Pramod Viswanath","We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic information for this task. We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality. We validate our algorithm across datasets and present state-of-the-art results.",,,,ACL
52,2017,Deep Pyramid Convolutional Neural Networks for Text Categorization,"Rie Johnson, Tong Zhang","This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text categorization that can efficiently represent long-range associations in text. In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data. However, the associated computational complexity increases as the networks go deeper, which poses serious challenges in practical applications. Moreover, it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data. Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much. We call it deep pyramid CNN. The proposed model with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization.",,,,ACL
53,2017,Improved Neural Relation Detection for Knowledge Base Question Answering,"Mo Yu, Wenpeng Yin, Kazi Saidul Hasan, Cicero dos Santos","Relation detection is a core component of many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning which detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different levels of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.",,,,ACL
54,2017,Deep Keyphrase Generation,"Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He","Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at https://github.com/memray/seq2seq-keyphrase.",,,,ACL
55,2017,Attention-over-Attention Neural Networks for Reading Comprehension,"Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang","Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. The proposed model aims to place another attention mechanism over the document-level attention and induces “attended attention” for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children’s Book Test.",,,,ACL
56,2017,Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations,"Gabriel Doyle, Amir Goldberg, Sameer Srivastava, Michael Frank","Cultural fit is widely believed to affect the success of individuals and the groups to which they belong. Yet it remains an elusive, poorly measured construct. Recent research draws on computational linguistics to measure cultural fit but overlooks asymmetries in cultural adaptation. By contrast, we develop a directed, dynamic measure of cultural fit based on linguistic alignment, which estimates the influence of one person’s word use on another’s and distinguishes between two enculturation mechanisms: internalization and self-regulation. We use this measure to trace employees’ enculturation trajectories over a large, multi-year corpus of corporate emails and find that patterns of alignment in the first six months of employment are predictive of individuals’ downstream outcomes, especially involuntary exit. Further predictive analyses suggest referential alignment plays an overlooked role in linguistic alignment.",,,,ACL
57,2017,Representations of language in a model of visually grounded speech signal,"Grzegorz Chrupała, Lieke Gelderloos, Afra Alishahi","We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.",,,,ACL
58,2017,Spectral Analysis of Information Density in Dialogue Predicts Collaborative Task Performance,"Yang Xu, David Reitter","We propose a perspective on dialogue that focuses on relative information contributions of conversation partners as a key to successful communication. We predict the success of collaborative task in English and Danish corpora of task-oriented dialogue. Two features are extracted from the frequency domain representations of the lexical entropy series of each interlocutor, power spectrum overlap (PSO) and relative phase (RP). We find that PSO is a negative predictor of task success, while RP is a positive one. An SVM with these features significantly improved on previous task success prediction models. Our findings suggest that the strategic distribution of information density between interlocutors is relevant to task success.",,,,ACL
59,2017,Affect-LM: A Neural Language Model for Customizable Affective Text Generation,"Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency","Human verbal communication includes affective messages which are conveyed through use of emotionally colored words. There has been a lot of research effort in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generation of conversational text, conditioned on affect categories. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using Amazon Mechanical Turk show that Affect-LM can generate naturally looking emotional sentences without sacrificing grammatical correctness. Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction.",,,,ACL
60,2017,Domain Attention with an Ensemble of Experts,"Young-Bum Kim, Karl Stratos, Dongchan Kim","An important problem in domain adaptation is to quickly generalize to a new domain with limited supervision given K existing domains. One approach is to retrain a global model across all K + 1 domains using standard techniques, for instance Daumé III (2009). However, it is desirable to adapt without having to re-estimate a global model from scratch each time a new domain with potentially new intents and slots is added. We describe a solution based on attending an ensemble of domain experts. We assume K domain specific intent and slot models trained on respective domains. When given domain K + 1, our model uses a weighted combination of the K domain experts’ feedback along with its own opinion to make predictions on the new domain. In experiments, the model significantly outperforms baselines that do not use domain adaptation and also performs better than the full retraining approach.",,,,ACL
61,2017,Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,"Tiancheng Zhao, Ran Zhao, Maxine Eskenazi","While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder from word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that capture the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved through introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence of discourse-level decision-making.",,,,ACL
62,2017,Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning,"Jason D. Williams, Kavosh Asadi, Geoffrey Zweig","End-to-end learning of recurrent neural networks (RNNs) is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors. We introduce Hybrid Code Networks (HCNs), which combine an RNN with domain-specific knowledge encoded as software and system action templates. Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state. In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both. HCNs attain state-of-the-art performance on the bAbI dialog dataset (Bordes and Weston, 2016), and outperform two commercially deployed customer-facing dialog systems at our company.",,,,ACL
63,2017,Generating Contrastive Referring Expressions,"Martín Villalba, Christoph Teichmann, Alexander Koller","The referring expressions (REs) produced by a natural language generation (NLG) system can be misunderstood by the hearer, even when they are semantically correct. In an interactive setting, the NLG system can try to recognize such misunderstandings and correct them. We present an algorithm for generating corrective REs that use contrastive focus (“no, the BLUE button”) to emphasize the information the hearer most likely misunderstood. We show empirically that these contrastive REs are preferred over REs without contrast marking.",,,,ACL
64,2017,Modeling Source Syntax for Neural Machine Translation,"Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu","Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize parse trees of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.",,,,ACL
65,2017,Sequence-to-Dependency Neural Machine Translation,"Shuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li","Nowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned. Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and Japanese-English translation tasks.",,,,ACL
66,2017,Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning,"Jing Ma, Wei Gao, Kam-Fai Wong","How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-of-the-art rumor detection models.",,,,ACL
67,2017,EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks,"Muhammad Abdul-Mageed, Lyle Ungar","Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on emotion detection has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it. We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58%). We also extend the task beyond emotion types to model Robert Plutick’s 8 primary emotion dimensions, acquiring a superior accuracy of 95.68%.",,,,ACL
68,2017,Beyond Binary Labels: Political Ideology Prediction of Twitter Users,"Daniel Preoţiuc-Pietro, Ye Liu, Daniel Hopkins, Lyle Ungar","Automatic political orientation prediction from social media posts has to date proven successful only in distinguishing between publicly declared liberals and conservatives in the US. This study examines users’ political ideology using a seven-point scale which enables us to identify politically moderate and neutral users – groups which are of particular interest to political scientists and pollsters. Using a novel data set with political ideology labels self-reported through surveys, our goal is two-fold: a) to characterize the groups of politically engaged users through language use on Twitter; b) to build a fine-grained model that predicts political ideology of unseen users. Our results identify differences in both political leaning and engagement and the extent to which each group tweets using political keywords. Finally, we demonstrate how to improve ideology prediction accuracy by exploiting the relationships between the user groups.",,,,ACL
69,2017,Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter,"Kristen Johnson, Di Jin, Dan Goldwasser","Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring political framing typically analyze frame usage in longer texts, such as congressional speeches. We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter. Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.",,,,ACL
70,2017,A Nested Attention Neural Hybrid Model for Grammatical Error Correction,"Jianshu Ji, Qinlong Wang, Kristina Toutanova, Yongen Gong","Grammatical error correction (GEC) systems strive to correct both global errors inword order and usage, and local errors inspelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC.Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information, and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset.Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective incorrecting local errors that involve small edits in orthography.",,,,ACL
71,2017,TextFlow: A Text Similarity Measure based on Continuous Sequences,"Yassine Mrabet, Halil Kilicoglu, Dina Demner-Fushman","Text similarity measures are used in multiple tasks such as plagiarism detection, information ranking and recognition of paraphrases and textual entailment. While recent advances in deep learning highlighted the relevance of sequential models in natural language generation, existing similarity measures do not fully exploit the sequential nature of language. Examples of such similarity measures include n-grams and skip-grams overlap which rely on distinct slices of the input texts. In this paper we present a novel text similarity measure inspired from a common representation in DNA sequence alignment algorithms. The new measure, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the similarity value. Our experiments on 8 different datasets show very encouraging results in paraphrase detection, textual entailment recognition and ranking relevance.",,,,ACL
72,2017,"Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts","Chenhao Tan, Dallas Card, Noah A. Smith","Understanding how ideas relate to each other is a fundamental question in many domains, ranging from intellectual history to public communication. Because ideas are naturally embedded in texts, we propose the first framework to systematically characterize the relations between ideas based on their occurrence in a corpus of documents, independent of how these ideas are represented. Combining two statistics—cooccurrence within documents and prevalence correlation over time—our approach reveals a number of different ways in which ideas can cooperate and compete. For instance, two ideas can closely track each other’s prevalence over time, and yet rarely cooccur, almost like a “cold war” scenario. We observe that pairwise cooccurrence and prevalence correlation exhibit different distributions. We further demonstrate that our approach is able to uncover intriguing relations between ideas through in-depth case studies on news articles and research papers.",,,,ACL
73,2017,Polish evaluation dataset for compositional distributional semantics models,"Alina Wróblewska, Katarzyna Krasnowska-Kieraś","The paper presents a procedure of building an evaluation dataset. for the validation of compositional distributional semantics models estimated for languages other than English. The procedure generally builds on steps designed to assemble the SICK corpus, which contains pairs of English sentences annotated for semantic relatedness and entailment, because we aim at building a comparable dataset. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the need for language-specific transformation rules. The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset. The resource consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish.",,,,ACL
74,2017,Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction,"Christopher Bryant, Mariano Felice, Ted Briscoe","Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as “Good” or “Acceptable” in at least 95% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.",,,,ACL
75,2017,Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability,"Saku Sugawara, Yusuke Kido, Hikaru Yokono, Akiko Aizawa","Knowing the quality of reading comprehension (RC) datasets is important for the development of natural-language understanding systems. In this study, two classes of metrics were adopted for evaluating RC datasets: prerequisite skills and readability. We applied these classes to six existing datasets, including MCTest and SQuAD, and highlighted the characteristics of the datasets according to each metric and the correlation between the two classes. Our dataset analysis suggests that the readability of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy to read but difficult to answer.",,,,ACL
76,2017,A Minimal Span-Based Neural Constituency Parser,"Mitchell Stern, Jacob Andreas, Dan Klein","In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).",,,,ACL
77,2017,Semantic Dependency Parsing via Book Embedding,"Weiwei Sun, Junjie Cao, Xiaojun Wan","We model a dependency graph as a book, a particular kind of topological space, for semantic dependency parsing. The spine of the book is made up of a sequence of words, and each page contains a subset of noncrossing arcs. To build a semantic graph for a given sentence, we design new Maximum Subgraph algorithms to generate noncrossing graphs on each page, and a Lagrangian Relaxation-based algorithm tocombine pages into a book. Experiments demonstrate the effectiveness of the bookembedding framework across a wide range of conditions. Our parser obtains comparable results with a state-of-the-art transition-based parser.",,,,ACL
78,2017,Neural Word Segmentation with Rich Pretraining,"Jie Yang, Yue Zhang, Fei Dong","Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.",,,,ACL
79,2017,Neural Machine Translation via Binary Code Prediction,"Yusuke Oda, Philip Arthur, Graham Neubig, Koichiro Yoshino","In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.",,,,ACL
80,2017,What do Neural Machine Translation Models Learn about Morphology?,"Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad","Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.",,,,ACL
81,2017,Context-Dependent Sentiment Analysis in User-Generated Videos,"Soujanya Poria, Erik Cambria, Devamanyu Hazarika, Navonil Majumder","Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the interdependencies and relations among the utterances of a video. In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process. Our method shows 5-10% performance improvement over the state of the art and high robustness to generalizability.",,,,ACL
82,2017,A Multidimensional Lexicon for Interpersonal Stancetaking,"Umashanthi Pavalanathan, Jim Fitzpatrick, Scott Kiesling, Jacob Eisenstein","The sociolinguistic construct of stancetaking describes the activities through which discourse participants create and signal relationships to their interlocutors, to the topic of discussion, and to the talk itself. Stancetaking underlies a wide range of interactional phenomena, relating to formality, politeness, affect, and subjectivity. We present a computational approach to stancetaking, in which we build a theoretically-motivated lexicon of stance markers, and then use multidimensional analysis to identify a set of underlying stance dimensions. We validate these dimensions intrinscially and extrinsically, showing that they are internally coherent, match pre-registered hypotheses, and correlate with social phenomena.",,,,ACL
83,2017,Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,"Jeffrey Lund, Connor Cook, Kevin Seppi, Jordan Boyd-Graber","Interactive topic models are powerful tools for those seeking to understand large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, going beyond existing single word anchor algorithms—an approach we call “Tandem Anchors”. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and non-interactive approaches. Tandem anchors are faster and more intuitive than existing interactive approaches.",,,,ACL
84,2017,Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task,"Omid Bakhshandeh, James Allen","Understanding common entities and their attributes is a primary requirement for any system that comprehends natural language. In order to enable learning about common entities, we introduce a novel machine comprehension task, GuessTwo: given a short paragraph comparing different aspects of two real-world semantically-similar entities, a system should guess what those entities are. Accomplishing this task requires deep language understanding which enables inference, connecting each comparison paragraph to different levels of knowledge about world entities and their attributes. So far we have crowdsourced a dataset of more than 14K comparison paragraphs comparing entities from a variety of categories such as fruits and animals. We have designed two schemes for evaluation: open-ended, and binary-choice prediction. For benchmarking further progress in the task, we have collected a set of paragraphs as the test set on which human can accomplish the task with an accuracy of 94.2% on open-ended prediction. We have implemented various models for tackling the task, ranging from semantic-driven to neural models. The semantic-driven approach outperforms the neural models, however, the results indicate that the task is very challenging across the models.",,,,ACL
85,2017,Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees,"Arzoo Katiyar, Claire Cardie","We present a novel attention-based recurrent neural network for joint extraction of entity mentions and relations. We show that attention along with long short term memory (LSTM) network can extract semantic relations between entity mentions without having access to dependency trees. Experiments on Automatic Content Extraction (ACE) corpora show that our model significantly outperforms feature-based joint model by Li and Ji (2014). We also compare our model with an end-to-end tree-based LSTM model (SPTree) by Miwa and Bansal (2016) and show that our model performs within 1% on entity mentions and 2% on relations. Our fine-grained analysis also shows that our model performs significantly better on Agent-Artifact relations, while SPTree performs better on Physical and Part-Whole relations.",,,,ACL
86,2017,Naturalizing a Programming Language via Interactive Learning,"Sida I. Wang, Samuel Ginn, Percy Liang, Christopher D. Manning","Our goal is to create a convenient natural language interface for performing well-specified but complex actions such as analyzing data, manipulating text, and querying databases. However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language. To bridge this gap, we start with a core programming language and allow users to “naturalize” the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9% of the last 10K utterances.",,,,ACL
87,2017,Semantic Word Clusters Using Signed Spectral Clustering,"João Sedoc, Jean Gallier, Dean Foster, Lyle Ungar","Vector space representations of words capture many aspects of word similarity, but such methods tend to produce vector spaces in which antonyms (as well as synonyms) are close to each other. For spectral clustering using such word embeddings, words are points in a vector space where synonyms are linked with positive weights, while antonyms are linked with negative weights. We present a new signed spectral normalized graph cut algorithm, signed clustering, that overlays existing thesauri upon distributionally derived vector representations of words, so that antonym relationships between word pairs are represented by negative weights. Our signed clustering algorithm produces clusters of words that simultaneously capture distributional and synonym relations. By using randomized spectral decomposition (Halko et al., 2011) and sparse matrices, our method is both fast and scalable. We validate our clusters using datasets containing human judgments of word pair similarities and show the benefit of using our word clusters for sentiment prediction.",,,,ACL
88,2017,An Interpretable Knowledge Transfer Model for Knowledge Base Completion,"Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy","Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets—WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.",,,,ACL
89,2017,Learning a Neural Semantic Parser from User Feedback,"Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy","We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.",,,,ACL
90,2017,Joint Modeling of Content and Discourse Relations in Dialogues,"Kechen Qin, Lu Wang, Joseph Kim",We present a joint modeling approach to identify salient discussion points in spoken meetings as well as to label the discourse relations between speaker turns. A variation of our model is also discussed when discourse relations are treated as latent variables. Experimental results on two popular meeting corpora show that our joint model can outperform state-of-the-art approaches for both phrase-based content selection and discourse relation prediction tasks. We also evaluate our model on predicting the consistency among team members’ understanding of their group decisions. Classifiers trained with features constructed from our model achieve significant better predictive performance than the state-of-the-art.,,,,ACL
91,2017,Argument Mining with Structured SVMs and RNNs,"Vlad Niculae, Joonsuk Park, Claire Cardie","We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure. (This is the case in over 20% of the web comments dataset we release.) Our model jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.",,,,ACL
92,2017,Neural Discourse Structure for Text Categorization,"Yangfeng Ji, Noah A. Smith","We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.",,,,ACL
93,2017,Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification,"Lianhui Qin, Zhisong Zhang, Hai Zhao, Zhiting Hu","Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.",,,,ACL
94,2017,Don’t understand a measure? Learn it: Structured Prediction for Coreference Resolution optimizing its measures,"Iryna Haponchyk, Alessandro Moschitti","An interesting aspect of structured prediction is the evaluation of an output structure against the gold standard. Especially in the loss-augmented setting, the need of finding the max-violating constraint has severely limited the expressivity of effective loss functions. In this paper, we trade off exact computation for enabling the use and study of more complex loss functions for coreference resolution. Most interestingly, we show that such functions can be (i) automatically learned also from controversial but commonly accepted coreference measures, e.g., MELA, and (ii) successfully used in learning algorithms. The accurate model comparison on the standard CoNLL-2012 setting shows the benefit of more expressive loss functions.",,,,ACL
95,2017,Bayesian Modeling of Lexical Resources for Low-Resource Settings,"Nicholas Andrews, Mark Dredze, Benjamin Van Durme, Jason Eisner","Lexical resources such as dictionaries and gazetteers are often used as auxiliary data for tasks such as part-of-speech induction and named-entity recognition. However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of features which generalize better. In this paper, we investigate a more robust approach: we stipulate that the lexicon is the result of an assumed generative process. Practically, this means that we may treat the lexical resources as observations under the proposed generative model. The lexical resources provide training data for the generative model without requiring separate data to estimate lexical feature weights. We evaluate the proposed approach in two settings: part-of-speech induction and low-resource named-entity recognition.",,,,ACL
96,2017,Semi-Supervised QA with Generative Domain-Adaptive Nets,"Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, William Cohen","We study the problem of semi-supervised question answering—utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.",,,,ACL
97,2017,From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood,"Kelvin Guu, Panupong Pasupat, Evan Liu, Percy Liang","Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.",,,,ACL
98,2017,Diversity driven attention model for query-based abstractive summarization,"Preksha Nema, Mitesh M. Khapra, Anirban Laha, Balaraman Ravindran","Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.",,,,ACL
99,2017,Get To The Point: Summarization with Pointer-Generator Networks,"Abigail See, Peter J. Liu, Christopher D. Manning","Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",,,,ACL
100,2017,Supervised Learning of Automatic Pyramid for Optimization-Based Multi-Document Summarization,"Maxime Peyrard, Judith Eckle-Kohler","We present a new supervised framework that learns to estimate automatic Pyramid scores and uses them for optimization-based extractive multi-document summarization. For learning automatic Pyramid scores, we developed a method for automatic training data generation which is based on a genetic algorithm using automatic Pyramid as the fitness function. Our experimental evaluation shows that our new framework significantly outperforms strong baselines regarding automatic Pyramid, and that there is much room for improvement in comparison with the upper-bound for automatic Pyramid.",,,,ACL
101,2017,Selective Encoding for Abstractive Sentence Summarization,"Qingyu Zhou, Nan Yang, Furu Wei, Ming Zhou","We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.",,,,ACL
102,2017,PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents,"Corina Florescu, Cornelia Caragea","The large and growing amounts of online scholarly data present both challenges and opportunities to enhance knowledge discovery. One such challenge is to automatically extract a small set of keyphrases from a document that can accurately describe the document’s content and can facilitate fast information processing. In this paper, we propose PositionRank, an unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a word’s occurrences into a biased PageRank. Our model obtains remarkable improvements in performance over PageRank models that do not take into account word positions as well as over strong baselines for this task. Specifically, on several datasets of research papers, PositionRank achieves improvements as high as 29.09%.",,,,ACL
103,2017,Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses,"Ryan Lowe, Michael Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier","Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem.We present an evaluation model (ADEM)that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model’s predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue mod-els unseen during training, an important step for automatic dialogue evaluation.",,,,ACL
104,2017,A Transition-Based Directed Acyclic Graph Parser for UCCA,"Daniel Hershcovich, Omri Abend, Ari Rappoport","We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.",,,,ACL
105,2017,Abstract Syntax Networks for Code Generation and Semantic Parsing,"Maxim Rabinovich, Mitchell Stern, Dan Klein","Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.",,,,ACL
106,2017,Visualizing and Understanding Neural Machine Translation,"Yanzhuo Ding, Yang Liu, Huanbo Luan, Maosong Sun","While neural machine translation (NMT) has made remarkable progress in recent years, it is hard to interpret its internal workings due to the continuous representations and non-linearity of neural networks. In this work, we propose to use layer-wise relevance propagation (LRP) to compute the contribution of each contextual word to arbitrary hidden states in the attention-based encoder-decoder framework. We show that visualization with LRP helps to interpret the internal workings of NMT and analyze translation errors.",,,,ACL
107,2017,Detecting annotation noise in automatically labelled data,"Ines Rehbein, Josef Ruppenhofer","We introduce a method for error detection in automatically annotated text, aimed at supporting the creation of high-quality language resources at affordable cost. Our method combines an unsupervised generative model with human supervision from active learning. We test our approach on in-domain and out-of-domain data in two languages, in AL simulations and in a real world setting. For all settings, the results show that our method is able to detect annotation errors with high precision and high recall.",,,,ACL
108,2017,Abstractive Document Summarization with a Graph-Based Attentional Neural Model,"Jiwei Tan, Xiaojun Wan, Jianguo Xiao","Abstractive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization, which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.",,,,ACL
109,2017,Probabilistic Typology: Deep Generative Models of Vowel Inventories,"Ryan Cotterell, Jason Eisner","Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most—but not all—languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.",,,,ACL
110,2017,Adversarial Multi-Criteria Learning for Chinese Word Segmentation,"Xinchi Chen, Zhan Shi, Xipeng Qiu, Xuanjing Huang","Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github.",,,,ACL
111,2017,Neural Joint Model for Transition-based Chinese Syntactic Analysis,"Shuhei Kurita, Daisuke Kawahara, Sadao Kurohashi","We present neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing. Our models are the first neural approaches for fully joint Chinese analysis that is known to prevent the error propagation problem of pipeline models. Although word embeddings play a key role in dependency parsing, they cannot be applied directly to the joint task in the previous work. To address this problem, we propose embeddings of character strings, in addition to words. Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing. We also explore bi-LSTM models with fewer features.",,,,ACL
112,2017,Robust Incremental Neural Semantic Graph Parsing,"Jan Buys, Phil Blunsom","Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.",,,,ACL
113,2017,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,"Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao","Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What’s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.",,,,ACL
114,2017,A Local Detection Approach for Named Entity Recognition and Mention Detection,"Mingbin Xu, Hui Jiang, Sedtawut Watcharawittayakul","In this paper, we study a novel approach for named entity recognition (NER) and mention detection (MD) in natural language processing. Instead of treating NER as a sequence labeling problem, we propose a new local detection approach, which relies on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixed-size representation. Subsequently, a simple feedforward neural network (FFNN) is learned to either reject or predict entity label for each individual text fragment. The proposed method has been evaluated in several popular NER and MD tasks, including CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks. Our method has yielded pretty strong performance in all of these examined tasks. This local detection approach has shown many advantages over the traditional sequence labeling methods.",,,,ACL
115,2017,Vancouver Welcomes You! Minimalist Location Metonymy Resolution,"Milan Gritta, Mohammad Taher Pilehvar, Nut Limsopatham, Nigel Collier","Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.",,,,ACL
116,2017,"Unifying Text, Metadata, and User Network Representations with a Neural Network for Geolocation Prediction","Yasuhide Miura, Motoki Taniguchi, Tomoki Taniguchi, Tomoko Ohkuma","We propose a novel geolocation prediction model using a complex neural network. Geolocation prediction in social media has attracted many researchers to use information of various types. Our model unifies text, metadata, and user network representations with an attention mechanism to overcome previous ensemble approaches. In an evaluation using two open datasets, the proposed model exhibited a maximum 3.8% increase in accuracy and a maximum of 6.6% increase in accuracy@161 against previous models. We further analyzed several intermediate layers of our model, which revealed that their states capture some statistical characteristics of the datasets.",,,,ACL
117,2017,Multi-Task Video Captioning with Video and Entailment Generation,"Ramakanth Pasunuru, Mohit Bansal","Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailing caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.",,,,ACL
118,2017,Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts,"Leandro Santos, Edilson Anselmo Corrêa Júnior, Osvaldo Oliveira Jr, Diego Amancio","Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose. Linguistic features, mainly from parsers, have been used to detect MCI, but this is not suitable for large-scale assessments. MCI disfluencies produce non-grammatical speech that requires manual or high precision automatic correction of transcripts. In this paper, we modeled transcripts into complex networks and enriched them with word embedding (CNE) to better represent short texts produced in neuropsychological assessments. The network measurements were applied with well-known classifiers to automatically identify MCI in transcripts, in a binary classification task. A comparison was made with the performance of traditional approaches using Bag of Words (BoW) and linguistic features for three datasets: DementiaBank in English, and Cinderella and Arizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using only complex networks, while Support Vector Machine was superior to other classifiers. CNE provided the highest accuracies for DementiaBank and Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably owing to its short narratives. The approach using linguistic features yielded higher accuracy if the transcriptions of the Cinderella dataset were manually revised. Taken together, the results indicate that complex networks enriched with embedding is promising for detecting MCI in large-scale assessments.",,,,ACL
119,2017,Adversarial Adaptation of Synthetic or Stale Data,"Young-Bum Kim, Karl Stratos, Dongchan Kim","Two types of data shift common in practice are 1. transferring from synthetic data to live user data (a deployment shift), and 2. transferring from stale data to current data (a temporal shift). Both cause a distribution mismatch between training and evaluation, leading to a model that overfits the flawed training data and performs poorly on the test data. We propose a solution to this mismatch problem by framing it as domain adaptation, treating the flawed training dataset as a source domain and the evaluation dataset as a target domain. To this end, we use and build on several recent advances in neural domain adaptation such as adversarial training (Ganinet al., 2016) and domain separation network (Bousmalis et al., 2016), proposing a new effective adversarial training scheme. In both supervised and unsupervised adaptation scenarios, our approach yields clear improvement over strong baselines.",,,,ACL
120,2017,Chat Detection in an Intelligent Assistant: Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems,"Satoshi Akasaki, Nobuhiro Kaji","Recently emerged intelligent assistants on smartphones and home electronics (e.g., Siri and Alexa) can be seen as novel hybrids of domain-specific task-oriented spoken dialogue systems and open-domain non-task-oriented ones. To realize such hybrid dialogue systems, this paper investigates determining whether or not a user is going to have a chat with the system. To address the lack of benchmark datasets for this task, we construct a new dataset consisting of 15,160 utterances collected from the real log data of a commercial intelligent assistant (and will release the dataset to facilitate future research activity). In addition, we investigate using tweets and Web search queries for handling open-domain user utterances, which characterize the task of chat detection. Experimental experiments demonstrated that, while simple supervised methods are effective, the use of the tweets and search queries further improves the F1-score from 86.21 to 87.53.",,,,ACL
121,2017,A Neural Local Coherence Model,"Dat Tien Nguyen, Shafiq Joty","We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.",,,,ACL
122,2017,Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation (ONLG),"Tomer Cagan, Stefan L. Frank, Reut Tsarfaty","Opinionated Natural Language Generation (ONLG) is a new, challenging, task that aims to automatically generate human-like, subjective, responses to opinionated articles online. We present a data-driven architecture for ONLG that generates subjective responses triggered by users’ agendas, consisting of topics and sentiments, and based on wide-coverage automatically-acquired generative grammars. We compare three types of grammatical representations that we design for ONLG, which interleave different layers of linguistic information and are induced from a new, enriched dataset we developed. Our evaluation shows that generation with Relational-Realizational (Tsarfaty and Sima’an, 2008) inspired grammar gets better language model scores than lexicalized grammars ‘a la Collins (2003), and that the latter gets better human-evaluation scores. We also show that conditioning the generation on topic models makes generated responses more relevant to the document content.",,,,ACL
123,2017,Learning to Ask: Neural Question Generation for Reading Comprehension,"Xinya Du, Junru Shao, Claire Cardie","We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e.,, grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).",,,,ACL
124,2017,Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback,"Avinesh P.V.S, Christian M. Meyer","In this paper, we propose an extractive multi-document summarization (MDS) system using joint optimization and active learning for content selection grounded in user feedback. Our method interactively obtains user feedback to gradually improve the results of a state-of-the-art integer linear programming (ILP) framework for MDS. Our methods complement fully automatic methods in producing high-quality summaries with a minimum number of iterations and feedbacks. We conduct multiple simulation-based experiments and analyze the effect of feedback-based concept selection in the ILP setup in order to maximize the user-desired content in the summary.",,,,ACL
125,2017,Flexible and Creative Chinese Poetry Generation Using Neural Memory,"Jiyuan Zhang, Yang Feng, Dong Wang, Yang Wang","It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.",,,,ACL
126,2017,Learning to Generate Market Comments from Stock Prices,"Soichiro Murakami, Akihiko Watanabe, Akira Miyazawa, Keiichi Goshima","This paper presents a novel encoder-decoder model for automatically generating market comments from stock prices. The model first encodes both short- and long-term series of stock prices so that it can mention short- and long-term changes in stock prices. In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic operation such as subtraction or rounding, and applying it to the input stock prices. Empirical experiments show that our best model generates market comments at the fluency and the informativeness approaching human-generated reference texts.",,,,ACL
127,2017,Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains,"Liangguo Wang, Jing Jiang, Hai Leong Chieu, Chen Hui Ong","In this paper, we study how to improve the domain adaptability of a deletion-based Long Short-Term Memory (LSTM) neural network model for sentence compression. We hypothesize that syntactic information helps in making such models more robust across domains. We propose two major changes to the model: using explicit syntactic features and introducing syntactic constraints through Integer Linear Programming (ILP). Our evaluation shows that the proposed model works better than the original model as well as a traditional non-neural-network-based model in a cross-domain setting.",,,,ACL
128,2017,Transductive Non-linear Learning for Chinese Hypernym Prediction,"Chengyu Wang, Junchi Yan, Aoying Zhou, Xiaofeng He","Finding the correct hypernyms for entities is essential for taxonomy learning, fine-grained entity categorization, query understanding, etc. Due to the flexibility of the Chinese language, it is challenging to identify hypernyms in Chinese accurately. Rather than extracting hypernyms from texts, in this paper, we present a transductive learning approach to establish mappings from entities to hypernyms in the embedding space directly. It combines linear and non-linear embedding projection models, with the capacity of encoding arbitrary language-specific rules. Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction.",,,,ACL
129,2017,A Constituent-Centric Neural Architecture for Reading Comprehension,"Pengtao Xie, Eric Xing","Reading comprehension (RC), aiming to understand natural texts and answer questions therein, is a challenging task. In this paper, we study the RC problem on the Stanford Question Answering Dataset (SQuAD). Observing from the training set that most correct answers are centered around constituents in the parse tree, we design a constituent-centric neural architecture where the generation of candidate answers and their representation learning are both based on constituents and guided by the parse tree. Under this architecture, the search space of candidate answers can be greatly reduced without sacrificing the coverage of correct answers and the syntactic, hierarchical and compositional structure among constituents can be well captured, which contributes to better representation learning of the candidate answers. On SQuAD, our method achieves the state of the art performance and the ablation study corroborates the effectiveness of individual modules.",,,,ACL
130,2017,Cross-lingual Distillation for Text Classification,"Ruochen Xu, Yiming Yang","Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.",,,,ACL
131,2017,Understanding and Predicting Empathic Behavior in Counseling Therapy,"Verónica Pérez-Rosas, Rada Mihalcea, Kenneth Resnicow, Satinder Singh","Counselor empathy is associated with better outcomes in psychology and behavioral counseling. In this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. Particularly, we analyze aspects such as participants’ engagement, participants’ verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.",,,,ACL
132,2017,Leveraging Knowledge Bases in LSTMs for Improving Machine Reading,"Bishan Yang, Tom Mitchell","This paper focuses on how to take advantage of external knowledge bases (KBs) to improve recurrent neural networks for machine reading. Traditional methods that exploit knowledge from KBs encode knowledge as discrete indicator features. Not only do these features generalize poorly, but they require task-specific feature engineering to achieve good performance. We propose KBLSTM, a novel neural model that leverages continuous representations of KBs to enhance the learning of recurrent neural networks for machine reading. To effectively integrate background knowledge with information from the currently processed text, our model employs an attention mechanism with a sentinel to adaptively decide whether to attend to background knowledge and which information from KBs is useful. Experimental results show that our model achieves accuracies that surpass the previous state-of-the-art results for both entity extraction and event extraction on the widely used ACE2005 dataset.",,,,ACL
133,2017,Prerequisite Relation Learning for Concepts in MOOCs,"Liangming Pan, Chengjiang Li, Juanzi Li, Jie Tang","What prerequisite knowledge should students achieve a level of mastery before moving forward to learn subsequent coursewares? We study the extent to which the prerequisite relation between knowledge concepts in Massive Open Online Courses (MOOCs) can be inferred automatically. In particular, what kinds of information can be leverage to uncover the potential prerequisite relation between knowledge concepts. We first propose a representation learning-based method for learning latent representations of course concepts, and then investigate how different features capture the prerequisite relations between concepts. Our experiments on three datasets form Coursera show that the proposed method achieves significant improvements (+5.9-48.0% by F1-score) comparing with existing methods.",,,,ACL
134,2017,Unsupervised Text Segmentation Based on Native Language Characteristics,"Shervin Malmasi, Mark Dras, Mark Johnson, Lan Du","Most work on segmenting text does so on the basis of topic changes, but it can be of interest to segment by other, stylistically expressed characteristics such as change of authorship or native language. We propose a Bayesian unsupervised text segmentation approach to the latter. While baseline models achieve essentially random segmentation on our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation.",,,,ACL
135,2017,Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection,"Jian Ni, Georgiana Dinu, Radu Florian","The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper, we present two weakly supervised approaches for cross-lingual NER with no human annotation in a target language. The first approach is to create automatically labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuristic scheme that effectively selects good-quality projection-labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without re-training. We also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches. We evaluate the performance of the proposed approaches on both in-house and open NER data for several target languages. The results show that the combined systems outperform three other weakly supervised approaches on the CoNLL data.",,,,ACL
136,2017,Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks,"Abhisek Chakrabarty, Onkar Arun Pandit, Utpal Garain","We introduce a composite deep neural network architecture for supervised and language independent context sensitive lemmatization. The proposed method considers the task as to identify the correct edit tree representing the transformation between a word-lemma pair. To find the lemma of a surface word, we exploit two successive bidirectional gated recurrent structures - the first one is used to extract the character level dependencies and the next one captures the contextual information of the given word. The key advantages of our model compared to the state-of-the-art lemmatizers such as Lemming and Morfette are - (i) it is independent of human decided features (ii) except the gold lemma, no other expensive morphological attribute is required for joint learning. We evaluate the lemmatizer on nine languages - Bengali, Catalan, Dutch, Hindi, Hungarian, Italian, Latin, Romanian and Spanish. It is found that except Bengali, the proposed method outperforms Lemming and Morfette on the other languages. To train the model on Bengali, we develop a gold lemma annotated dataset (having 1,702 sentences with a total of 20,257 word tokens), which is an additional contribution of this work.",,,,ACL
137,2017,Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling,"Kazuya Kawakami, Chris Dyer, Phil Blunsom","Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the “bursty” distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.",,,,ACL
138,2017,Bandit Structured Prediction for Neural Sequence-to-Sequence Learning,"Julia Kreutzer, Artem Sokolov, Stefan Riezler","Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.",,,,ACL
139,2017,Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization,"Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu","Although neural machine translation has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge. In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into neural machine translation. We represent prior knowledge sources as features in a log-linear model, which guides the learning processing of the neural translation model. Experiments on Chinese-English dataset show that our approach leads to significant improvements.",,,,ACL
140,2017,Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation,"Jinchao Zhang, Mingxuan Wang, Qun Liu, Jie Zhou","This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance. Our proposed models enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality.",,,,ACL
141,2017,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,"Chris Hokamp, Qun Liu","We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model’s output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.",,,,ACL
142,2017,Combating Human Trafficking with Multimodal Deep Models,"Edmund Tong, Amir Zadeh, Cara Jones, Louis-Philippe Morency","Human trafficking is a global epidemic affecting millions of people across the planet. Sex trafficking, the dominant form of human trafficking, has seen a significant rise mostly due to the abundance of escort websites, where human traffickers can openly advertise among at-will escort advertisements. In this paper, we take a major step in the automatic detection of advertisements suspected to pertain to human trafficking. We present a novel dataset called Trafficking-10k, with more than 10,000 advertisements annotated for this task. The dataset contains two sources of information per advertisement: text and images. For the accurate detection of trafficking advertisements, we designed and trained a deep multimodal model called the Human Trafficking Deep Network (HTDN).",,,,ACL
143,2017,MalwareTextDB: A Database for Annotated Malware Articles,"Swee Kiat Lim, Aldrian Obaja Muis, Wei Lu, Chen Hui Ong","Cybersecurity risks and malware threats are becoming increasingly dangerous and common. Despite the severity of the problem, there has been few NLP efforts focused on tackling cybersecurity. In this paper, we discuss the construction of a new database for annotated malware texts. An annotation framework is introduced based on the MAEC vocabulary for defining malware characteristics, along with a database consisting of 39 annotated APT reports with a total of 6,819 sentences. We also use the database to construct models that can potentially help cybersecurity researchers in their data collection and analytics efforts.",,,,ACL
144,2017,A Corpus of Annotated Revisions for Studying Argumentative Writing,"Fan Zhang, Homa B. Hashemi, Rebecca Hwa, Diane Litman","This paper presents ArgRewrite, a corpus of between-draft revisions of argumentative essays. Drafts are manually aligned at the sentence level, and the writer’s purpose for each revision is annotated with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction.",,,,ACL
145,2017,Watset: Automatic Induction of Synsets from a Graph of Synonyms,"Dmitry Ustalov, Alexander Panchenko, Chris Biemann","This paper presents a new graph-based approach that induces synsets using synonymy dictionaries and word embeddings. First, we build a weighted graph of synonyms extracted from commonly available resources, such as Wiktionary. Second, we apply word sense induction to deal with ambiguous words. Finally, we cluster the disambiguated version of the ambiguous input graph into synsets. Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform a fuzzy clustering of the graph. Despite its simplicity, our approach shows excellent results, outperforming five competitive state-of-the-art methods in terms of F-score on three gold standard datasets for English and Russian derived from large-scale manually constructed lexical resources.",,,,ACL
146,2017,Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis,"Hiroki Ouchi, Hiroyuki Shindo, Yuji Matsumoto","The performance of Japanese predicate argument structure (PAS) analysis has improved in recent years thanks to the joint modeling of interactions between multiple predicates. However, this approach relies heavily on syntactic information predicted by parsers, and suffers from errorpropagation. To remedy this problem, we introduce a model that uses grid-type recurrent neural networks. The proposed model automatically induces features sensitive to multi-predicate interactions from the word sequence information of a sentence. Experiments on the NAIST Text Corpus demonstrate that without syntactic information, our model outperforms previous syntax-dependent models.",,,,ACL
147,2017,TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension,"Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer","We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.",,,,ACL
148,2017,Learning Semantic Correspondences in Technical Documentation,"Kyle Richardson, Jonas Kuhn","We consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation. We focus specifically on the problem of learning translational correspondences between text descriptions and grounded representations in the target documentation, such as formal representation of functions or code templates. Our approach exploits the parallel nature of such documentation, or the tight coupling between high-level text and the low-level representations we aim to learn. Data is collected by mining technical documents for such parallel text-representation pairs, which we use to train a simple semantic parsing model. We report new baseline results on sixteen novel datasets, including the standard library documentation for nine popular programming languages across seven natural languages, and a small collection of Unix utility manuals.",,,,ACL
149,2017,Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding,"Yixin Cao, Lifu Huang, Heng Ji, Xu Chen","Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense. In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance.",,,,ACL
150,2017,Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication,"Lanbo She, Joyce Chai","To enable human-robot communication and collaboration, previous works represent grounded verb semantics as the potential change of state to the physical world caused by these verbs. Grounded verb semantics are acquired mainly based on the parallel data of the use of a verb phrase and its corresponding sequences of primitive actions demonstrated by humans. The rich interaction between teachers and students that is considered important in learning new skills has not yet been explored. To address this limitation, this paper presents a new interactive learning approach that allows robots to proactively engage in interaction with human partners by asking good questions to learn models for grounded verb semantics. The proposed approach uses reinforcement learning to allow the robot to acquire an optimal policy for its question-asking behaviors by maximizing the long-term reward. Our empirical results have shown that the interactive learning approach leads to more reliable models for grounded verb semantics, especially in the noisy environment which is full of uncertainties. Compared to previous work, the models acquired from interactive learning result in a 48% to 145% performance gain when applied in new situations.",,,,ACL
151,2017,Multimodal Word Distributions,"Ben Athiwaratkun, Andrew Wilson","Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.",,,,ACL
152,2017,Enhanced LSTM for Natural Language Inference,"Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei","Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result—it further improves the performance even when added to the already very strong model.",,,,ACL
153,2017,Linguistic analysis of differences in portrayal of movie characters,"Anil Ramakrishna, Victor R. Martínez, Nikolaos Malandrakis, Karan Singla","We examine differences in portrayal of characters in movies using psycholinguistic and graph theoretic measures computed directly from screenplays. Differences are examined with respect to characters’ gender, race, age and other metadata. Psycholinguistic metrics are extrapolated to dialogues in movies using a linear regression model built on a set of manually annotated seed words. Interesting patterns are revealed about relationships between genders of production team and the gender ratio of characters. Several correlations are noted between gender, race, age of characters and the linguistic metrics.",,,,ACL
154,2017,Linguistically Regularized LSTM for Sentiment Classification,"Qiao Qian, Minlie Huang, Jinhao Lei, Xiaoyan Zhu","This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words). In this paper, we propose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression.",,,,ACL
155,2017,Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation,"Lotem Peled, Roi Reichart","Sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment. In other words, “Sarcasm is the giant chasm between what I say, and the person who doesn’t get it.”. In this paper we present the novel task of sarcasm interpretation, defined as the generation of a non-sarcastic utterance conveying the same message as the original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets, each interpreted by five human judges. Addressing the task as monolingual machine translation (MT), we experiment with MT algorithms and evaluation measures. We then present SIGN: an MT based sarcasm interpretation algorithm that targets sentiment words, a defining element of textual sarcasm. We show that while the scores of n-gram based automatic measures are similar for all interpretation models, SIGN’s interpretations are scored higher by humans for adequacy and sentiment polarity. We conclude with a discussion on future research directions for our new task.",,,,ACL
156,2017,Active Sentiment Domain Adaptation,"Fangzhao Wu, Yongfeng Huang, Jun Yan","Domain adaptation is an important technology to handle domain dependence problem in sentiment analysis field. Existing methods usually rely on sentiment classifiers trained in source domains. However, their performance may heavily decline if the distributions of sentiment features in source and target domains have significant difference. In this paper, we propose an active sentiment domain adaptation approach to handle this problem. Instead of the source domain sentiment classifiers, our approach adapts the general-purpose sentiment lexicons to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode, as well as the domain-specific sentiment similarities among words mined from unlabeled samples of target domain. A unified model is proposed to fuse different types of sentiment information and train sentiment classifier for target domain. Extensive experiments on benchmark datasets show that our approach can train accurate sentiment classifier with less labeled samples.",,,,ACL
157,2017,Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models,"Navid Rekabsaz, Mihai Lupu, Artem Baklanov, Alexander Dür","Volatility prediction—an essential concept in financial markets—has recently been addressed using sentiment analysis methods. We investigate the sentiment of annual disclosures of companies in stock markets to forecast volatility. We specifically explore the use of recent Information Retrieval (IR) term weighting models that are effectively extended by related terms using word embeddings. In parallel to textual information, factual market data have been widely used as the mainstream approach to forecast market risk. We therefore study different fusion methods to combine text and market data resources. Our word embedding-based approach significantly outperforms state-of-the-art methods. In addition, we investigate the characteristics of the reports of the companies in different financial sectors.",,,,ACL
158,2017,CANE: Context-Aware Network Embedding for Relation Modeling,"Cunchao Tu, Han Liu, Zhiyuan Liu, Maosong Sun","Network embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively. Therefore, we present Context-Aware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our model with existing NE models on three real-world datasets. Experimental results show that CANE achieves significant improvement than state-of-the-art methods on link prediction and comparable performance on vertex classification. The source code and datasets can be obtained from https://github.com/thunlp/CANE.",,,,ACL
159,2017,Universal Dependencies Parsing for Colloquial Singaporean English,"Hongmin Wang, Yue Zhang, GuangYong Leonard Chan, Jie Yang","Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-of-the-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies. To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages. We make both our annotation and parser available for further research.",,,,ACL
160,2017,Generic Axiomatization of Families of Noncrossing Graphs in Dependency Parsing,"Anssi Yli-Jyrä, Carlos Gómez-Rodríguez","We present a simple encoding for unlabeled noncrossing graphs and show how its latent counterpart helps us to represent several families of directed and undirected graphs used in syntactic and semantic parsing of natural language as context-free languages. The families are separated purely on the basis of forbidden patterns in latent encoding, eliminating the need to differentiate the families of non-crossing graphs in inference algorithms: one algorithm works for all when the search space can be controlled in parser input.",,,,ACL
161,2017,Semi-supervised sequence tagging with bidirectional language models,"Matthew Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power","Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.",,,,ACL
162,2017,Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings,"He He, Anusha Balakrishnan, Mihail Eric, Percy Liang","We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.",,,,ACL
163,2017,Neural Belief Tracker: Data-Driven Dialogue State Tracking,"Nikola Mrkšić, Diarmuid Ó Séaghdha, Tsung-Hsien Wen, Blaise Thomson","One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user’s goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users’ language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.",,,,ACL
164,2017,Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms,"Shulin Liu, Yubo Chen, Kang Liu, Jun Zhao","This paper tackles the task of event detection (ED), which involves identifying and categorizing events. We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms. In specific, we systematically investigate the proposed model under the supervision of different attention strategies. Experimental results show that our approach advances state-of-the-arts and achieves the best F1 score on ACE 2005 dataset.",,,,ACL
165,2017,Topical Coherence in LDA-based Models through Induced Segmentation,"Hesam Amoualian, Wei Lu, Eric Gaussier, Georgios Balikas","This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.",,,,ACL
166,2017,Jointly Extracting Relations with Class Ties via Effective Deep Ranking,"Hai Ye, Wenhan Chao, Zhunchen Luo, Zhoujun Li","Connections between relations in relation extraction, which we call class ties, are common. In distantly supervised scenario, one entity tuple may have multiple relation facts. Exploiting class ties between relations of one entity tuple will be promising for distantly supervised relation extraction. However, previous models are not effective or ignore to model this property. In this work, to effectively leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network (CNN) with a general pairwise ranking framework, in which three novel ranking loss functions are introduced. Additionally, an effective method is presented to relieve the severe class imbalance problem from NR (not relation) for model training. Experiments on a widely used dataset show that leveraging class ties will enhance extraction and demonstrate the effectiveness of our model to learn class ties. Our model outperforms the baselines significantly, achieving state-of-the-art performance.",,,,ACL
167,2017,Search-based Neural Structured Learning for Sequential Question Answering,"Mohit Iyyer, Wen-tau Yih, Ming-Wei Chang","Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but inter-related questions. We collect a dataset of 6,066 question sequences that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs in total. To solve this sequential question answering task, we propose a novel dynamic neural semantic parsing framework trained using a weakly supervised reward-guided search. Our model effectively leverages the sequential context to outperform state-of-the-art QA systems that are designed to answer highly complex questions.",,,,ACL
168,2017,Gated-Attention Readers for Text Comprehension,"Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William Cohen","In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task–the CNN & Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention.",,,,ACL
169,2017,Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering,"Jianbo Ye, Yanran Li, Zhaohui Wu, James Z. Wang","Word embeddings have become widely-used in document analysis. While a large number of models for mapping words to vector spaces have been developed, it remains undetermined how much net gain can be achieved over traditional approaches based on bag-of-words. In this paper, we propose a new document clustering approach by combining any word embedding with a state-of-the-art algorithm for clustering empirical distributions. By using the Wasserstein distance between distributions, the word-to-word semantic relationship is taken into account in a principled way. The new clustering method is easy to use and consistently outperforms other methods on a variety of data sets. More importantly, the method provides an effective framework for determining when and how much word embeddings contribute to document analysis. Experimental results with multiple embedding models are reported.",,,,ACL
170,2017,Towards a Seamless Integration of Word Senses into Downstream NLP Applications,"Mohammad Taher Pilehvar, Jose Camacho-Collados, Roberto Navigli, Nigel Collier","Lexical ambiguity can impede NLP systems from accurate understanding of semantics. Despite its potential benefits, the integration of sense-level information into NLP systems has remained understudied. By incorporating a novel disambiguation algorithm into a state-of-the-art classification model, we create a pipeline to integrate sense-level information into downstream NLP applications. We show that a simple disambiguation of the input text can lead to consistent performance improvement on multiple topic categorization and polarity detection datasets, particularly when the fine granularity of the underlying sense inventory is reduced and the document is sufficiently large. Our results also point to the need for sense representation research to focus more on in vivo evaluations which target the performance in downstream NLP applications rather than artificial benchmarks.",,,,ACL
171,2017,Reading Wikipedia to Answer Open-Domain Questions,"Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes",This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.,,,,ACL
172,2017,Learning to Skim Text,"Adams Wei Yu, Hongrae Lee, Quoc Le","Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.",,,,ACL
173,2017,An Algebra for Feature Extraction,Vivek Srikumar,"Though feature extraction is a necessary first step in statistical NLP, it is often seen as a mere preprocessing step. Yet, it can dominate computation time, both during training, and especially at deployment. In this paper, we formalize feature extraction from an algebraic perspective. Our formalization allows us to define a message passing algorithm that can restructure feature templates to be more computationally efficient. We show via experiments on text chunking and relation extraction that this restructuring does indeed speed up feature extraction in practice by reducing redundant computation.",,,,ACL
174,2017,Chunk-based Decoder for Neural Machine Translation,"Shonosuke Ishiwatari, Jingtao Yao, Shujie Liu, Mu Li","Chunks (or phrases) once played a pivotal role in machine translation. By using a chunk rather than a word as the basic translation unit, local (intra-chunk) and global (inter-chunk) word orders and dependencies can be easily modeled. The chunk structure, despite its importance, has not been considered in the decoders used for neural machine translation (NMT). In this paper, we propose chunk-based decoders for (NMT), each of which consists of a chunk-level decoder and a word-level decoder. The chunk-level decoder models global dependencies while the word-level decoder decides the local word order in a chunk. To output a target sentence, the chunk-level decoder generates a chunk representation containing global information, which the word-level decoder then uses as a basis to predict the words inside the chunk. Experimental results show that our proposed decoders can significantly improve translation performance in a WAT ‘16 English-to-Japanese translation task.",,,,ACL
175,2017,Doubly-Attentive Decoder for Multi-modal Neural Machine Translation,"Iacer Calixto, Qun Liu, Nick Campbell","We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.",,,,ACL
176,2017,A Teacher-Student Framework for Zero-Resource Neural Machine Translation,"Yun Chen, Yang Liu, Yong Cheng, Victor O.K. Li","While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on the assumption, our method is able to train a source-to-target NMT model (“student”) without parallel corpora available guided by an existing pivot-to-target NMT model (“teacher”) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.",,,,ACL
177,2017,Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder,"Huadong Chen, Shujian Huang, David Chiang, Jiajun Chen","Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.",,,,ACL
178,2017,Cross-lingual Name Tagging and Linking for 282 Languages,"Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman","The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating “silver-standard” annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.",,,,ACL
179,2017,Adversarial Training for Unsupervised Bilingual Lexicon Induction,"Meng Zhang, Yang Liu, Huanbo Luan, Maosong Sun","Word embeddings are well known to capture linguistic regularities of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previous endeavors to connect separate monolingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lexicon. In this work, we show that such cross-lingual connection can actually be established without any form of supervision. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues.",,,,ACL
180,2017,Estimating Code-Switching on Twitter with a Novel Generalized Word-Level Language Detection Technique,"Shruti Rijhwani, Royal Sequiera, Monojit Choudhury, Kalika Bali","Word-level language detection is necessary for analyzing code-switched text, where multiple languages could be mixed within a sentence. Existing models are restricted to code-switching between two specific languages and fail in real-world scenarios as text input rarely has a priori information on the languages used. We present a novel unsupervised word-level language detection technique for code-switched text for an arbitrarily large number of languages, which does not require any manually annotated training data. Our experiments with tweets in seven languages show a 74% relative error reduction in word-level labeling with respect to competitive baselines. We then use this system to conduct a large-scale quantitative analysis of code-switching patterns on Twitter, both global as well as region-specific, with 58M tweets.",,,,ACL
181,2017,Using Global Constraints and Reranking to Improve Cognates Detection,"Michael Bloodgood, Benjamin Strauss","Global constraints and reranking have not been used in cognates detection research to date. We propose methods for using global constraints by performing rescoring of the score matrices produced by state of the art cognates detection systems. Using global constraints to perform rescoring is complementary to state of the art methods for performing cognates detection and results in significant performance improvements beyond current state of the art performance on publicly available datasets with different language pairs and various conditions such as different levels of baseline state of the art performance and different data size conditions, including with more realistic large data size conditions than have been evaluated with in the past.",,,,ACL
182,2017,One-Shot Neural Cross-Lingual Transfer for Paradigm Completion,"Katharina Kann, Ryan Cotterell, Hinrich Schütze","We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.",,,,ACL
183,2017,Morphological Inflection Generation with Hard Monotonic Attention,"Roee Aharoni, Yoav Goldberg","We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft (Bahdanau, 2014) attention models for the task, shedding some light on the features such models extract.",,,,ACL
184,2017,From Characters to Words to in Between: Do We Capture Morphology?,"Clara Vania, Adam Lopez","Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.",,,,ACL
185,2017,Riemannian Optimization for Skip-Gram Negative Sampling,"Alexander Fonarev, Oleksii Grinchuk, Gleb Gusev, Pavel Serdyukov","Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in “word2vec” software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.",,,,ACL
186,2017,Deep Multitask Learning for Semantic Dependency Parsing,"Hao Peng, Sam Thomson, Noah A. Smith","We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches—one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at https://github.com/Noahs-ARK/NeurboParser.",,,,ACL
187,2017,Improved Word Representation Learning with Sememes,"Yilin Niu, Ruobing Xie, Zhiyuan Liu, Maosong Sun","Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes. Since sememes are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases. In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks. The key idea is to utilize word sememes to capture exact meanings of a word within specific contexts accurately. More specifically, we follow the framework of Skip-gram and present three sememe-encoded models to learn representations of sememes, senses and words, where we apply the attention scheme to detect word senses in various contexts. We conduct experiments on two tasks including word similarity and word analogy, and our models significantly outperform baselines. The results indicate that WRL can benefit from sememes via the attention scheme, and also confirm our models being capable of correctly modeling sememe information.",,,,ACL
188,2017,Learning Character-level Compositionality with Visual Features,"Frederick Liu, Han Lu, Chieh Lo, Graham Neubig","Previous work has modeled the compositionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry topical content which resulting in embeddings that are coherent in visual space.",,,,ACL
189,2017,A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data,"Qiaolin Xia, Lei Sha, Baobao Chang, Zhifang Sui","Previous studies on Chinese semantic role labeling (SRL) have concentrated on a single semantically annotated corpus. But the training data of single corpus is often limited. Whereas the other existing semantically annotated corpora for Chinese SRL are scattered across different annotation frameworks. But still, Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In this paper, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that our model outperforms state-of-the-art methods.",,,,ACL
190,2017,Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings,"John Wieting, Kevin Gimpel","We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the Gated Recurrent Averaging Network, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.",,,,ACL
191,2017,Ontology-Aware Token Embeddings for Prepositional Phrase Attachment,"Pradeep Dasigi, Waleed Ammar, Chris Dyer, Eduard Hovy","Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.",,,,ACL
192,2017,Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition,"Ellie Pavlick, Marius Paşca","We present a method for populating fine-grained classes (e.g., “1950s American jazz musicians”) with instances (e.g., Charles Mingus ). While state-of-the-art methods tend to treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a >10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns.",,,,ACL
193,2017,"Parsing to 1-Endpoint-Crossing, Pagenumber-2 Graphs","Junjie Cao, Sheng Huang, Weiwei Sun, Xiaojun Wan","We study the Maximum Subgraph problem in deep dependency parsing. We consider two restrictions to deep dependency graphs: (a) 1-endpoint-crossing and (b) pagenumber-2. Our main contribution is an exact algorithm that obtains maximum subgraphs satisfying both restrictions simultaneously in time O(n5). Moreover, ignoring one linguistically-rare structure descreases the complexity to O(n4). We also extend our quartic-time algorithm into a practical parser with a discriminative disambiguation model and evaluate its performance on four linguistic data sets used in semantic dependency parsing.",,,,ACL
194,2017,Semi-supervised Multitask Learning for Sequence Labeling,Marek Rei,"We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.",,,,ACL
195,2017,Semantic Parsing of Pre-university Math Problems,"Takuya Matsuzaki, Takumi Ito, Hidenao Iwane, Hirokazu Anai",We have been developing an end-to-end math problem solving system that accepts natural language input. The current paper focuses on how we analyze the problem sentences to produce logical forms. We chose a hybrid approach combining a shallow syntactic analyzer and a manually-developed lexicalized grammar. A feature of the grammar is that it is extensively typed on the basis of a formal ontology for pre-university math. These types are helpful in semantic disambiguation inside and across sentences. Experimental results show that the hybrid system produces a well-formed logical form with 88% precision and 56% recall.,,,,ACL
1,2018,Probabilistic FastText for Multi-Sense Word Embeddings,"Ben Athiwaratkun, Andrew Wilson, Anima Anandkumar","We introduce Probabilistic FastText, a new model for word embeddings that can capture multiple word senses, sub-word structure, and uncertainty information. In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams. This representation allows the model to share the “strength” across sub-word structures (e.g. Latin roots), producing accurate representations of rare, misspelt, or even unseen words. Moreover, each component of the mixture can capture a different word sense. Probabilistic FastText outperforms both FastText, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets. We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings. Thus, our model is the first to achieve best of both the worlds: multi-sense representations while having enriched semantics on rare words.",,,,ACL
2,2018,A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors,"Mikhail Khodak, Nikunj Saunshi, Yingyu Liang, Tengyu Ma","Motivations like domain adaptation, transfer learning, and feature learning have fueled interest in inducing embeddings for rare or unseen words, n-grams, synsets, and other textual features. This paper introduces a la carte embedding, a simple and general alternative to the usual word2vec-based approaches for building such representations that is based upon recent theoretical results for GloVe-like embeddings. Our method relies mainly on a linear transformation that is efficiently learnable using pretrained word vectors and linear regression. This transform is applicable on the fly in the future when a new text feature or rare word is encountered, even if only a single usage example is available. We introduce a new dataset showing how the a la carte method requires fewer examples of words in context to learn high-quality embeddings and we obtain state-of-the-art results on a nonce task and some unsupervised document classification tasks.",,,,ACL
3,2018,Unsupervised Learning of Distributional Relation Vectors,"Shoaib Jameel, Zied Bouraoui, Steven Schockaert","Word embedding models such as GloVe rely on co-occurrence statistics to learn vector representations of word meaning. While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words, existing approaches for modeling such relationships are based on manipulating pre-trained word vectors. In this paper, we introduce a novel method which directly learns relation vectors from co-occurrence statistics. To this end, we first introduce a variant of GloVe, in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors. We then show how relation vectors can be naturally embedded into the resulting vector space.",,,,ACL
4,2018,Explicit Retrofitting of Distributional Word Vectors,"Goran Glavaš, Ivan Vulić","Semantic specialization of distributional word vectors, referred to as retrofitting, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation. Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints. In this work, in contrast, we transform external lexico-semantic relations into training examples which we use to learn an explicit retrofitting model (ER). The ER model allows us to learn a global specialization function and specialize the vectors of words unobserved in the training data as well. We report large gains over original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks − lexical simplification and dialog state tracking. Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces.",,,,ACL
5,2018,Unsupervised Neural Machine Translation with Weight Sharing,"Zhen Yang, Wei Chen, Feng Wang, Bo Xu","Unsupervised neural machine translation (NMT) is a recently proposed approach for machine translation which aims to train the model without using any labeled data. The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared-latent space, which is weak in keeping the unique and internal characteristics of each language, such as the style, terminology, and sentence structure. To address this issue, we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high-level representations of the input sentences. Besides, two different generative adversarial networks (GANs), namely the local GAN and global GAN, are proposed to enhance the cross-language translation. With this new approach, we achieve significant improvements on English-German, English-French and Chinese-to-English translation tasks.",,,,ACL
6,2018,Triangular Architecture for Rare Language Translation,"Shuo Ren, Wenhu Chen, Shujie Liu, Mu Li","Neural Machine Translation (NMT) performs poor on the low-resource language pair (X,Z), especially when Z is a rare language. By introducing another rich language Y, we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data (Y,Z) (may be small) and (X,Y) (can be rich) to improve the translation performance of low-resource pairs. In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with an unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X,Y). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.",,,,ACL
7,2018,Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates,Taku Kudo,"Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.",,,,ACL
8,2018,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,"Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson","The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT’14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",,,,ACL
9,2018,Ultra-Fine Entity Typing,"Eunsol Choi, Omer Levy, Yejin Choi, Luke Zettlemoyer","We introduce a new entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity. This formulation allows us to use a new type of distant supervision at large scale: head words, which indicate the type of the noun phrases they appear in. We show that these ultra-fine types can be crowd-sourced, and introduce new evaluation sets that are much more diverse and fine-grained than existing benchmarks. We present a model that can predict ultra-fine types, and is trained using a multitask objective that pools our new head-word supervision with prior supervision from entity linking. Experimental results demonstrate that our model is effective in predicting entity types at varying granularity; it achieves state of the art performance on an existing fine-grained entity typing benchmark, and sets baselines for our newly-introduced datasets.",,,,ACL
10,2018,Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking,"Shikhar Murty, Patrick Verga, Luke Vilnis, Irena Radovanovic","Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies. Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies. This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset. We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMentions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and TypeNet, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types. In experiments on all three datasets we show substantial gains from hierarchy-aware training.",,,,ACL
11,2018,Improving Knowledge Graph Embedding Using Simple Constraints,"Boyang Ding, Quan Wang, Bin Wang, Li Guo","Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/iieir-km/ComplEx-NNE_AER.",,,,ACL
12,2018,Towards Understanding the Geometry of Knowledge Graph Embeddings,"Chandrahas, Aditya Sharma, Partha Talukdar","Knowledge Graph (KG) embedding has emerged as a very active area of research over the last few years, resulting in the development of several embedding methods. These KG embedding methods represent KG entities and relations as vectors in a high-dimensional space. Despite this popularity and effectiveness of KG embeddings in various tasks (e.g., link prediction), geometric understanding of such embeddings (i.e., arrangement of entity and relation vectors in vector space) is unexplored – we fill this gap in the paper. We initiate a study to analyze the geometry of KG embeddings and correlate it with task performance and other hyperparameters. To the best of our knowledge, this is the first study of its kind. Through extensive experiments on real-world datasets, we discover several insights. For example, we find that there are sharp differences between the geometry of embeddings learnt by different classes of KG embeddings methods. We hope that this initial study will inspire other follow-up research on this important but unexplored problem.",,,,ACL
13,2018,A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss,"Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min","We propose a unified model combining the strength of extractive and abstractive summarization. On the one hand, a simple extractive model can obtain sentence-level attention with high ROUGE scores but less readable. On the other hand, a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph. In our model, sentence-level attention is used to modulate the word-level attention such that words in less attended sentences are less likely to be generated. Moreover, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions. By end-to-end training our model with the inconsistency loss and original losses of extractive and abstractive models, we achieve state-of-the-art ROUGE scores while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation.",,,,ACL
14,2018,Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks,"Aishwarya Jadhav, Vaibhav Rajan","We present a new neural sequence-to-sequence model for extractive summarization called SWAP-NET (Sentences and Words from Alternating Pointer Networks). Extractive summaries comprising a salient subset of input sentences, often also contain important key words. Guided by this principle, we design SWAP-NET that models the interaction of key words and salient sentences using a new two-level pointer network based architecture. SWAP-NET identifies both salient sentences and key words in an input document, and then combines them to form the extractive summary. Experiments on large scale benchmark corpora demonstrate the efficacy of SWAP-NET that outperforms state-of-the-art extractive summarizers.",,,,ACL
15,2018,"Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization","Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei","Most previous seq2seq summarization systems purely depend on the source text to generate summaries, which tends to work unstably. Inspired by the traditional template-based summarization approaches, this paper proposes to use existing summaries as soft templates to guide the seq2seq model. To this end, we use a popular IR platform to Retrieve proper summaries as candidate templates. Then, we extend the seq2seq framework to jointly conduct template Reranking and template-aware summary generation (Rewriting). Experiments show that, in terms of informativeness, our model significantly outperforms the state-of-the-art methods, and even soft templates themselves demonstrate high competitiveness. In addition, the import of high-quality external summaries improves the stability and readability of generated summaries.",,,,ACL
16,2018,Simple and Effective Text Simplification Using Semantic and Neural Methods,"Elior Sulem, Omri Abend, Ari Rappoport","Sentence splitting is a major simplification operator. Here we present a simple and efficient splitting algorithm based on an automatic semantic parser. After splitting, the text is amenable for further fine-tuned simplification operations. In particular, we show that neural Machine Translation can be effectively used in this situation. Previous application of Machine Translation for simplification suffers from a considerable disadvantage in that they are over-conservative, often failing to modify the source in any way. Splitting based on semantic parsing, as proposed here, alleviates this issue. Extensive automatic and human evaluation shows that the proposed method compares favorably to the state-of-the-art in combined lexical and structural simplification.",,,,ACL
17,2018,"Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20,000 English Words",Saif Mohammad,"Words play a central role in language and thought. Factor analysis studies have shown that the primary dimensions of meaning are valence, arousal, and dominance (VAD). We present the NRC VAD Lexicon, which has human ratings of valence, arousal, and dominance for more than 20,000 English words. We use Best–Worst Scaling to obtain fine-grained scores and address issues of annotation consistency that plague traditional rating scale methods of annotation. We show that the ratings obtained are vastly more reliable than those in existing lexicons. We also show that there exist statistically significant differences in the shared understanding of valence, arousal, and dominance across demographic variables such as age, gender, and personality.",,,,ACL
18,2018,Comprehensive Supersense Disambiguation of English Prepositions and Possessives,"Nathan Schneider, Jena D. Hwang, Vivek Srikumar, Jakob Prange","Semantic relations are often signaled with prepositional or possessive marking—but extreme polysemy bedevils their analysis and automatic interpretation. We introduce a new annotation scheme, corpus, and task for the disambiguation of prepositions and possessives in English. Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; use broadly applicable supersense classes rather than fine-grained dictionary definitions; unite prepositions and possessives under the same class inventory; and distinguish between a marker’s lexical contribution and the role it marks in the context of a predicate or scene. Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task.",,,,ACL
19,2018,"A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature","Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang","We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the ‘PICO’ elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.",,,,ACL
20,2018,Efficient Online Scalar Annotation with Bounded Support,"Keisuke Sakaguchi, Benjamin Van Durme","We describe a novel method for efficiently eliciting scalar annotations for dataset construction and system quality estimation by human judgments. We contrast direct assessment (annotators assign scores to items directly), online pairwise ranking aggregation (scores derive from annotator comparison of items), and a hybrid approach (EASL: Efficient Annotation of Scalar Labels) proposed here. Our proposal leads to increased correlation with ground truth, at far greater annotator efficiency, suggesting this strategy as an improved mechanism for dataset creation and manual system evaluation.",,,,ACL
21,2018,Neural Argument Generation Augmented with Externally Retrieved Evidence,"Xinyu Hua, Lu Wang","High quality arguments are essential elements for human reasoning and decision-making processes. However, effective argument construction is a challenging task for both human and machines. In this work, we study a novel task on automatically generating arguments of a different stance for a given statement. We propose an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia. Our model first generates a set of talking point phrases as intermediate representation, followed by a separate decoder producing the final argument based on both input and the keyphrases. Experiments on a large-scale dataset collected from Reddit show that our model constructs arguments with more topic-relevant content than popular sequence-to-sequence generation models according to automatic evaluation and human assessments.",,,,ACL
22,2018,A Stylometric Inquiry into Hyperpartisan and Fake News,"Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",,,,ACL
23,2018,Retrieval of the Best Counterargument without Prior Topic Knowledge,"Henning Wachsmuth, Shahbaz Syed, Benno Stein","Given any argument on any controversial topic, how to counter it? This question implies the challenging retrieval task of finding the best counterargument. Since prior knowledge of a topic cannot be expected in general, we hypothesize the best counterargument to invoke the same aspects as the argument while having the opposite stance. To operationalize our hypothesis, we simultaneously model the similarity and dissimilarity of pairs of arguments, based on the words and embeddings of the arguments’ premises and conclusions. A salient property of our model is its independence from the topic at hand, i.e., it applies to arbitrary arguments. We evaluate different model variations on millions of argument pairs derived from the web portal idebate.org. Systematic ranking experiments suggest that our hypothesis is true for many arguments: For 7.6 candidates with opposing stance on average, we rank the best counterargument highest with 60% accuracy. Even among all 2801 test set pairs as candidates, we still find the best one about every third time.",,,,ACL
24,2018,LinkNBed: Multi-Graph Representation Learning with Entity Linkage,"Rakshit Trivedi, Bunyamin Sisman, Xin Luna Dong, Christos Faloutsos","Knowledge graphs have emerged as an important model for studying complex multi-relational data. This has given rise to the construction of numerous large scale but incomplete knowledge graphs encoding information extracted from various resources. An effective and scalable approach to jointly learn over multiple graphs and eventually construct a unified graph is a crucial next step for the success of knowledge-based inference for many downstream applications. To this end, we propose LinkNBed, a deep relational learning framework that learns entity and relationship representations across multiple graphs. We identify entity linkage across graphs as a vital component to achieve our goal. We design a novel objective that leverage entity linkage and build an efficient multi-task training procedure. Experiments on link prediction and entity linkage demonstrate substantial improvements over the state-of-the-art relational learning approaches.",,,,ACL
25,2018,Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures,"Luke Vilnis, Xiang Li, Shikhar Murty, Andrew McCallum","Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations). Probabilistic extensions of OE have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anti-correlation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts, and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs, and investigate the power of the model.",,,,ACL
26,2018,Graph-to-Sequence Learning using Gated Graph Neural Networks,"Daniel Beck, Gholamreza Haffari, Trevor Cohn","Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architectures on graph-to-sequence obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work. Experimental results shows that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation.",,,,ACL
27,2018,"Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context","Urvashi Khandelwal, He He, Peng Qi, Dan Jurafsky","We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.",,,,ACL
28,2018,"Bridging CNNs, RNNs, and Weighted Finite-State Machines","Roy Schwartz, Sam Thomson, Noah A. Smith","Recurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a new model that aims to bridge these two approaches. SoPa combines neural representation learning with weighted finite-state automata (WFSAs) to learn a soft version of traditional surface patterns. We show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly, to a restricted form of WFSA. Empirically, on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.",,,,ACL
29,2018,Zero-shot Learning of Classifiers from Natural Language Quantification,"Shashank Srivastava, Igor Labutov, Tom Mitchell","Humans can efficiently learn new concepts using language. We present a framework through which a set of explanations of a concept can be used to learn a classifier without access to any labeled examples. We use semantic parsing to map explanations to probabilistic assertions grounded in latent class labels and observed attributes of unlabeled data, and leverage the differential semantics of linguistic quantifiers (e.g., ‘usually’ vs ‘always’) to drive model training. Experiments on three domains show that the learned classifiers outperform previous approaches for learning with limited data, and are comparable with fully supervised classifiers trained from a small number of labeled examples.",,,,ACL
30,2018,Sentence-State LSTM for Text Representation,"Yue Zhang, Qi Liu, Linfeng Song","Bi-directional LSTMs are a powerful tool for text representation. On the other hand, they have been shown to suffer various limitations due to their sequential nature. We investigate an alternative LSTM structure for encoding text, which consists of a parallel state for each word. Recurrent steps are used to perform local and global information exchange between words simultaneously, rather than incremental reading of a sequence of words. Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers.",,,,ACL
31,2018,Universal Language Model Fine-tuning for Text Classification,"Jeremy Howard, Sebastian Ruder","Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",,,,ACL
32,2018,Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement,"Nina Poerner, Hinrich Schütze, Benjamin Roth","The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.",,,,ACL
33,2018,Improving Text-to-SQL Evaluation Methodology,"Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",,,,ACL
34,2018,Semantic Parsing with Syntax- and Table-Aware SQL Generation,"Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji","We present a generative model to map natural language questions into SQL queries. Existing neural network based approaches typically generate a SQL query word-by-word, however, a large portion of the generated results is incorrect or not executable due to the mismatch between question words and table contents. Our approach addresses this problem by considering the structure of table and the syntax of SQL language. The quality of the generated SQL query is significantly improved through (1) learning to replicate content from column names, cells or SQL keywords; and (2) improving the generation of WHERE clause by leveraging the column-cell relation. Experiments are conducted on WikiSQL, a recently released dataset with the largest question- SQL pairs. Our approach significantly improves the state-of-the-art execution accuracy from 69.0% to 74.4%.",,,,ACL
35,2018,Multitask Parsing Across Semantic Representations,"Daniel Hershcovich, Omri Abend, Ari Rappoport","The ability to consolidate information of different types is at the core of intelligence, and has tremendous practical value in allowing learning for one task to benefit from generalizations learned for others. In this paper we tackle the challenging task of improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings.",,,,ACL
36,2018,Character-Level Models versus Morphology in Semantic Role Labeling,"Gözde Gül Şahin, Mark Steedman","Character-level models have become a popular approach specially for their accessibility and ability to handle unseen data. However, little is known on their ability to reveal the underlying morphological structure of a word, which is a crucial skill for high-level semantic analysis tasks, such as semantic role labeling (SRL). In this work, we train various types of SRL models that use word, character and morphology level information and analyze how performance of characters compare to words and morphology for several languages. We conduct an in-depth error analysis for each morphological typology and analyze the strengths and limitations of character-level models that relate to out-of-domain data, training data size, long range dependencies and model complexity. Our exhaustive analyses shed light on important characteristics of character-level models and their semantic capability.",,,,ACL
37,2018,AMR Parsing as Graph Prediction with Latent Alignment,"Chunchuan Lyu, Ivan Titov","Abstract meaning representations (AMRs) are broad-coverage sentence-level semantic representations. AMRs represent sentences as rooted labeled directed acyclic graphs. AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the graphs and words in the corresponding sentences. We introduce a neural parser which treats alignments as latent variables within a joint probabilistic model of concepts, relations and alignments. As exact inference requires marginalizing over alignments and is infeasible, we use the variational autoencoding framework and a continuous relaxation of the discrete alignments. We show that joint modeling is preferable to using a pipeline of align and parse. The parser achieves the best reported results on the standard benchmark (74.4% on LDC2016E25).",,,,ACL
38,2018,Accurate SHRG-Based Semantic Parsing,"Yufei Chen, Weiwei Sun, Xiaojun Wan","We demonstrate that an SHRG-based parser can produce semantic graphs much more accurately than previously shown, by relating synchronous production rules to the syntacto-semantic composition process. Our parser achieves an accuracy of 90.35 for EDS (89.51 for DMRS) in terms of elementary dependency match, which is a 4.87 (5.45) point improvement over the best existing data-driven model, indicating, in our view, the importance of linguistically-informed derivation for data-driven semantic parsing. This accuracy is equivalent to that of English Resource Grammar guided models, suggesting that (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations.",,,,ACL
39,2018,Using Intermediate Representations to Solve Math Word Problems,"Danqing Huang, Jin-Ge Yao, Chin-Yew Lin, Qingyu Zhou","To solve math word problems, previous statistical approaches attempt at learning a direct mapping from a problem description to its corresponding equation system. However, such mappings do not include the information of a few higher-order operations that cannot be explicitly represented in equations but are required to solve the problem. The gap between natural language and equations makes it difficult for a learned model to generalize from limited data. In this work we present an intermediate meaning representation scheme that tries to reduce this gap. We use a sequence-to-sequence model with a novel attention regularization term to generate the intermediate forms, then execute them to obtain the final answers. Since the intermediate forms are latent, we propose an iterative labeling framework for learning by leveraging supervision signals from both equations and answers. Our experiments show using intermediate forms outperforms directly predicting equations.",,,,ACL
40,2018,Discourse Representation Structure Parsing,"Jiangming Liu, Shay B. Cohen, Mirella Lapata","We introduce an open-domain neural semantic parser which generates formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). We propose a method which transforms Discourse Representation Structures (DRSs) to trees and develop a structure-aware model which decomposes the decoding process into three stages: basic DRS structure prediction, condition prediction (i.e., predicates and relations), and referent prediction (i.e., variables). Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin.",,,,ACL
41,2018,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,"Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min","Many deep learning architectures have been proposed to model the compositionality in text sequences, requiring substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (i) a max-pooling operation for improved interpretability; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (i) (long) document classification; (ii) text sequence matching; and (iii) short text tasks, including classification and tagging.",,,,ACL
42,2018,ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations,"John Wieting, Kevin Gimpel","We describe ParaNMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following Wieting et al. (2017). Our hope is that ParaNMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use ParaNMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.",,,,ACL
43,2018,"Event2Mind: Commonsense Inference on Events, Intents, and Reactions","Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A. Smith","We investigate a new commonsense inference task: given an event described in a short free-form text (“X drinks coffee in the morning”), a system reasons about the likely intents (“X wants to stay awake”) and reactions (“X feels alert”) of the event’s participants. To support this study, we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. We report baseline performance on this task, demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants. In addition, we demonstrate how commonsense inference on people’s intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts.",,,,ACL
44,2018,Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis,"Shuhei Kurita, Daisuke Kawahara, Sadao Kurohashi","Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult. To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with PAS. However, since it is prohibitively expensive, it is promising to take advantage of a large amount of raw corpora. In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a raw corpus. In our experiments, our model outperforms existing state-of-the-art models for Japanese PAS analysis.",,,,ACL
45,2018,Improving Event Coreference Resolution by Modeling Correlations between Event Coreference Chains and Document Topic Structures,"Prafulla Kumar Choubey, Ruihong Huang","This paper proposes a novel approach for event coreference resolution that models correlations between event coreference chains and document topical structures through an Integer Linear Programming formulation. We explicitly model correlations between the main event chains of a document with topic transition sentences, inter-coreference chain correlations, event mention distributional characteristics and sub-event structure, and use them with scores obtained from a local coreference relation classifier for jointly resolving multiple event chains in a document. Our experiments across KBP 2016 and 2017 datasets suggest that each of the structures contribute to improving event coreference resolution performance.",,,,ACL
46,2018,DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction,"Pengda Qin, Weiran Xu, William Yang Wang","Distant supervision can effectively label data for relation extraction, but suffers from the noise labeling problem. Recent works mainly perform soft bag-level noise reduction strategies to find the relatively better samples in a sentence bag, which is suboptimal compared with making a hard decision of false positive samples in sentence level. In this paper, we introduce an adversarial learning framework, which we named DSGAN, to learn a sentence-level true-positive generator. Inspired by Generative Adversarial Networks, we regard the positive samples generated by the generator as the negative samples to train the discriminator. The optimal generator is obtained until the discrimination ability of the discriminator has the greatest decline. We adopt the generator to filter distant supervision training dataset and redistribute the false positive instances into the negative set, in which way to provide a cleaned dataset for relation classification. The experimental results show that the proposed strategy significantly improves the performance of distant supervision relation extraction comparing to state-of-the-art systems.",,,,ACL
47,2018,Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism,"Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu","The relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. We divided the sentences into three types according to triplet overlap degree, including Normal, EntityPairOverlap and SingleEntiyOverlap. Existing methods mainly focus on Normal class and fail to extract relational triplets precisely. In this paper, we propose an end-to-end model based on sequence-to-sequence learning with copy mechanism, which can jointly extract relational facts from sentences of any of these classes. We adopt two different strategies in decoding process: employing only one united decoder or applying multiple separated decoders. We test our models in two public datasets and our model outperform the baseline method significantly.",,,,ACL
48,2018,Self-regulation: Employing a Generative Adversarial Network to Improve Event Detection,"Yu Hong, Wenxuan Zhou, Jingli Zhang, Guodong Zhou","Due to the ability of encoding and mapping semantic information into a high-dimensional latent feature space, neural networks have been successfully used for detecting events to a certain extent. However, such a feature space can be easily contaminated by spurious features inherent in event detection. In this paper, we propose a self-regulated learning approach by utilizing a generative adversarial network to generate spurious features. On the basis, we employ a recurrent network to eliminate the fakes. Detailed experiments on the ACE 2005 and TAC-KBP 2015 corpora show that our proposed method is highly effective and adaptable.",,,,ACL
49,2018,Context-Aware Neural Model for Temporal Information Extraction,"Yuanliang Meng, Anna Rumshisky","We propose a context-aware neural network model for temporal information extraction. This model has a uniform architecture for event-event, event-timex and timex-timex pairs. A Global Context Layer (GCL), inspired by Neural Turing Machine (NTM), stores processed temporal relations in narrative order, and retrieves them for use when relevant entities come in. Relations are then classified in context. The GCL model has long-term memory and attention mechanisms to resolve irregular long-distance dependencies that regular RNNs such as LSTM cannot recognize. It does not require any new input features, while outperforming the existing models in literature. To our knowledge it is also the first model to use NTM-like architecture to process the information from global context in discourse-scale natural text processing. We are going to release the source code in the future.",,,,ACL
50,2018,Temporal Event Knowledge Acquisition via Identifying Narratives,"Wenlin Yao, Ruihong Huang","Inspired by the double temporality characteristic of narrative texts, we propose a novel approach for acquiring rich temporal “before/after” event knowledge across sentences in narrative stories. The double temporality states that a narrative story often describes a sequence of events following the chronological order and therefore, the temporal order of events matches with their textual order. We explored narratology principles and built a weakly supervised approach that identifies 287k narrative paragraphs from three large corpora. We then extracted rich temporal event knowledge from these narrative paragraphs. Such event knowledge is shown useful to improve temporal relation classification and outperforms several recent neural network models on the narrative cloze task.",,,,ACL
51,2018,Textual Deconvolution Saliency (TDS) : a deep tool box for linguistic analysis,"Laurent Vanni, Melanie Ducoffe, Carlos Aguilar, Frederic Precioso","In this paper, we propose a new strategy, called Text Deconvolution Saliency (TDS), to visualize linguistic information detected by a CNN for text classification. We extend Deconvolution Networks to text in order to present a new perspective on text analysis to the linguistic community. We empirically demonstrated the efficiency of our Text Deconvolution Saliency on corpora from three different languages: English, French, and Latin. For every tested dataset, our Text Deconvolution Saliency automatically encodes complex linguistic patterns based on co-occurrences and possibly on grammatical and syntax analysis.",,,,ACL
52,2018,Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach,"Shafiq Joty, Muhammad Tasnim Mohiuddin, Dat Tien Nguyen","We propose a novel coherence model for written asynchronous conversations (e.g., forums, emails), and show its applications in coherence assessment and thread reconstruction tasks. We conduct our research in two steps. First, we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions. Then, we extend the model to asynchronous conversations by incorporating the underlying conversational structure in the entity grid representation and feature computation. Our model achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing models. We also demonstrate its effectiveness in reconstructing thread structures.",,,,ACL
53,2018,Deep Reinforcement Learning for Chinese Zero Pronoun Resolution,"Qingyu Yin, Yu Zhang, Wei-Nan Zhang, Ting Liu","Recent neural network models for Chinese zero pronoun resolution gain great performance by capturing semantic information for zero pronouns and candidate antecedents, but tend to be short-sighted, operating solely by making local decisions. They typically predict coreference links between the zero pronoun and one single candidate antecedent at a time while ignoring their influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is crucial for classifying later zero pronoun-candidate antecedent pairs, a need which leads traditional models of zero pronoun resolution to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to deal with the task. With the help of the reinforcement learning agent, our system learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 show that our approach substantially outperforms the state-of-the-art methods under three experimental settings.",,,,ACL
54,2018,Entity-Centric Joint Modeling of Japanese Coreference Resolution and Predicate Argument Structure Analysis,"Tomohide Shibata, Sadao Kurohashi","Predicate argument structure analysis is a task of identifying structured events. To improve this field, we need to identify a salient entity, which cannot be identified without performing coreference resolution and predicate argument structure analysis simultaneously. This paper presents an entity-centric joint model for Japanese coreference resolution and predicate argument structure analysis. Each entity is assigned an embedding, and when the result of both analyses refers to an entity, the entity embedding is updated. The analyses take the entity embedding into consideration to access the global information of entities. Our experimental results demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically, which is a notoriously difficult task in predicate argument structure analysis.",,,,ACL
55,2018,"Constraining MGbank: Agreement, L-Selection and Supertagging in Minimalist Grammars",John Torr,"This paper reports on two strategies that have been implemented for improving the efficiency and precision of wide-coverage Minimalist Grammar (MG) parsing. The first extends the formalism presented in Torr and Stabler (2016) with a mechanism for enforcing fine-grained selectional restrictions and agreements. The second is a method for factoring computationally costly null heads out from bottom-up MG parsing; this has the additional benefit of rendering the formalism fully compatible for the first time with highly efficient Markovian supertaggers. These techniques aided in the task of generating MGbank, the first wide-coverage corpus of Minimalist Grammar derivation trees.",,,,ACL
56,2018,Not that much power: Linguistic alignment is influenced more by low-level linguistic features rather than social power,"Yang Xu, Jeremy Cole, David Reitter","Linguistic alignment between dialogue partners has been claimed to be affected by their relative social power. A common finding has been that interlocutors of higher power tend to receive more alignment than those of lower power. However, these studies overlook some low-level linguistic features that can also affect alignment, which casts doubts on these findings. This work characterizes the effect of power on alignment with logistic regression models in two datasets, finding that the effect vanishes or is reversed after controlling for low-level features such as utterance length. Thus, linguistic alignment is explained better by low-level features than by social power. We argue that a wider range of factors, especially cognitive factors, need to be taken into account for future studies on observational data when social factors of language use are in question.",,,,ACL
57,2018,"TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation","Alexander Fabbri, Irene Li, Prawat Trairatvorakul, Yijiao He","The field of Natural Language Processing (NLP) is growing rapidly, with new research published daily along with an abundance of tutorials, codebases and other online resources. In order to learn this dynamic field or stay up-to-date on the latest research, students as well as educators and researchers must constantly sift through multiple sources to find valuable, relevant information. To address this situation, we introduce TutorialBank, a new, publicly available dataset which aims to facilitate NLP education and research. We have manually collected and categorized over 5,600 resources on NLP as well as the related fields of Artificial Intelligence (AI), Machine Learning (ML) and Information Retrieval (IR). Our dataset is notably the largest manually-picked corpus of resources intended for NLP education which does not include only academic papers. Additionally, we have created both a search engine and a command-line tool for the resources and have annotated the corpus to include lists of research topics, relevant resources for each topic, prerequisite relations among topics, relevant sub-parts of individual resources, among other annotations. We are releasing the dataset and present several avenues for further research.",,,,ACL
58,2018,Give Me More Feedback: Annotating Argument Persuasiveness and Related Attributes in Student Essays,"Winston Carlile, Nishant Gurrapadi, Zixuan Ke, Vincent Ng","While argument persuasiveness is one of the most important dimensions of argumentative essay quality, it is relatively little studied in automated essay scoring research. Progress on scoring argument persuasiveness is hindered in part by the scarcity of annotated corpora. We present the first corpus of essays that are simultaneously annotated with argument components, argument persuasiveness scores, and attributes of argument components that impact an argument’s persuasiveness. This corpus could trigger the development of novel computational models concerning argument persuasiveness that provide useful feedback to students on why their arguments are (un)persuasive in addition to how persuasive they are.",,,,ACL
59,2018,Inherent Biases in Reference-based Evaluation for Grammatical Error Correction,"Leshem Choshen, Omri Abend","The prevalent use of too few references for evaluating text-to-text generation is known to bias estimates of their quality (henceforth, low coverage bias or LCB). This paper shows that overcoming LCB in Grammatical Error Correction (GEC) evaluation cannot be attained by re-scaling or by increasing the number of references in any feasible range, contrary to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims.",,,,ACL
60,2018,The price of debiasing automatic metrics in natural language evalaution,"Arun Chaganty, Stephen Mussmann, Percy Liang","For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements. On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive. In this paper, we use control variates to combine automatic metrics with human evaluation to obtain an unbiased estimator with lower cost than human evaluation alone. In practice, however, we obtain only a 7-13% cost reduction on evaluating summarization and open-response question answering systems. We then prove that our estimator is optimal: there is no unbiased estimator with lower cost. Our theory further highlights the two fundamental bottlenecks—the automatic metric and the prompt shown to human evaluators—both of which need to be improved to obtain greater cost savings.",,,,ACL
61,2018,Neural Document Summarization by Jointly Learning to Score and Select Sentences,"Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang","Sentence scoring and sentence selection are two main steps in extractive document summarization systems. However, previous works treat them as two separated subtasks. In this paper, we present a novel end-to-end neural network framework for extractive document summarization by jointly learning to score and select sentences. It first reads the document sentences with a hierarchical encoder to obtain the representation of sentences. Then it builds the output summary by extracting sentences one by one. Different from previous methods, our approach integrates the selection strategy into the scoring model, which directly predicts the relative importance given previously selected sentences. Experiments on the CNN/Daily Mail dataset show that the proposed framework significantly outperforms the state-of-the-art extractive summarization models.",,,,ACL
62,2018,Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization,"Guokan Shang, Wensi Ding, Zekun Zhang, Antoine Tixier","We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our work combines the strengths of multiple recent approaches while addressing their weaknesses. Moreover, we leverage recent advances in word embeddings and graph degeneracy applied to NLP to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the AMI and ICSI corpus show that our system improves on the state-of-the-art. Code and data are publicly available, and our system can be interactively tested.",,,,ACL
63,2018,Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting,"Yen-Chun Chen, Mohit Bansal","Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (i.e., compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) on the CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores. Moreover, by first operating at the sentence-level and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC-2002 dataset, where we achieve higher scores than a state-of-the-art model.",,,,ACL
64,2018,Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation,"Han Guo, Ramakanth Pasunuru, Mohit Bansal","An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also propose novel multi-task architectures with high-level (semantic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution). Overall, we achieve statistically significant improvements over the state-of-the-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also present several quantitative and qualitative analysis studies of our model’s learned saliency and entailment skills.",,,,ACL
65,2018,Modeling and Prediction of Online Product Review Helpfulness: A Survey,"Gerardo Ocampo Diaz, Vincent Ng","As the amount of free-form user-generated reviews in e-commerce websites continues to increase, there is an increasing need for automatic mechanisms that sift through the vast amounts of user reviews and identify quality content. Review helpfulness modeling is a task which studies the mechanisms that affect review helpfulness and attempts to accurately predict it. This paper provides an overview of the most relevant work in helpfulness prediction and understanding in the past decade, discusses the insights gained from said work, and provides guidelines for future research.",,,,ACL
66,2018,Mining Cross-Cultural Differences and Similarities in Social Media,"Bill Yuchen Lin, Frank F. Xu, Kenny Zhu, Seung-won Hwang","Cross-cultural differences and similarities are common in cross-lingual natural language understanding, especially for research in social media. For instance, people of distinct cultures often hold different opinions on a single named entity. Also, understanding slang terms across languages requires knowledge of cross-cultural similarities. In this paper, we study the problem of computing such cross-cultural differences and similarities. We present a lightweight yet effective approach, and evaluate it on two novel tasks: 1) mining cross-cultural differences of named entities and 2) finding similar terms for slang across languages. Experimental results show that our framework substantially outperforms a number of baseline methods on both tasks. The framework could be useful for machine translation applications and research in computational social science.",,,,ACL
67,2018,Classification of Moral Foundations in Microblog Political Discourse,"Kristen Johnson, Dan Goldwasser","Previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. Additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. Based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on Twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. The contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.",,,,ACL
68,2018,Coarse-to-Fine Decoding for Neural Semantic Parsing,"Li Dong, Mirella Lapata","Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.",,,,ACL
69,2018,Confidence Modeling for Neural Semantic Parsing,"Li Dong, Chris Quirk, Mirella Lapata","In this work we focus on confidence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various metrics to quantify these factors. These metrics are then used to estimate confidence scores that indicate whether model predictions are likely to be correct. Beyond confidence estimation, we identify which parts of the input contribute to uncertain predictions allowing users to interpret their model, and verify or refine its input. Experimental results show that our confidence model significantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores.",,,,ACL
70,2018,StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing,"Pengcheng Yin, Chunting Zhou, Junxian He, Graham Neubig","Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures. Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. We introduce StructVAE, a variational auto-encoding model for semi-supervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances. StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models.",,,,ACL
71,2018,Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing,"Bo Chen, Le Sun, Xianpei Han","This paper proposes a neural semantic parsing approach – Sequence-to-Action, which models semantic parsing as an end-to-end semantic graph generation process. Our method simultaneously leverages the advantages from two recent promising directions of semantic parsing. Firstly, our model uses a semantic graph to represent the meaning of a sentence, which has a tight-coupling with knowledge bases. Secondly, by leveraging the powerful representation learning and prediction ability of neural network models, we propose a RNN model which can effectively map sentences to action sequences for semantic graph generation. Experiments show that our method achieves state-of-the-art performance on Overnight dataset and gets competitive performance on Geo and Atis datasets.",,,,ACL
72,2018,On the Limitations of Unsupervised Bilingual Dictionary Induction,"Anders Søgaard, Sebastian Ruder, Ivan Vulić","Unsupervised machine translation - i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora - seems impossible, but nevertheless, Lample et al. (2017) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised cross-lingual word embedding technique for bilingual dictionary induction (Conneau et al., 2017), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.",,,,ACL
73,2018,A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings,"Mikel Artetxe, Gorka Labaka, Eneko Agirre","Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap.",,,,ACL
74,2018,A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling,"Ying Lin, Shengqi Yang, Veselin Stoyanov, Heng Ji","We propose a multi-lingual multi-task architecture to develop supervised models with a minimal amount of labeled data for sequence labeling. In this new architecture, we combine various transfer models using two layers of parameter sharing. On the first layer, we construct the basis of the architecture to provide universal word representation and feature extraction capability for all models. On the second level, we adopt different parameter sharing strategies for different transfer schemes. This architecture proves to be particularly effective for low-resource settings, when there are less than 200 training sentences for the target task. Using Name Tagging as a target task, our approach achieved 4.3%-50.5% absolute F-score gains compared to the mono-lingual single-task baseline model.",,,,ACL
75,2018,Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable,"Viktor Hangya, Fabienne Braune, Alexander Fraser, Hinrich Schütze","Bilingual tasks, such as bilingual lexicon induction and cross-lingual classification, are crucial for overcoming data sparsity in the target language. Resources required for such tasks are often out-of-domain, thus domain adaptation is an important problem here. We make two contributions. First, we test a delightfully simple method for domain adaptation of bilingual word embeddings. We evaluate these embeddings on two bilingual tasks involving different domains: cross-lingual twitter sentiment classification and medical bilingual lexicon induction. Second, we tailor a broadly applicable semi-supervised classification method from computer vision to these tasks. We show that this method also helps in low-resource setups. Using both methods together we achieve large improvements over our baselines, by using only additional unlabeled data.",,,,ACL
76,2018,Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge,"Todor Mihaylov, Anette Frank","We introduce a neural reading comprehension model that integrates external commonsense knowledge, encoded as a key-value memory, in a cloze-style setting. Instead of relying only on document-to-question interaction or discrete features as in prior work, our model attends to relevant external knowledge and combines this knowledge with the context representation before inferring the answer. This allows the model to attract and imply knowledge from an external knowledge source that is not explicitly stated in the text, but that is relevant for inferring the answer. Our model improves results over a very strong baseline on a hard Common Nouns dataset, making it a strong competitor of much more complex models. By including knowledge explicitly, our model can also provide evidence about the background knowledge used in the RC process.",,,,ACL
77,2018,Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds,"Igor Labutov, Bishan Yang, Anusha Prakash, Amos Azaria","Question Answering (QA), as a research field, has primarily focused on either knowledge bases (KBs) or free text as a source of knowledge. These two sources have historically shaped the kinds of questions that are asked over these sources, and the methods developed to answer them. In this work, we look towards a practical use-case of QA over user-instructed knowledge that uniquely combines elements of both structured QA over knowledge bases, and unstructured QA over narrative, introducing the task of multi-relational QA over personal narrative. As a first step towards this goal, we make three key contributions: (i) we generate and release TextWorldsQA, a set of five diverse datasets, where each dataset contains dynamic narrative that describes entities and relations in a simulated world, paired with variably compositional questions over that knowledge, (ii) we perform a thorough evaluation and analysis of several state-of-the-art QA models and their variants at this task, and (iii) we release a lightweight Python-based framework we call TextWorlds for easily generating arbitrary additional worlds and narrative, with the goal of allowing the community to create and share a growing collection of diverse worlds as a test-bed for this task.",,,,ACL
78,2018,Simple and Effective Multi-Paragraph Reading Comprehension,"Christopher Clark, Matt Gardner","We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.",,,,ACL
79,2018,Semantically Equivalent Adversarial Rules for Debugging NLP models,"Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin","Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) – semantic-preserving perturbations that induce changes in the model’s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",,,,ACL
80,2018,Style Transfer Through Back-Translation,"Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, Alan W Black","Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new method for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations: sentiment, gender and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency.",,,,ACL
81,2018,Generating Fine-Grained Open Vocabulary Entity Type Descriptions,"Rajarshi Bhowmik, Gerard de Melo","While large-scale knowledge graphs provide vast amounts of structured facts about entities, a short textual description can often be useful to succinctly characterize an entity and its type. Unfortunately, many knowledge graphs entities lack such textual descriptions. In this paper, we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words. We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines.",,,,ACL
82,2018,Hierarchical Neural Story Generation,"Angela Fan, Mike Lewis, Yann Dauphin","We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",,,,ACL
83,2018,No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling,"Xin Wang, Wenhu Chen, Yuan-Fang Wang, William Yang Wang","Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.",,,,ACL
84,2018,Bridging Languages through Images with Deep Partial Canonical Correlation Analysis,"Guy Rotman, Ivan Vulić, Roi Reichart","We present a deep neural network that leverages images to improve bilingual text embeddings. Relying on bilingual image tags and descriptions, our approach conditions text embedding induction on the shared visual information for both languages, producing highly correlated bilingual embeddings. In particular, we propose a novel model based on Partial Canonical Correlation Analysis (PCCA). While the original PCCA finds linear projections of two views in order to maximize their canonical correlation conditioned on a shared third variable, we introduce a non-linear Deep PCCA (DPCCA) model, and develop a new stochastic iterative algorithm for its optimization. We evaluate PCCA and DPCCA on multilingual word similarity and cross-lingual image description retrieval. Our models outperform a large variety of previous methods, despite not having access to any visual signal during test time inference.",,,,ACL
85,2018,Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search,"Jamie Kiros, William Chan, Geoffrey Hinton","We introduce Picturebook, a large-scale lookup operation to ground language via ‘snapshots’ of our physical world accessed through image search. For each word in a vocabulary, we extract the top-k images from Google image search and feed the images through a convolutional network to extract a word embedding. We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations. We also introduce Inverse Picturebook, a mechanism to map a Picturebook embedding back into words. We experiment and report results across a wide range of tasks: word similarity, natural language inference, semantic relatedness, sentiment/topic classification, image-sentence ranking and machine translation. We also show that gate activations corresponding to Picturebook embeddings are highly correlated to human judgments of concreteness ratings.",,,,ACL
86,2018,What Action Causes This? Towards Naive Physical Action-Effect Prediction,"Qiaozi Gao, Shaohua Yang, Joyce Chai, Lucy Vanderwende","Despite recent advances in knowledge representation, automated reasoning, and machine learning, artificial agents still lack the ability to understand basic action-effect relations regarding the physical world, for example, the action of cutting a cucumber most likely leads to the state where the cucumber is broken apart into smaller pieces. If artificial agents (e.g., robots) ever become our partners in joint tasks, it is critical to empower them with such action-effect understanding so that they can reason about the state of the world and plan for actions. Towards this goal, this paper introduces a new task on naive physical action-effect prediction, which addresses the relations between concrete actions (expressed in the form of verb-noun pairs) and their effects on the state of the physical world as depicted by images. We collected a dataset for this task and developed an approach that harnesses web image data through distant supervision to facilitate learning for action-effect prediction. Our empirical results have shown that web data can be used to complement a small number of seed examples (e.g., three examples for each action) for model learning. This opens up possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans with a few examples.",,,,ACL
87,2018,Transformation Networks for Target-Oriented Sentiment Classification,"Xin Li, Lidong Bing, Wai Lam, Bei Shi","Target-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence. RNN with attention seems a good fit for the characteristics of this task, and indeed it achieves the state-of-the-art performance. After re-examining the drawbacks of attention mechanism and the obstacles that block CNN to perform well in this classification task, we propose a new model that achieves new state-of-the-art results on a few benchmarks. Instead of attention, our model employs a CNN layer to extract salient features from the transformed word representations originated from a bi-directional RNN layer. Between the two layers, we propose a component which first generates target-specific representations of words in the sentence, and then incorporates a mechanism for preserving the original contextual information from the RNN layer.",,,,ACL
88,2018,Target-Sensitive Memory Networks for Aspect Sentiment Classification,"Shuai Wang, Sahisnu Mazumder, Bing Liu, Mianwei Zhou","Aspect sentiment classification (ASC) is a fundamental task in sentiment analysis. Given an aspect/target and a sentence, the task classifies the sentiment polarity expressed on the target in the sentence. Memory networks (MNs) have been used for this task recently and have achieved state-of-the-art results. In MNs, attention mechanism plays a crucial role in detecting the sentiment context for the given target. However, we found an important problem with the current MNs in performing the ASC task. Simply improving the attention mechanism will not solve it. The problem is referred to as target-sensitive sentiment, which means that the sentiment polarity of the (detected) context is dependent on the given target and it cannot be inferred from the context alone. To tackle this problem, we propose the target-sensitive memory networks (TMNs). Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated.",,,,ACL
89,2018,Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification,"Raksha Sharma, Pushpak Bhattacharyya, Sandipan Dandapat, Himanshu Sharad Bhatt","Getting manually labeled data in each domain is always an expensive and a time consuming task. Cross-domain sentiment analysis has emerged as a demanding concept where a labeled source domain facilitates a sentiment classifier for an unlabeled target domain. However, polarity orientation (positive or negative) and the significance of a word to express an opinion often differ from one domain to another domain. Owing to these differences, cross-domain sentiment classification is still a challenging task. In this paper, we propose that words that do not change their polarity and significance represent the transferable (usable) information across domains for cross-domain sentiment classification. We present a novel approach based on χ2 test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains. Furthermore, we show that a weighted ensemble of the classifiers enhances the cross-domain classification performance.",,,,ACL
90,2018,Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach,"Jingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang","The goal of sentiment-to-sentiment “translation” is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of parallel data. To solve this problem, we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutralization module and an emotionalization module. We evaluate our approach on two review datasets, Yelp and Amazon. Experimental results show that our approach significantly outperforms the state-of-the-art systems. Especially, the proposed method substantially improves the content preservation performance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets, respectively.",,,,ACL
91,2018,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,"Boyuan Pan, Yazheng Yang, Zhou Zhao, Yueting Zhuang","Natural Language Inference (NLI), also known as Recognizing Textual Entailment (RTE), is one of the most important problems in natural language processing. It requires to infer the logical relationship between two given sentences. While current approaches mostly focus on the interaction architectures of the sentences, in this paper, we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model. We observe that people usually use some discourse markers such as “so” or “but” to represent the logical relationship between two sentences. These words potentially have deep connections with the meanings of the sentences, thus can be utilized to help improve the representations of them. Moreover, we use reinforcement learning to optimize a new objective function with a reward defined by the property of the NLI datasets to make full use of the labels information. Experiments show that our method achieves the state-of-the-art performance on several large-scale datasets.",,,,ACL
92,2018,Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module,"Juan Pavez, Héctor Allende, Héctor Allende-Cid","During the last years, there has been a lot of interest in achieving some kind of complex reasoning using deep neural networks. To do that, models like Memory Networks (MemNNs) have combined external memory storages and attention mechanisms. These architectures, however, lack of more complex reasoning mechanisms that could allow, for instance, relational reasoning. Relation Networks (RNs), on the other hand, have shown outstanding results in relational reasoning tasks. Unfortunately, their computational cost grows quadratically with the number of memories, something prohibitive for larger problems. To solve these issues, we introduce the Working Memory Network, a MemNN architecture with a novel working memory storage and reasoning module. Our model retains the relational reasoning abilities of the RN while reducing its computational complexity from quadratic to linear. We tested our model on the text QA dataset bAbI and the visual QA dataset NLVR. In the jointly trained bAbI-10k, we set a new state-of-the-art, achieving a mean error of less than 0.5%. Moreover, a simple ensemble of two of our models solves all 20 tasks in the joint version of the benchmark.",,,,ACL
93,2018,Reasoning with Sarcasm by Reading In-Between,"Yi Tay, Anh Tuan Luu, Siu Cheung Hui, Jian Su","Sarcasm is a sophisticated speech act which commonly manifests on social communities such as Twitter and Reddit. The prevalence of sarcasm on the social web is highly disruptive to opinion mining systems due to not only its tendency of polarity flipping but also usage of figurative language. Sarcasm commonly manifests with a contrastive theme either between positive-negative sentiments or between literal-figurative scenarios. In this paper, we revisit the notion of modeling contrast in order to reason with sarcasm. More specifically, we propose an attention-based neural model that looks in-between instead of across, enabling it to explicitly model contrast and incongruity. We conduct extensive experiments on six benchmark datasets from Twitter, Reddit and the Internet Argument Corpus. Our proposed model not only achieves state-of-the-art performance on all datasets but also enjoys improved interpretability.",,,,ACL
94,2018,Adversarial Contrastive Estimation,"Avishek Joey Bose, Huan Ling, Yanshuai Cao","Learning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sampler. The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data. We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics.",,,,ACL
95,2018,Adaptive Scaling for Sparse Detection in Information Extraction,"Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun","This paper focuses on detection tasks in information extraction, where positive instances are sparsely distributed and models are usually evaluated using F-measure on positive classes. These characteristics often result in deficient performance of neural network based detection models. In this paper, we propose adaptive scaling, an algorithm which can handle the positive sparsity problem and directly optimize over F-measure via dynamic cost-sensitive learning. To this end, we borrow the idea of marginal utility from economics and propose a theoretical framework for instance importance measuring without introducing any additional hyper-parameters. Experiments show that our algorithm leads to a more effective and stable training of neural network based detection models.",,,,ACL
96,2018,Strong Baselines for Neural Semi-Supervised Learning under Domain Shift,"Sebastian Ruder, Barbara Plank","Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks for part-of-speech tagging and sentiment analysis are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state-of-the-art for NLP. Hence classic approaches constitute an important and strong baseline.",,,,ACL
97,2018,Fluency Boost Learning and Inference for Neural Grammatical Error Correction,"Tao Ge, Furu Wei, Ming Zhou","Most of the neural sequence-to-sequence (seq2seq) models for grammatical error correction (GEC) have two limitations: (1) a seq2seq model may not be well generalized with only limited error-corrected data; (2) a seq2seq model may fail to completely correct a sentence with multiple errors through normal seq2seq inference. We attempt to address these limitations by proposing a fluency boost learning and inference mechanism. Fluency boosting learning generates fluency-boost sentence pairs during training, enabling the error correction model to learn how to improve a sentence’s fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally with multiple inference steps until the sentence’s fluency stops increasing. Experiments show our approaches improve the performance of seq2seq models for GEC, achieving state-of-the-art results on both CoNLL-2014 and JFLEG benchmark datasets.",,,,ACL
98,2018,A Neural Architecture for Automated ICD Coding,"Pengtao Xie, Eric Xing","The International Classification of Diseases (ICD) provides a hierarchy of diagnostic codes for classifying diseases. Medical coding – which assigns a subset of ICD codes to a patient visit – is a mandatory process that is crucial for patient care and billing. Manual coding is time-consuming, expensive, and error prone. In this paper, we build a neural architecture for automated coding. It takes the diagnosis descriptions (DDs) of a patient as inputs and selects the most relevant ICD codes. This architecture contains four major ingredients: (1) tree-of-sequences LSTM encoding of code descriptions (CDs), (2) adversarial learning for reconciling the different writing styles of DDs and CDs, (3) isotonic constraints for incorporating the importance order among the assigned codes, and (4) attentional matching for performing many-to-one and one-to-many mappings from DDs to CDs. We demonstrate the effectiveness of the proposed methods on a clinical datasets with 59K patient visits.",,,,ACL
99,2018,Domain Adaptation with Adversarial Training and Graph Embeddings,"Firoj Alam, Shafiq Joty, Muhammad Imran","The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.",,,,ACL
100,2018,TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring,"Cancan Jin, Ben He, Kai Hui, Le Sun","Existing automated essay scoring (AES) models rely on rated essays for the target prompt as training data. Despite their successes in prompt-dependent AES, how to effectively predict essay ratings under a prompt-independent setting remains a challenge, where the rated essays for the target prompt are not available. To close this gap, a two-stage deep neural network (TDNN) is proposed. In particular, in the first stage, using the rated essays for non-target prompts as the training data, a shallow model is learned to select essays with an extreme quality for the target prompt, serving as pseudo training data; in the second stage, an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the first step. Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task.",,,,ACL
101,2018,Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation,"Tiancheng Zhao, Kyusong Lee, Maxine Eskenazi","The encoder-decoder dialog model is one of the most prominent methods used to build dialog systems in complex domains. Yet it is limited because it cannot output interpretable actions as in traditional systems, which hinders humans from understanding its generation process. We present an unsupervised discrete sentence representation learning method that can integrate with any existing encoder-decoder dialog models for interpretable response generation. Building upon variational autoencoders (VAEs), we present two novel models, DI-VAE and DI-VST that improve VAEs and can discover interpretable semantics via either auto encoding or context predicting. Our methods have been validated on real-world dialog datasets to discover semantic representations and enhance encoder-decoder models with interpretable generation.",,,,ACL
102,2018,Learning to Control the Specificity in Neural Response Generation,"Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan","In conversation, a general response (e.g., “I don’t know”) could correspond to a large variety of input utterances. Previous generative conversational models usually employ a single model to learn the relationship between different utterance-response pairs, thus tend to favor general and trivial responses which appear frequently. To address this problem, we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity. Specifically, we introduce an explicit specificity control variable into a sequence-to-sequence model, which interacts with the usage representation of words through a Gaussian Kernel layer, to guide the model to generate responses at different specificity levels. We describe two ways to acquire distant labels for the specificity control variable in learning. Empirical studies show that our model can significantly outperform the state-of-the-art response generation models under both automatic and human evaluations.",,,,ACL
103,2018,Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network,"Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu","Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context. In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention. Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017) and we extend the attention mechanism in two ways. First, we construct representations of text segments at different granularities solely with stacked self-attention. Second, we try to extract the truly matched segment pairs with attention across the context and response. We jointly introduce those two kinds of attention in one uniform neural network. Experiments on two large-scale multi-turn response selection tasks show that our proposed model significantly outperforms the state-of-the-art models.",,,,ACL
104,2018,MojiTalk: Generating Emotional Responses at Scale,"Xianda Zhou, William Yang Wang","Generating emotional language is a key step towards building empathetic natural language processing agents. However, a major challenge for this line of research is the lack of large-scale labeled training data, and previous studies are limited to only small sets of human annotated sentiment labels. Additionally, explicitly controlling the emotion and sentiment of generated text is also difficult. In this paper, we take a more radical approach: we exploit the idea of leveraging Twitter data that are naturally labeled with emojis. We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence. We investigate several conditional variational autoencoders training on these conversations, which allow us to use emojis to control the emotion of the generated text. Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate high-quality abstractive conversation responses in accordance with designated emotions.",,,,ACL
105,2018,Taylor’s law for Human Linguistic Sequences,"Tatsuru Kobayashi, Kumiko Tanaka-Ishii","Taylor’s law describes the fluctuation characteristics underlying a system in which the variance of an event within a time span grows by a power law with respect to the mean. Although Taylor’s law has been applied in many natural and social systems, its application for language has been scarce. This article describes a new way to quantify Taylor’s law in natural language and conducts Taylor analysis of over 1100 texts across 14 languages. We found that the Taylor exponents of natural language written texts exhibit almost the same value. The exponent was also compared for other language-related data, such as the child-directed speech, music, and programming languages. The results show how the Taylor exponent serves to quantify the fundamental structural complexity underlying linguistic time series. The article also shows the applicability of these findings in evaluating language models.",,,,ACL
106,2018,A Framework for Representing Language Acquisition in a Population Setting,"Jordan Kodner, Christopher Cerezo Falco",Language variation and change are driven both by individuals’ internal cognitive processes and by the social structures through which language propagates. A wide range of computational frameworks have been proposed to connect these drivers. We compare the strengths and weaknesses of existing approaches and propose a new analytic framework which combines previous network models’ ability to capture realistic social structure with practically and more elegant computational properties. The framework privileges the process of language acquisition and embeds learners in a social network but is modular so that population structure can be combined with different acquisition models. We demonstrate two applications for the framework: a test of practical concerns that arise when modeling acquisition in a population setting and an application of the framework to recent work on phonological mergers in progress.,,,,ACL
107,2018,Prefix Lexicalization of Synchronous CFGs using Synchronous TAG,"Logan Born, Anoop Sarkar","We show that an epsilon-free, chain-free synchronous context-free grammar (SCFG) can be converted into a weakly equivalent synchronous tree-adjoining grammar (STAG) which is prefix lexicalized. This transformation at most doubles the grammar’s rank and cubes its size, but we show that in practice the size increase is only quadratic. Our results extend Greibach normal form from CFGs to SCFGs and prove new formal properties about SCFG, a formalism with many applications in natural language processing.",,,,ACL
108,2018,Straight to the Tree: Constituency Parsing with Neural Syntactic Distance,"Yikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessandro Sordoni","In this work, we propose a novel constituency parsing scheme. The model first predicts a real-valued scalar, named syntactic distance, for each split position in the sentence. The topology of grammar tree is then determined by the values of syntactic distances. Compared to traditional shift-reduce parsing schemes, our approach is free from the potentially disastrous compounding error. It is also easier to parallelize and much faster. Our model achieves the state-of-the-art single model F1 score of 92.1 on PTB and 86.4 on CTB dataset, which surpasses the previous single model results by a large margin.",,,,ACL
109,2018,Gaussian Mixture Latent Vector Grammars,"Yanpeng Zhao, Liwen Zhang, Kewei Tu","We introduce Latent Vector Grammars (LVeGs), a new framework that extends latent variable grammars such that each nonterminal symbol is associated with a continuous vector space representing the set of (infinitely many) subtypes of the nonterminal. We show that previous models such as latent variable grammars and compositional vector grammars can be interpreted as special cases of LVeGs. We then present Gaussian Mixture LVeGs (GM-LVeGs), a new special case of LVeGs that uses Gaussian mixtures to formulate the weights of production rules over subtypes of nonterminals. A major advantage of using Gaussian mixtures is that the partition function and the expectations of subtype rules can be computed using an extension of the inside-outside algorithm, which enables efficient inference and learning. We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies.",,,,ACL
110,2018,Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples,"Vidur Joshi, Matthew Peters, Mark Hopkins","We revisit domain adaptation for parsers in the neural era. First we show that recent advances in word representations greatly diminish the need for domain adaptation when the target domain is syntactically similar to the source domain. As evidence, we train a parser on the Wall Street Journal alone that achieves over 90% F1 on the Brown corpus. For more syntactically distant domains, we provide a simple way to adapt a parser using only dozens of partial annotations. For instance, we increase the percentage of error-free geometry-domain parses in a held-out set from 45% to 73% using approximately five dozen training examples. In the process, we demonstrate a new state-of-the-art single model result on the Wall Street Journal test set of 94.3%. This is an absolute increase of 1.7% over the previous state-of-the-art of 92.6%.",,,,ACL
111,2018,Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations,"Vered Shwartz, Ido Dagan","Revealing the implicit semantic relation between the constituents of a noun-compound is important for many NLP applications. It has been addressed in the literature either as a classification task to a set of pre-defined relations or by producing free text paraphrases explicating the relations. Most existing paraphrasing methods lack the ability to generalize, and have a hard time interpreting infrequent or new noun-compounds. We propose a neural model that generalizes better by representing paraphrases in a continuous space, generalizing for both unseen noun-compounds and rare paraphrases. Our model helps improving performance on both the noun-compound paraphrasing and classification tasks.",,,,ACL
112,2018,Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings,"Maksim Tkachenko, Chong Cher Chia, Hady Lauw","We explore the notion of subjectivity, and hypothesize that word embeddings learnt from input corpora of varying levels of subjectivity behave differently on natural language processing tasks such as classifying a sentence by sentiment, subjectivity, or topic. Through systematic comparative analyses, we establish this to be the case indeed. Moreover, based on the discovery of the outsized role that sentiment words play on subjectivity-sensitive tasks such as sentiment classification, we develop a novel word embedding SentiVec which is infused with sentiment information from a lexical resource, and is shown to outperform baselines on such tasks.",,,,ACL
113,2018,Word Embedding and WordNet Based Metaphor Identification and Interpretation,"Rui Mao, Chenghua Lin, Frank Guerin","Metaphoric expressions are widespread in natural language, posing a significant challenge for various natural language processing tasks such as Machine Translation. Current word embedding based metaphor identification models cannot identify the exact metaphorical words within a sentence. In this paper, we propose an unsupervised learning method that identifies and interprets metaphors at word-level without any preprocessing, outperforming strong baselines in the metaphor identification task. Our model extends to interpret the identified metaphors, paraphrasing them into their literal counterparts, so that they can be better translated by machines. We evaluated this with two popular translation systems for English to Chinese, showing that our model improved the systems significantly.",,,,ACL
114,2018,Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings,"Yang Xu, Jiawei Liu, Wei Yang, Liusheng Huang","Traditional word embedding approaches learn semantic information at word level while ignoring the meaningful internal structures of words like morphemes. Furthermore, existing morphology-based models directly incorporate morphemes to train word embeddings, but still neglect the latent meanings of morphemes. In this paper, we explore to employ the latent meanings of morphological compositions of words to train and enhance word embeddings. Based on this purpose, we propose three Latent Meaning Models (LMMs), named LMM-A, LMM-S and LMM-M respectively, which adopt different strategies to incorporate the latent meanings of morphemes during the training process. Experiments on word similarity, syntactic analogy and text classification are conducted to validate the feasibility of our models. The results demonstrate that our models outperform the baselines on five word similarity datasets. On Wordsim-353 and RG-65 datasets, our models nearly achieve 5% and 7% gains over the classic CBOW model, respectively. For the syntactic analogy and text classification tasks, our models also surpass all the baselines including a morphology-based model.",,,,ACL
115,2018,A Stochastic Decoder for Neural Machine Translation,"Philip Schulz, Wilker Aziz, Trevor Cohn","The process of translation is ambiguous, in that there are typically many valid translations for a given sentence. This gives rise to significant variation in parallel corpora, however, most current models of machine translation do not account for this variation, instead treating the problem as a deterministic process. To this end, we present a deep generative model of machine translation which incorporates a chain of latent variables, in order to account for local lexical and syntactic variation in parallel corpora. We provide an in-depth analysis of the pitfalls encountered in variational inference for training deep generative models. Experiments on several different language pairs demonstrate that the model consistently improves over strong baselines.",,,,ACL
116,2018,Forest-Based Neural Machine Translation,"Chunpeng Ma, Akihiro Tamura, Masao Utiyama, Tiejun Zhao","Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors. For statistical machine translation (SMT), forest-based methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-sequence NMT model). The BLEU score of the proposed method is higher than that of the sequence-to-sequence NMT, tree-based NMT, and forest-based SMT systems.",,,,ACL
117,2018,Context-Aware Neural Machine Translation Learns Anaphora Resolution,"Elena Voita, Pavel Serdyukov, Rico Sennrich, Ivan Titov","Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6).",,,,ACL
118,2018,Document Context Neural Machine Translation with Memory Networks,"Sameen Maruf, Gholamreza Haffari","We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model end-to-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",,,,ACL
119,2018,Which Melbourne? Augmenting Geocoding with Maps,"Milan Gritta, Mohammad Taher Pilehvar, Nigel Collier","The purpose of text geolocation is to associate geographic information contained in a document with a set (or sets) of coordinates, either implicitly by using linguistic features and/or explicitly by using geographic metadata combined with heuristics. We introduce a geocoder (location mention disambiguator) that achieves state-of-the-art (SOTA) results on three diverse datasets by exploiting the implicit lexical clues. Moreover, we propose a new method for systematic encoding of geographic metadata to generate two distinct views of the same text. To that end, we introduce the Map Vector (MapVec), a sparse representation obtained by plotting prior geographic probabilities, derived from population figures, on a World Map. We then integrate the implicit (language) and explicit (map) features to significantly improve a range of metrics. We also introduce an open-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing.",,,,ACL
120,2018,Learning Prototypical Goal Activities for Locations,"Tianyu Jiang, Ellen Riloff","People go to different places to engage in activities that reflect their goals. For example, people go to restaurants to eat, libraries to study, and churches to pray. We refer to an activity that represents a common reason why people typically go to a location as a prototypical goal activity (goal-act). Our research aims to learn goal-acts for specific locations using a text corpus and semi-supervised learning. First, we extract activities and locations that co-occur in goal-oriented syntactic patterns. Next, we create an activity profile matrix and apply a semi-supervised label propagation algorithm to iteratively revise the activity strengths for different locations using a small set of labeled data. We show that this approach outperforms several baseline methods when judged against goal-acts identified by human annotators.",,,,ACL
121,2018,Guess Me if You Can: Acronym Disambiguation for Enterprises,"Yang Li, Bo Zhao, Ariel Fuxman, Fangbo Tao","Acronyms are abbreviations formed from the initial components of words or phrases. In enterprises, people often use acronyms to make communications more efficient. However, acronyms could be difficult to understand for people who are not familiar with the subject matter (new employees, etc.), thereby affecting productivity. To alleviate such troubles, we study how to automatically resolve the true meanings of acronyms in a given context. Acronym disambiguation for enterprises is challenging for several reasons. First, acronyms may be highly ambiguous since an acronym used in the enterprise could have multiple internal and external meanings. Second, there are usually no comprehensive knowledge bases such as Wikipedia available in enterprises. Finally, the system should be generic to work for any enterprise. In this work we propose an end-to-end framework to tackle all these challenges. The framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output. Our disambiguation models are trained via distant supervised learning, without requiring any manually labeled training examples. Therefore, our proposed framework can be deployed to any enterprise to support high-quality acronym disambiguation. Experimental results on real world data justified the effectiveness of our system.",,,,ACL
122,2018,A Multi-Axis Annotation Scheme for Event Temporal Relations,"Qiang Ning, Hao Wu, Dan Roth","Existing temporal relation (TempRel) annotation schemes often have low inter-annotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition. This paper proposes a new multi-axis modeling to better capture the temporal structure of events. In addition, we identify that event end-points are a major source of confusion in annotation, so we also propose to annotate TempRels based on start-points only. A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60’s to 80’s (Cohen’s Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.",,,,ACL
123,2018,Exemplar Encoder-Decoder for Neural Conversation Generation,"Gaurav Pandey, Danish Contractor, Vineet Kumar, Sachindra Joshi","In this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses. Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model and the corresponding responses are used by our decoder to generate the ground truth response. The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context. As a result, our model learns to assign higher similarity scores to those retrieved contexts whose responses are crucial for generating the final response. We present detailed experiments on two large data sets and we find that our method out-performs state of the art sequence to sequence generative models on several recently proposed evaluation metrics.",,,,ACL
124,2018,DialSQL: Dialogue Based Structured Query Generation,"Izzeddin Gur, Semih Yavuz, Yu Su, Xifeng Yan","The recent advance in deep learning and semantic parsing has significantly improved the translation accuracy of natural language questions to structured queries. However, further improvement of the existing approaches turns out to be quite challenging. Rather than solely relying on algorithmic innovations, in this work, we introduce DialSQL, a dialogue-based structured query generation framework that leverages human intelligence to boost the performance of existing algorithms via user interaction. DialSQL is capable of identifying potential errors in a generated SQL query and asking users for validation via simple multi-choice questions. User feedback is then leveraged to revise the query. We design a generic simulator to bootstrap synthetic training dialogues and evaluate the performance of DialSQL on the WikiSQL dataset. Using SQLNet as a black box query generation tool, DialSQL improves its performance from 61.3% to 69.0% using only 2.4 validation questions per dialogue.",,,,ACL
125,2018,Conversations Gone Awry: Detecting Early Signs of Conversational Failure,"Justine Zhang, Jonathan Chang, Cristian Danescu-Niculescu-Mizil, Lucas Dixon","One of the main challenges online social systems face is the prevalence of antisocial behavior, such as harassment and personal attacks. In this work, we introduce the task of predicting from the very start of a conversation whether it will get out of hand. As opposed to detecting undesirable behavior after the fact, this task aims to enable early, actionable prediction at a time when the conversation might still be salvaged. To this end, we develop a framework for capturing pragmatic devices—such as politeness strategies and rhetorical prompts—used to start a conversation, and analyze their relation to its future trajectory. Applying this framework in a controlled setting, we demonstrate the feasibility of detecting early warning signs of antisocial behavior in online discussions.",,,,ACL
126,2018,Are BLEU and Meaning Representation in Opposition?,"Ondřej Cífka, Ondřej Bojar","One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.",,,,ACL
127,2018,Automatic Metric Validation for Grammatical Error Correction,"Leshem Choshen, Omri Abend","Metric validation in Grammatical Error Correction (GEC) is currently done by observing the correlation between human and metric-induced rankings. However, such correlation studies are costly, methodologically troublesome, and suffer from low inter-rater agreement. We propose MAEGE, an automatic methodology for GEC metric validation, that overcomes many of the difficulties in the existing methodology. Experiments with MAEGE shed a new light on metric quality, showing for example that the standard M2 metric fares poorly on corpus-level ranking. Moreover, we use MAEGE to perform a detailed analysis of metric behavior, showing that some types of valid edits are consistently penalized by existing metrics.",,,,ACL
128,2018,The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing,"Rotem Dror, Gili Baumer, Segev Shlomov, Roi Reichart","Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/ theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied. in NLP research in a statistically sound manner.",,,,ACL
129,2018,Distilling Knowledge for Search-based Structured Prediction,"Yijia Liu, Wanxiang Che, Huaipeng Zhao, Bing Qin","Many natural language processing tasks can be modeled into structured prediction and solved as a search problem. In this paper, we distill an ensemble of multiple models trained with different initialization into a single model. In addition to learning to match the ensemble’s probability output on the reference states, we also use the ensemble to explore the search space and learn from the encountered states in the exploration. Experimental results on two typical search-based structured prediction tasks – transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures.",,,,ACL
130,2018,Stack-Pointer Networks for Dependency Parsing,"Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng","We introduce a novel architecture for dependency parsing: stack-pointer networks (StackPtr). Combining pointer networks (Vinyals et al., 2015) with an internal stack, the proposed model first reads and encodes the whole sentence, then builds the dependency tree top-down (from root-to-leaf) in a depth-first fashion. The stack tracks the status of the depth-first search and the pointer networks select one child for the word at the top of the stack at each step. The StackPtr parser benefits from the information of whole sentence and all previously derived subtree structures, and removes the left-to-right restriction in classical transition-based parsers. Yet the number of steps for building any (non-projective) parse tree is linear in the length of the sentence just as other transition-based parsers, yielding an efficient decoding algorithm with O(n2) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-the-art performances on 21 of them",,,,ACL
131,2018,Twitter Universal Dependency Parsing for African-American and Mainstream American English,"Su Lin Blodgett, Johnny Wei, Brendan O’Connor","Due to the presence of both Twitter-specific conventions and non-standard and dialectal language, Twitter presents a significant parsing challenge to current dependency parsing tools. We broaden English dependency parsing to handle social media English, particularly social media African-American English (AAE), by developing and annotating a new dataset of 500 tweets, 250 of which are in AAE, within the Universal Dependencies 2.0 framework. We describe our standards for handling Twitter- and AAE-specific features and evaluate a variety of cross-domain strategies for improving parsing with no, or very little, in-domain labeled data, including a new data synthesis approach. We analyze these methods’ impact on performance disparities between AAE and Mainstream American English tweets, and assess parsing accuracy for specific AAE lexical and syntactic features. Our annotated data and a parsing model are available at: http://slanglab.cs.umass.edu/TwitterAAE/.",,,,ACL
132,2018,"LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better","Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yogatama","Language exhibits hierarchical structure, but recent work using a subject-verb agreement diagnostic argued that state-of-the-art language models, LSTMs, fail to learn long-range syntax sensitive dependencies. Using the same diagnostic, we show that, in fact, LSTMs do succeed in learning such dependencies—provided they have enough capacity. We then explore whether models that have access to explicit syntactic information learn agreement more effectively, and how the way in which this structural information is incorporated into the model impacts performance. We find that the mere presence of syntactic information does not improve accuracy, but when model architecture is determined by syntax, number agreement is improved. Further, we find that the choice of how syntactic structure is built affects how well number agreement is learned: top-down construction outperforms left-corner and bottom-up variants in capturing non-local structural dependencies.",,,,ACL
133,2018,Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures,"Wenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun Ren","Existing solutions to task-oriented dialogue systems follow pipeline designs which introduces architectural complexity and fragility. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude. It significantly outperforms state-of-the-art pipeline-based methods on large datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail.",,,,ACL
134,2018,An End-to-end Approach for Handling Unknown Slot Values in Dialogue State Tracking,"Puyang Xu, Qi Hu","We highlight a practical yet rarely discussed problem in dialogue state tracking (DST), namely handling unknown slot values. Previous approaches generally assume predefined candidate lists and thus are not designed to output unknown values, especially when the spoken language understanding (SLU) module is absent as in many end-to-end (E2E) systems. We describe in this paper an E2E architecture based on the pointer network (PtrNet) that can effectively extract unknown slot values while still obtains state-of-the-art accuracy on the standard DSTC2 benchmark. We also provide extensive empirical evidence to show that tracking unknown values can be challenging and our approach can bring significant improvement with the help of an effective feature dropout technique.",,,,ACL
135,2018,Global-Locally Self-Attentive Encoder for Dialogue State Tracking,"Victor Zhong, Caiming Xiong, Richard Socher","Dialogue state tracking, which estimates user goals and requests given the dialogue context, is an essential part of task-oriented dialogue systems. In this paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker (GLAD), which learns representations of the user utterance and previous system actions with global-local modules. Our model uses global modules to shares parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. We show that this significantly improves tracking of rare states. GLAD obtains 88.3% joint goal accuracy and 96.4% request accuracy on the WoZ state tracking task, outperforming prior work by 3.9% and 4.8%. On the DSTC2 task, our model obtains 74.7% joint goal accuracy and 97.3% request accuracy, outperforming prior work by 1.3% and 0.8%",,,,ACL
136,2018,Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems,"Andrea Madotto, Chien-Sheng Wu, Pascale Fung","End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.",,,,ACL
137,2018,Tailored Sequence to Sequence Models to Different Conversation Scenarios,"Hainan Zhang, Yanyan Lan, Jiafeng Guo, Jun Xu","Sequence to sequence (Seq2Seq) models have been widely used for response generation in the area of conversation. However, the requirements for different conversation scenarios are distinct. For example, customer service requires the generated responses to be specific and accurate, while chatbot prefers diverse responses so as to attract different users. The current Seq2Seq model fails to meet these diverse requirements, by using a general average likelihood as the optimization criteria. As a result, it usually generates safe and commonplace responses, such as ‘I don’t know’. In this paper, we propose two tailored optimization criteria for Seq2Seq to different conversation scenarios, i.e., the maximum generated likelihood for specific-requirement scenario, and the conditional value-at-risk for diverse-requirement scenario. Experimental results on the Ubuntu dialogue corpus (Ubuntu service scenario) and Chinese Weibo dataset (social chatbot scenario) show that our proposed models not only satisfies diverse requirements for different scenarios, but also yields better performances against traditional Seq2Seq models in terms of both metric-based and human evaluations.",,,,ACL
138,2018,Knowledge Diffusion for Neural Dialogue Generation,"Shuman Liu, Hongshen Chen, Zhaochun Ren, Yang Feng","End-to-end neural dialogue generation has shown promising results recently, but it does not employ knowledge to guide the generation and hence tends to generate short, general, and meaningless responses. In this paper, we propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities. With the help of facts matching and entity diffusion, the neural dialogue generation is augmented with the ability of convergent and divergent thinking over the knowledge base. Our empirical study on a real-world dataset prove that our model is capable of generating meaningful, diverse and natural responses for both factoid-questions and knowledge grounded chi-chats. The experiment results also show that our model outperforms competitive baseline models significantly.",,,,ACL
139,2018,Generating Informative Responses with Controlled Sentence Function,"Pei Ke, Jian Guan, Minlie Huang, Xiaoyan Zhu","Sentence function is a significant factor to achieve the purpose of the speaker, which, however, has not been touched in large-scale conversation generation so far. In this paper, we present a model to generate informative responses with controlled sentence function. Our model utilizes a continuous latent variable to capture various word patterns that realize the expected sentence function, and introduces a type controller to deal with the compatibility of controlling sentence function and generating informative content. Conditioned on the latent variable, the type controller determines the type (i.e., function-related, topic, and ordinary word) of a word to be generated at each decoding position. Experiments show that our model outperforms state-of-the-art baselines, and it has the ability to generate responses with both controlled sentence function and informative content.",,,,ACL
140,2018,Sentiment Adaptive End-to-End Dialog Systems,"Weiyan Shi, Zhou Yu","End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating. However, current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.",,,,ACL
141,2018,Embedding Learning Through Multilingual Concept Induction,"Philipp Dufter, Mengjie Zhao, Martin Schmitt, Alexander Fraser",We present a new method for estimating vector space representations of words: embedding learning by concept induction. We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space. An extensive experimental evaluation on crosslingual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches.,,,,ACL
142,2018,Isomorphic Transfer of Syntactic Structures in Cross-Lingual NLP,"Edoardo Maria Ponti, Roi Reichart, Anna Korhonen, Ivan Vulić","The transfer or share of knowledge between languages is a potential solution to resource scarcity in NLP. However, the effectiveness of cross-lingual transfer can be challenged by variation in syntactic structures. Frameworks such as Universal Dependencies (UD) are designed to be cross-lingually consistent, but even in carefully designed resources trees representing equivalent sentences may not always overlap. In this paper, we measure cross-lingual syntactic variation, or anisomorphism, in the UD treebank collection, considering both morphological and structural properties. We show that reducing the level of anisomorphism yields consistent gains in cross-lingual transfer tasks. We introduce a source language selection procedure that facilitates effective cross-lingual parser transfer, and propose a typologically driven method for syntactic tree processing which reduces anisomorphism. Our results show the effectiveness of this method for both machine translation and cross-lingual sentence similarity, demonstrating the importance of syntactic structure compatibility for boosting cross-lingual transfer in NLP.",,,,ACL
143,2018,Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data,"Adithya Pratapa, Gayatri Bhat, Monojit Choudhury, Sunayana Sitaram","Training language models for Code-mixed (CM) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language. We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory. We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can significantly reduce the perplexity of an RNN-based language model. We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs.",,,,ACL
144,2018,Chinese NER Using Lattice LSTM,"Yue Zhang, Jie Yang","We investigate a lattice-structured LSTM model for Chinese NER, which encodes a sequence of input characters as well as all potential words that match a lexicon. Compared with character-based methods, our model explicitly leverages word and word sequence information. Compared with word-based methods, lattice LSTM does not suffer from segmentation errors. Gated recurrent cells allow our model to choose the most relevant characters and words from a sentence for better NER results. Experiments on various datasets show that lattice LSTM outperforms both word-based and character-based LSTM baselines, achieving the best results.",,,,ACL
145,2018,Nugget Proposal Networks for Chinese Event Detection,"Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun","Neural network based models commonly regard event detection as a word-wise classification task, which suffer from the mismatch problem between words and event triggers, especially in languages without natural word delimiters such as Chinese. In this paper, we propose Nugget Proposal Networks (NPNs), which can solve the word-trigger mismatch problem by directly proposing entire trigger nuggets centered at each character regardless of word boundaries. Specifically, NPNs perform event detection in a character-wise paradigm, where a hybrid representation for each character is first learned to capture both structural and semantic information from both characters and words. Then based on learned representations, trigger nuggets are proposed and categorized by exploiting character compositional structures of Chinese event triggers. Experiments on both ACE2005 and TAC KBP 2017 datasets show that NPNs significantly outperform the state-of-the-art methods.",,,,ACL
146,2018,Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation,"Madhav Nimishakavi, Manish Gupta, Partha Talukdar","Relation Schema Induction (RSI) is the problem of identifying type signatures of arguments of relations from unlabeled text. Most of the previous work in this area have focused only on binary RSI, i.e., inducing only the subject and object type signatures per relation. However, in practice, many relations are high-order, i.e., they have more than two arguments and inducing type signatures of all arguments is necessary. For example, in the sports domain, inducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is more informative than inducing just win(WinningPlayer, OpponentPlayer). We refer to this problem as Higher-order Relation Schema Induction (HRSI). In this paper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a novel framework for the HRSI problem. To the best of our knowledge, this is the first attempt at inducing higher-order relation schemata from unlabeled text. Using the experimental analysis on three real world datasets we show how TFBA helps in dealing with sparsity and induce higher-order schemata.",,,,ACL
147,2018,Discovering Implicit Knowledge with Unary Relations,"Michael Glass, Alfio Gliozzo","State-of-the-art relation extraction approaches are only able to recognize relationships between mentions of entity arguments stated explicitly in the text and typically localized to the same sentence. However, the vast majority of relations are either implicit or not sententially localized. This is a major problem for Knowledge Base Population, severely limiting recall. In this paper we propose a new methodology to identify relations between two entities, consisting of detecting a very large number of unary relations, and using them to infer missing entities. We describe a deep learning architecture able to learn thousands of such relations very efficiently by using a common deep learning based representation. Our approach largely outperforms state of the art relation extraction technology on a newly introduced web scale knowledge base population benchmark, that we release to the research community.",,,,ACL
148,2018,Improving Entity Linking by Modeling Latent Relations between Mentions,"Phong Le, Ivan Titov","Entity linking involves aligning textual mentions of named entities to their corresponding entries in a knowledge base. Entity linking systems often exploit relations between textual mentions in a document (e.g., coreference) to decide if the linking decisions are compatible. Unlike previous approaches, which relied on supervised systems or heuristics to predict these relations, we treat relations as latent variables in our neural entity-linking model. We induce the relations without any supervision while optimizing the entity-linking system in an end-to-end fashion. Our multi-relational model achieves the best reported scores on the standard benchmark (AIDA-CoNLL) and substantially outperforms its relation-agnostic version. Its training also converges much faster, suggesting that the injected structural bias helps to explain regularities in the training data.",,,,ACL
149,2018,Dating Documents using Graph Convolution Networks,"Shikhar Vashishth, Shib Sankar Dasgupta, Swayambhu Nath Ray, Partha Talukdar","Document date is essential for many important tasks, such as document retrieval, summarization, event detection, etc. While existing approaches for these tasks assume accurate knowledge of the document date, this is not always available, especially for arbitrary documents from the Web. Document Dating is a challenging problem which requires inference over the temporal structure of the document. Prior document dating systems have largely relied on handcrafted features while ignoring such document-internal structures. In this paper, we propose NeuralDater, a Graph Convolutional Network (GCN) based document dating approach which jointly exploits syntactic and temporal graph structures of document in a principled way. To the best of our knowledge, this is the first application of deep learning for the problem of document dating. Through extensive experiments on real-world datasets, we find that NeuralDater significantly outperforms state-of-the-art baseline by 19% absolute (45% relative) accuracy points.",,,,ACL
150,2018,A Graph-to-Sequence Model for AMR-to-Text Generation,"Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea","The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus facing challenges with large-graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.",,,,ACL
151,2018,GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data,"Bayu Distiawan Trisedya, Jianzhong Qi, Rui Zhang, Wei Wang","A knowledge base is a large repository of facts that are mainly represented as RDF triples, each of which consists of a subject, a predicate (relationship), and an object. The RDF triple representation offers a simple interface for applications to access the facts. However, this representation is not in a natural language form, which is difficult for humans to understand. We address this problem by proposing a system to translate a set of RDF triples into natural sentences based on an encoder-decoder framework. To preserve as much information from RDF triples as possible, we propose a novel graph-based triple encoder. The proposed encoder encodes not only the elements of the triples but also the relationships both within a triple and between the triples. Experimental results show that the proposed encoder achieves a consistent improvement over the baseline models by up to 17.6%, 6.0%, and 16.4% in three common metrics BLEU, METEOR, and TER, respectively.",,,,ACL
152,2018,Learning to Write with Cooperative Discriminators,"Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut","Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators each specialize in a different principle of communication, such as Grice’s maxims, and are collectively combined with the base RNN generator through a composite decoding objective. Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.",,,,ACL
153,2018,A Neural Approach to Pun Generation,"Zhiwei Yu, Jiwei Tan, Xiaojun Wan","Automatic pun generation is an interesting and challenging text generation task. Previous efforts rely on templates or laboriously manually annotated pun datasets, which heavily constrains the quality and diversity of generated puns. Since sequence-to-sequence models provide an effective technique for text generation, it is promising to investigate these models on the pun generation task. In this paper, we propose neural network models for homographic pun generation, and they can generate puns without requiring any pun data for training. We first train a conditional neural language model from a general text corpus, and then generate puns from the language model with an elaborately designed decoding algorithm. Automatic and human evaluations show that our models are able to generate homographic puns of good readability and quality.",,,,ACL
154,2018,Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data,"Harsh Jhamtani, Varun Gangal, Eduard Hovy, Graham Neubig","This paper examines the problem of generating natural language descriptions of chess games. We introduce a new large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a chess game. The introduced dataset consists of more than 298K chess move-commentary pairs across 11K chess games. We highlight how this task poses unique research challenges in natural language generation: the data contain a large variety of styles of commentary and frequently depend on pragmatic context. We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of the game state that may be commented upon to describe a given chess move. Through a human study on predictions for a subset of the data which deals with direct move descriptions, we observe that outputs from our models are rated similar to ground truth commentary texts in terms of correctness and fluency.",,,,ACL
155,2018,From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction,"Zihang Dai, Qizhe Xie, Eduard Hovy","In this work, we study the credit assignment problem in reward augmented maximum likelihood (RAML) learning, and establish a theoretical equivalence between the token-level counterpart of RAML and the entropy regularized reinforcement learning. Inspired by the connection, we propose two sequence prediction algorithms, one extending RAML with fine-grained credit assignment and the other improving Actor-Critic with a systematic entropy regularization. On two benchmark datasets, we show the proposed algorithms outperform RAML and Actor-Critic respectively, providing new alternatives to sequence prediction.",,,,ACL
156,2018,DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension,"Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, Karthik Sankaranarayanan","We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding.",,,,ACL
157,2018,Stochastic Answer Networks for Machine Reading Comprehension,"Xiaodong Liu, Yelong Shen, Kevin Duh, Jianfeng Gao","We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. Compared to previous work such as ReasoNet which used reinforcement learning to determine the number of steps, the unique feature is the use of a kind of stochastic prediction dropout on the answer module (final layer) of the neural network during the training. We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD), the Adversarial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO).",,,,ACL
158,2018,Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering,"Wei Wang, Ming Yan, Chen Wu","This paper describes a novel hierarchical attention network for reading comprehension style question answering, which aims to answer questions for a given narrative paragraph. In the proposed method, attention and fusion are conducted horizontally and vertically across layers at different levels of granularity between question and paragraph. Specifically, it first encode the question and paragraph with fine-grained language embeddings, to better capture the respective representations at semantic level. Then it proposes a multi-granularity fusion approach to fully fuse information from both global and attended representations. Finally, it introduces a hierarchical attention network to focuses on the answer span progressively with multi-level soft-alignment. Extensive experiments on the large-scale SQuAD, TriviaQA dataset validate the effectiveness of the proposed method. At the time of writing the paper, our model achieves state-of-the-art on the both SQuAD and TriviaQA Wiki leaderboard as well as two adversarial SQuAD datasets.",,,,ACL
159,2018,Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension,"Zhen Wang, Jiachen Liu, Xinyan Xiao, Yajuan Lyu","While sophisticated neural-based techniques have been developed in reading comprehension, most approaches model the answer in an independent manner, ignoring its relations with other answer candidates. This problem can be even worse in open-domain scenarios, where candidates from multiple passages should be combined to answer a single question. In this paper, we formulate reading comprehension as an extract-then-select two-stage procedure. We first extract answer candidates from passages, then select the final answer by combining information from all the candidates. Furthermore, we regard candidate extraction as a latent variable and train the two-stage process jointly with reinforcement learning. As a result, our approach has improved the state-of-the-art performance significantly on two challenging open-domain reading comprehension datasets. Further analysis demonstrates the effectiveness of our model components, especially the information fusion of all the candidates and the joint training of the extract-then-select procedure.",,,,ACL
160,2018,Efficient and Robust Question Answering from Minimal Context over Documents,"Sewon Min, Victor Zhong, Richard Socher, Caiming Xiong","Neural models for question answering (QA) over documents have achieved significant performance improvements. Although effective, these models do not scale to large corpora due to their complex modeling of interactions between the document and the question. Moreover, recent work has shown that such models are sensitive to adversarial inputs. In this paper, we study the minimal context required to answer the question, and find that most questions in existing datasets can be answered with a small set of sentences. Inspired by this observation, we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model. Our overall system achieves significant reductions in training (up to 15 times) and inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs.",,,,ACL
161,2018,Denoising Distantly Supervised Open-Domain Question Answering,"Yankai Lin, Haozhe Ji, Zhiyuan Liu, Maosong Sun","Distantly supervised open-domain question answering (DS-QA) aims to find answers in collections of unlabeled text. Existing DS-QA models usually retrieve related paragraphs from a large-scale corpus and apply reading comprehension technique to extract answers from the most relevant paragraph. They ignore the rich information contained in other paragraphs. Moreover, distant supervision data inevitably accompanies with the wrong labeling problem, and these noisy data will substantially degrade the performance of DS-QA. To address these issues, we propose a novel DS-QA model which employs a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines.",,,,ACL
162,2018,Question Condensing Networks for Answer Selection in Community Question Answering,"Wei Wu, Xu Sun, Houfeng Wang","Answer selection is an important subtask of community question answering (CQA). In a real-world CQA forum, a question is often represented as two parts: a subject that summarizes the main points of the question, and a body that elaborates on the subject in detail. Previous researches on answer selection usually ignored the difference between these two parts and concatenated them as the question representation. In this paper, we propose the Question Condensing Networks (QCN) to make use of the subject-body relationship of community questions. In our model, the question subject is the primary part of the question representation, and the question body information is aggregated based on similarity and disparity with the question subject. Experimental results show that QCN outperforms all existing models on two CQA datasets.",,,,ACL
163,2018,Towards Robust Neural Machine Translation,"Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie Zhai","Small perturbations in the input can severely distort intermediate representations and thus impact translation quality of neural machine translation (NMT) models. In this paper, we propose to improve the robustness of NMT models with adversarial stability training. The basic idea is to make both the encoder and decoder in NMT models robust against input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. Experimental results on Chinese-English, English-German and English-French translation tasks show that our approaches can not only achieve significant improvements over strong NMT systems but also improve the robustness of NMT models.",,,,ACL
164,2018,Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings,"Shaohui Kuang, Junhui Li, António Branco, Weihua Luo","In neural machine translation, a source sequence of words is encoded into a vector from which a target sequence is generated in the decoding phase. Differently from statistical machine translation, the associations between source words and their possible target counterparts are not explicitly stored. Source and target words are at the two ends of a long information processing procedure, mediated by hidden states at both the source encoding and the target decoding phases. This makes it possible that a source word is incorrectly translated into a target word that is not any of its admissible equivalent counterparts in the target language. In this paper, we seek to somewhat shorten the distance between source and target words in that procedure, and thus strengthen their association, by means of a method we term bridging source and target word embeddings. We experiment with three strategies: (1) a source-side bridging model, where source word embeddings are moved one step closer to the output target sequence; (2) a target-side bridging model, which explores the more relevant source word embeddings for the prediction of the target sequence; and (3) a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation of ones by the others. Experiments and analysis presented in this paper demonstrate that the proposed bridging models are able to significantly improve quality of both sentence translation, in general, and alignment and translation of individual source words with target words, in particular.",,,,ACL
165,2018,Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning,"Julia Kreutzer, Joshua Uyheng, Stefan Riezler","We present a study on reinforcement learning (RL) from human bandit feedback for sequence-to-sequence learning, exemplified by the task of bandit neural machine translation (NMT). We investigate the reliability of human bandit feedback, and analyze the influence of reliability on the learnability of a reward estimator, and the effect of the quality of reward estimates on the overall RL task. Our analysis of cardinal (5-point ratings) and ordinal (pairwise preferences) feedback shows that their intra- and inter-annotator α-agreement is comparable. Best reliability is obtained for standardized cardinal feedback, and cardinal feedback is also easiest to learn and generalize from. Finally, improvements of over 1 BLEU can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into RL for NMT. This shows that RL is possible even from small amounts of fairly reliable human feedback, pointing to a great potential for applications at larger scale.",,,,ACL
166,2018,Accelerating Neural Transformer via an Average Attention Network,"Biao Zhang, Deyi Xiong, Jinsong Su","With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive architecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 different language pairs, we obtain robust and consistent speed-ups in decoding.",,,,ACL
167,2018,How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures,Tobias Domhan,"With recent advances in network architectures for Neural Machine Translation (NMT) recurrent models have effectively been replaced by either convolutional or self-attentional approaches, such as in the Transformer. While the main innovation of the Transformer architecture is its use of self-attentional layers, there are several other aspects, such as attention with multiple heads and the use of many attention layers, that distinguish the model from previous baselines. In this work we take a fine-grained look at the different architectures for NMT. We introduce an Architecture Definition Language (ADL) allowing for a flexible combination of common building blocks. Making use of this language we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention. Additionally, we find that self-attention is much more important on the encoder side than on the decoder side, where it can be replaced by a RNN or CNN without a loss in performance in most settings. Surprisingly, even a model without any target side self-attention performs well.",,,,ACL
168,2018,Weakly Supervised Semantic Parsing with Abstract Examples,"Omer Goldman, Veronica Latcinnik, Ehud Nave, Amir Globerson","Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways. First, a large search space of potential programs needs to be explored at training time to find a correct program. Second, spurious programs that accidentally lead to a correct denotation add noise to training. In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and program are lifted to an abstract form. We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training. To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spuriousness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation. Our method substantially improves performance, and reaches 82.5% accuracy, a 14.7% absolute accuracy improvement compared to the best reported accuracy so far.",,,,ACL
169,2018,Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback,"Carolin Lawrence, Stefan Riezler","Counterfactual learning from human bandit feedback describes a scenario where user feedback on the quality of outputs of a historic system is logged and used to improve a target system. We show how to apply this learning framework to neural semantic parsing. From a machine learning perspective, the key challenge lies in a proper reweighting of the estimator so as to avoid known degeneracies in counterfactual learning, while still being applicable to stochastic gradient optimization. To conduct experiments with human users, we devise an easy-to-use interface to collect human feedback on semantic parses. Our work is the first to show that semantic parsers can be improved significantly by counterfactual learning from logged human feedback data.",,,,ACL
170,2018,AMR dependency parsing with a typed semantic algebra,"Jonas Groschwitz, Matthias Lindemann, Meaghan Fowlie, Mark Johnson","We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph. This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system. We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and outperform strong baselines.",,,,ACL
171,2018,Sequence-to-sequence Models for Cache Transition Systems,"Xiaochang Peng, Linfeng Song, Daniel Gildea, Giorgio Satta","In this paper, we present a sequence-to-sequence based approach for mapping natural language sentences to AMR semantic graphs. We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system. To address the sparsity issue of neural AMR parsing, we feed feature embeddings from the transition state to provide relevant local information for each decoder state. We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.",,,,ACL
172,2018,Batch IS NOT Heavy: Learning Word Representations From All Samples,"Xin Xin, Fajie Yuan, Xiangnan He, Joemon M. Jose","Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations. However, it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution. Besides, SGD suffers from dramatic fluctuation due to the one-sample learning scheme. In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples. Remarkably, the time complexity of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples. We evaluate AllVec on several benchmark tasks. Experiments show that AllVec outperforms sampling-based SGD methods with comparable efficiency, especially for small training corpora.",,,,ACL
173,2018,Backpropagating through Structured Argmax using a SPIGOT,"Hao Peng, Sam Thomson, Noah A. Smith","We introduce structured projection of intermediate gradients (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers. SPIGOT requires no marginal inference, unlike structured attention networks and reinforcement learning-inspired solutions. Like so-called straight-through estimators, SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT’s proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed. We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.",,,,ACL
174,2018,Learning How to Actively Learn: A Deep Imitation Learning Approach,"Ming Liu, Wray Buntine, Gholamreza Haffari","Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary. We introduce a method that learns an AL “policy” using “imitation learning” (IL). Our IL-based approach makes use of an efficient and effective “algorithmic expert”, which provides the policy learner with good actions in the encountered AL situations. The AL strategy is then learned with a feedforward network, mapping situations to most informative query datapoints. We evaluate our method on two different tasks: text classification and named entity recognition. Experimental results show that our IL-based AL strategy is more effective than strong previous methods using heuristics and reinforcement learning.",,,,ACL
175,2018,Training Classifiers with Natural Language Explanations,"Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann","Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100 faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.",,,,ACL
176,2018,Did the Model Understand the Question?,"Pramod Kaushik Mudrakarta, Ankur Taly, Mukund Sundararajan, Kedar Dhamdhere","We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of “attribution” (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from 61.1% to 19%, and that of a tabular question answering model from 33.5% to 3.3%. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.",,,,ACL
177,2018,Harvesting Paragraph-level Question-Answer Pairs from Wikipedia,"Xinya Du, Claire Cardie","We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia.",,,,ACL
178,2018,Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification,"Yizhong Wang, Kai Liu, Jing Liu, Wei He","Machine reading comprehension (MRC) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine. Compared with MRC on a single passage, multi-passage MRC is more challenging, since we are likely to get multiple confusing answer candidates from different passages. To address this problem, we propose an end-to-end neural model that enables those answer candidates from different passages to verify each other based on their content representations. Specifically, we jointly train three modules that can predict the final answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification. The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset, both of which are designed for MRC in real-world settings.",,,,ACL
179,2018,Language Generation via DAG Transduction,"Yajie Ye, Weiwei Sun, Xiaojun Wan","A DAG automaton is a formal device for manipulating graphs. By augmenting a DAG automaton with transduction rules, a DAG transducer has potential applications in fundamental NLP tasks. In this paper, we propose a novel DAG transducer to perform graph-to-program transformation. The target structure of our transducer is a program licensed by a declarative programming language rather than linguistic structures. By executing such a program, we can easily get a surface string. Our transducer is designed especially for natural language generation (NLG) from type-logical semantic graphs. Taking Elementary Dependency Structures, a format of English Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design.",,,,ACL
180,2018,A Distributional and Orthographic Aggregation Model for English Derivational Morphology,"Daniel Deutsch, John Hewitt, Dan Roth","Modeling derivational morphology to generate words with particular semantics is useful in many text generation tasks, such as machine translation or abstractive question answering. In this work, we tackle the task of derived word generation. That is, we attempt to generate the word “runner” for “someone who runs.” We identify two key problems in generating derived words from root words and transformations. We contribute a novel aggregation model of derived word generation that learns derivational transformations both as orthographic functions using sequence-to-sequence models and as functions in distributional word embedding space. The model then learns to choose between the hypothesis of each system. We also present two ways of incorporating corpus information into derived word generation.",,,,ACL
181,2018,"Deep-speare: A joint neural model of poetic language, meter and rhyme","Jey Han Lau, Trevor Cohn, Timothy Baldwin, Julian Brooke","In this paper, we propose a joint architecture that captures language, rhyme and meter for sonnet modelling. We assess the quality of generated poems using crowd and expert judgements. The stress and rhyme models perform very well, as generated poems are largely indistinguishable from human-written poems. Expert evaluation, however, reveals that a vanilla language model captures meter implicitly, and that machine-generated poems still underperform in terms of readability and emotion. Our research shows the importance expert evaluation for poetry generation, and that future research should look beyond rhyme/meter and focus on poetic language.",,,,ACL
182,2018,NeuralREG: An end-to-end approach to referring expression generation,"Thiago Castro Ferreira, Diego Moussallem, Ákos Kádár, Sander Wubben","Traditionally, Referring Expression Generation (REG) models first decide on the form and then on the content of references to discourse entities in text, typically relying on features such as salience and grammatical function. In this paper, we present a new approach (NeuralREG), relying on deep neural networks, which makes decisions about form and content in one go without explicit feature extraction. Using a delexicalized version of the WebNLG corpus, we show that the neural model substantially improves over two strong baselines.",,,,ACL
183,2018,Stock Movement Prediction from Tweets and Historical Prices,"Yumo Xu, Shay B. Cohen","Stock movement prediction is a challenging problem: the market is highly stochastic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed model on a new stock movement prediction dataset which we collected.",,,,ACL
184,2018,Rumor Detection on Twitter with Tree-structured Recursive Neural Networks,"Jing Ma, Wei Gao, Kam-Fai Wong","Automatic rumor detection is technically very challenging. In this work, we try to learn discriminative features from tweets content by following their non-sequential propagation structure and generate more powerful representations for identifying different type of rumors. We propose two recursive neural models based on a bottom-up and a top-down tree-structured neural networks for rumor representation learning and classification, which naturally conform to the propagation layout of tweets. Results on two public Twitter datasets demonstrate that our recursive neural models 1) achieve much better performance than state-of-the-art approaches; 2) demonstrate superior capacity on detecting rumors at very early stage.",,,,ACL
185,2018,Visual Attention Model for Name Tagging in Multimodal Social Media,"Di Lu, Leonardo Neves, Vitor Carvalho, Ning Zhang","Everyday billions of multimodal posts containing both images and text are shared in social media sites such as Snapchat, Twitter or Instagram. This combination of image and text in a single message allows for more creative and expressive forms of communication, and has become increasingly common in such sites. This new paradigm brings new challenges for natural language understanding, as the textual component tends to be shorter, more informal, and often is only understood if combined with the visual context. In this paper, we explore the task of name tagging in multimodal social media posts. We start by creating two new multimodal datasets: the first based on Twitter posts and the second based on Snapchat captions (exclusively submitted to public and crowd-sourced stories). We then propose a novel model architecture based on Visual Attention that not only provides deeper visual understanding on the decisions of the model, but also significantly outperforms other state-of-the-art baseline methods for this task.",,,,ACL
186,2018,Multimodal Named Entity Disambiguation for Noisy Social Media Posts,"Seungwhan Moon, Leonardo Neves, Vitor Carvalho","We introduce the new Multimodal Named Entity Disambiguation (MNED) task for multimodal social media posts such as Snapchat or Instagram captions, which are composed of short captions with accompanying images. Social media posts bring significant challenges for disambiguation tasks because 1) ambiguity not only comes from polysemous entities, but also from inconsistent or incomplete notations, 2) very limited context is provided with surrounding words, and 3) there are many emerging entities often unseen during training. To this end, we build a new dataset called SnapCaptionsKB, a collection of Snapchat image captions submitted to public and crowd-sourced stories, with named entity mentions fully annotated and linked to entities in an external knowledge base. We then build a deep zeroshot multimodal network for MNED that 1) extracts contexts from both text and image, and 2) predicts correct entity in the knowledge graph embeddings space, allowing for zeroshot disambiguation of entities unseen in training set as well. The proposed model significantly outperforms the state-of-the-art text-only NED models, showing efficacy and potentials of the MNED task.",,,,ACL
187,2018,Semi-supervised User Geolocation via Graph Convolutional Networks,"Afshin Rahimi, Trevor Cohn, Timothy Baldwin","Social media user geolocation is vital to many applications such as event detection. In this paper, we propose GCN, a multiview geolocation model based on Graph Convolutional Networks, that uses both text and network context. We compare GCN to the state-of-the-art, and to two baselines we propose, and show that our model achieves or is competitive with the state-of-the-art over three benchmark geolocation datasets when sufficient supervision is available. We also evaluate GCN under a minimal supervision scenario, and show it outperforms baselines. We find that highway network gates are essential for controlling the amount of useful neighbourhood expansion in GCN.",,,,ACL
188,2018,Document Modeling with External Attention for Sentence Extraction,"Shashi Narayan, Ronald Cardenas, Nikos Papasarantopoulos, Shay B. Cohen","Document modeling is essential to a variety of natural language understanding tasks. We propose to use external information to improve document modeling for problems that can be framed as sentence extraction. We develop a framework composed of a hierarchical document encoder and an attention-based extractor with attention over external information. We evaluate our model on extractive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question). We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.",,,,ACL
189,2018,Neural Models for Documents with Metadata,"Dallas Card, Chenhao Tan, Noah A. Smith","Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.",,,,ACL
190,2018,NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing,"Dinghan Shen, Qinliang Su, Paidamoyo Chapfuwa, Wenlin Wang","Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled ad-hoc. In this paper, we present an end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary hashing codes are treated as Bernoulli latent variables. A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent variable to optimize the hash function. We also draw the connections between proposed method and rate-distortion theory, which provides a theoretical foundation for the effectiveness of our framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsupervised and supervised scenarios.",,,,ACL
191,2018,Large-Scale QA-SRL Parsing,"Nicholas FitzGerald, Julian Michael, Luheng He, Luke Zettlemoyer","We present a new large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations, and the first high-quality QA-SRL parser. Our corpus, QA-SRL Bank 2.0, consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with a new crowd-sourcing scheme that we show has high precision and good recall at modest cost. We also present neural models for two QA-SRL subtasks: detecting argument spans for a predicate and generating questions to label the semantic relationship. The best models achieve question accuracy of 82.6% and span-level accuracy of 77.6% (under human evaluation) on the full pipelined QA-SRL prediction task. They can also, as we show, be used to gather additional annotations at low cost.",,,,ACL
192,2018,"Syntax for Semantic Role Labeling, To Be, Or Not To Be","Shexia He, Zuchao Li, Hai Zhao, Hongxiao Bai","Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence. Previous studies have shown syntactic information has a remarkable contribution to SRL performance. However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone. This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework. We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information. Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both English and Chinese, showing the quantitative significance of syntax to neural SRL together with a thorough empirical survey over existing models.",,,,ACL
193,2018,Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation,"Alane Suhr, Yoav Artzi","We propose a learning approach for mapping context-dependent sequential instructions to actions. We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world. To train from start and goal states without access to demonstrations, we propose SESTRA, a learning algorithm that takes advantage of single-step reward observations and immediate expected reward maximization. We evaluate on the SCONE domains, and show absolute accuracy improvements of 9.8%-25.3% across the domains over approaches that use high-level logical representations.",,,,ACL
194,2018,Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding,"Bingfeng Luo, Yansong Feng, Zheng Wang, Songfang Huang","The success of many natural language processing (NLP) tasks is bound by the number and quality of annotated data, but there is often a shortage of such training data. In this paper, we ask the question: “Can we combine a neural network (NN) with regular expressions (RE) to improve supervised learning for NLP?”. In answer, we develop novel methods to exploit the rich expressiveness of REs at different levels within a NN, showing that the combination significantly enhances the learning effectiveness when a small number of training examples are available. We evaluate our approach by applying it to spoken language understanding for intent detection and slot filling. Experimental results show that our approach is highly effective in exploiting the available training data, giving a clear boost to the RE-unaware NN.",,,,ACL
195,2018,Token-level and sequence-level loss smoothing for RNN language models,"Maha Elbayad, Laurent Besacier, Jakob Verbeek","Despite the effectiveness of recurrent neural network language models, their maximum likelihood estimation suffers from two limitations. It treats all sentences that do not match the ground truth as equally poor, ignoring the structure of the output space. Second, it suffers from ’exposure bias’: during training tokens are predicted given ground-truth sequences, while at test time prediction is conditioned on generated output sequences. To overcome these limitations we build upon the recent reward augmented maximum likelihood approach that encourages the model to predict sentences that are close to the ground truth according to a given performance metric. We extend this approach to token-level loss smoothing, and propose improvements to the sequence-level smoothing approach. Our experiments on two different tasks, image captioning and machine translation, show that token-level and sequence-level loss smoothing are complementary, and significantly improve results.",,,,ACL
196,2018,Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers,"Georgios Spithourakis, Sebastian Riedel","Numeracy is the ability to understand and work with numbers. It is a necessary skill for composing and understanding documents in clinical, scientific, and other technical domains. In this paper, we explore different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary. Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude, respectively, over non-hierarchical models. A combination of strategies can further improve perplexity. Our continuous probability density function model reduces mean absolute percentage errors by 18% and 54% in comparison to the second best strategy for each dataset, respectively.",,,,ACL
197,2018,To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness,"Amulya Gupta, Zhu Zhang","With the recent success of Recurrent Neural Networks (RNNs) in Machine Translation (MT), attention mechanisms have become increasingly popular. The purpose of this paper is two-fold; firstly, we propose a novel attention model on Tree Long Short-Term Memory Networks (Tree-LSTMs), a tree-structured generalization of standard LSTM. Secondly, we study the interaction between attention and syntactic structures, by experimenting with three LSTM variants: bidirectional-LSTMs, Constituency Tree-LSTMs, and Dependency Tree-LSTMs. Our models are evaluated on two semantic relatedness tasks: semantic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and paraphrase detection for question pairs (Quora, 2017).",,,,ACL
198,2018,What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties,"Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault","Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. “Downstream” tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.",,,,ACL
199,2018,Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning,"Pengda Qin, Weiran Xu, William Yang Wang","Distant supervision has become the standard method for relation extraction. However, even though it is an efficient method, it does not come at no cost—The resulted distantly-supervised training samples are often very noisy. To combat the noise, most of the recent state-of-the-art approaches focus on selecting one-best sentence or calculating soft attention weights over the set of the sentences of one specific entity pair. However, these methods are suboptimal, and the false positive problem is still a key stumbling bottleneck for the performance. We argue that those incorrectly-labeled candidate sentences must be treated with a hard decision, rather than being dealt with soft attention weights. To do this, our paper describes a radical solution—We explore a deep reinforcement learning strategy to generate the false-positive indicator, where we automatically recognize false positives for each relation type without any supervised information. Unlike the removal operation in the previous studies, we redistribute them into the negative examples. The experimental results show that the proposed strategy significantly improves the performance of distant supervision comparing to state-of-the-art systems.",,,,ACL
200,2018,Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder,"Ryo Takahashi, Ran Tian, Kentaro Inui","Embedding models for entities and relations are extremely useful for recovering missing facts in a knowledge base. Intuitively, a relation can be modeled by a matrix mapping entity vectors. However, relations reside on low dimension sub-manifolds in the parameter space of arbitrary matrices – for one reason, composition of two relations M1, M2 may match a third M3 (e.g. composition of relations currency_of_country and country_of_film usually matches currency_of_film_budget), which imposes compositional constraints to be satisfied by the parameters (i.e. M1*M2=M3). In this paper we investigate a dimension reduction technique by training relations jointly with an autoencoder, which is expected to better capture compositional constraints. We achieve state-of-the-art on Knowledge Base Completion tasks with strongly improved Mean Rank, and show that joint training with an autoencoder leads to interpretable sparse codings of relations, helps discovering compositional constraints and benefits from compositional training. Our source code is released at github.com/tianran/glimvec.",,,,ACL
201,2018,Zero-Shot Transfer Learning for Event Extraction,"Lifu Huang, Heng Ji, Kyunghyun Cho, Ido Dagan","Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.",,,,ACL
202,2018,Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction,"Wenya Wang, Sinno Jialin Pan","Fine-grained opinion analysis aims to extract aspect and opinion terms from each sentence for opinion summarization. Supervised learning methods have proven to be effective for this task. However, in many domains, the lack of labeled data hinders the learning of a precise extraction model. In this case, unsupervised domain adaptation methods are desired to transfer knowledge from the source domain to any unlabeled target domain. In this paper, we develop a novel recursive neural network that could reduce domain shift effectively in word level through syntactic relations. We treat these relations as invariant “pivot information” across domains to build structural correspondences and generate an auxiliary task to predict the relation between any two adjacent words in the dependency tree. In the end, we demonstrate state-of-the-art results on three benchmark datasets.",,,,ACL
203,2018,Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning,"Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu","Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.",,,,ACL
204,2018,Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders,"Yansen Wang, Chenyi Liu, Minlie Huang, Liqiang Nie","Asking good questions in open-domain conversational systems is quite significant but rather untouched. This task, substantially different from traditional question generation, requires to question not only with various patterns but also on diverse and relevant topics. We observe that a good question is a natural composition of interrogatives, topic words, and ordinary words. Interrogatives lexicalize the pattern of questioning, topic words address the key information for topic transition in dialogue, and ordinary words play syntactical and grammatical roles in making a natural sentence. We devise two typed decoders (soft typed decoder and hard typed decoder) in which a type distribution over the three types is estimated and the type distribution is used to modulate the final generation distribution. Extensive experiments show that the typed decoders outperform state-of-the-art baselines and can generate more meaningful questions.",,,,ACL
205,2018,"Personalizing Dialogue Agents: I have a dog, do you have pets too?","Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam","Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",,,,ACL
206,2018,Efficient Large-Scale Neural Domain Classification with Personalized Attention,"Young-Bum Kim, Dongchan Kim, Anjishnu Kumar, Ruhi Sarikaya","In this paper, we explore the task of mapping spoken language utterances to one of thousands of natural language understanding domains in intelligent personal digital assistants (IPDAs). This scenario is observed in mainstream IPDAs in industry that allow third parties to develop thousands of new domains to augment built-in first party domains to rapidly increase domain coverage and overall IPDA capabilities. We propose a scalable neural model architecture with a shared encoder, a novel attention mechanism that incorporates personalization information and domain-specific classifiers that solves the problem efficiently. Our architecture is designed to efficiently accommodate incremental domain additions achieving two orders of magnitude speed up compared to full model retraining. We consider the practical constraints of real-time production systems, and design to minimize memory footprint and runtime latency. We demonstrate that incorporating personalization significantly improves domain classification accuracy in a setting with thousands of overlapping domains.",,,,ACL
207,2018,Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment,"Yue Gu, Kangning Yang, Shiyu Fu, Shuhong Chen","Multimodal affective computing, learning to recognize and interpret human affect and subjective information from multiple data sources, is still a challenge because: (i) it is hard to extract informative features to represent human affects from heterogeneous inputs; (ii) current fusion strategies only fuse different modalities at abstract levels, ignoring time-dependent interactions between modalities. Addressing such issues, we introduce a hierarchical multimodal architecture with attention and word-level fusion to classify utterance-level sentiment and emotion from text and audio data. Our introduced model outperforms state-of-the-art approaches on published datasets, and we demonstrate that our model is able to visualize and interpret synchronized attention over modalities.",,,,ACL
208,2018,Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph,"AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria","Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.",,,,ACL
209,2018,Efficient Low-rank Multimodal Fusion With Modality-Specific Factors,"Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang","Multimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is multimodal fusion. The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation. Previous research in this field has exploited the expressiveness of tensors for multimodal representation. However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor. In this paper, we propose the Low-rank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency. We evaluate our model on three different tasks: multimodal sentiment analysis, speaker trait analysis, and emotion recognition. Our model achieves competitive results on all these tasks while drastically reducing computational complexity. Additional experiments also show that our model can perform robustly for a wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations.",,,,ACL
210,2018,Discourse Coherence: Concurrent Explicit and Implicit Relations,"Hannah Rohde, Alexander Johnson, Nathan Schneider, Bonnie Webber","Theories of discourse coherence posit relations between discourse segments as a key feature of coherent text. Our prior work suggests that multiple discourse relations can be simultaneously operative between two segments for reasons not predicted by the literature. Here we test how this joint presence can lead participants to endorse seemingly divergent conjunctions (e.g., BUT and SO) to express the link they see between two segments. These apparent divergences are not symptomatic of participant naivety or bias, but arise reliably from the concurrent availability of multiple relations between segments – some available through explicit signals and some via inference. We believe that these new results can both inform future progress in theoretical work on discourse coherence and lead to higher levels of performance in discourse parsing.",,,,ACL
211,2018,A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text,"Shashank Srivastava, Nebojsa Jojic","We present a generative probabilistic model of documents as sequences of sentences, and show that inference in it can lead to extraction of long-range latent discourse structure from a collection of documents. The approach is based on embedding sequences of sentences from longer texts into a 2- or 3-D spatial grids, in which one or two coordinates model smooth topic transitions, while the third captures the sequential nature of the modeled text. A significant advantage of our approach is that the learned models are naturally visualizable and interpretable, as semantic similarity and sequential structure are modeled along orthogonal directions in the grid. We show that the method is effective in capturing discourse structures in narrative text across multiple genres, including biographies, stories, and newswire reports. In particular, our method outperforms or is competitive with state-of-the-art generative approaches on tasks such as predicting the outcome of a story, and sentence ordering.",,,,ACL
212,2018,Joint Reasoning for Temporal and Causal Relations,"Qiang Ning, Zhili Feng, Hao Wu, Dan Roth","Understanding temporal and causal relations between events is a fundamental natural language understanding task. Because a cause must occur earlier than its effect, temporal and causal relations are closely related and one relation often dictates the value of the other. However, limited attention has been paid to studying these two relations jointly. This paper presents a joint inference framework for them using constrained conditional models (CCMs). Specifically, we formulate the joint problem as an integer linear programming (ILP) problem, enforcing constraints that are inherent in the nature of time and causality. We show that the joint inference framework results in statistically significant improvement in the extraction of both temporal and causal relations from text.",,,,ACL
213,2018,Modeling Naive Psychology of Characters in Simple Commonsense Stories,"Hannah Rashkin, Antoine Bosselut, Maarten Sap, Kevin Knight","Understanding a narrative requires reading between the lines and reasoning about the unspoken but obvious implications about events and people’s mental states — a capability that is trivial for humans but remarkably hard for machines. To facilitate research addressing this challenge, we introduce a new annotation framework to explain naive psychology of story characters as fully-specified chains of mental states with respect to motivations and emotional reactions. Our work presents a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.",,,,ACL
214,2018,A Deep Relevance Model for Zero-Shot Document Filtering,"Chenliang Li, Wei Zhou, Feng Ji, Yu Duan","In the era of big data, focused analysis for diverse topics with a short response time becomes an urgent demand. As a fundamental task, information filtering therefore becomes a critical necessity. In this paper, we propose a novel deep relevance model for zero-shot document filtering, named DAZER. DAZER estimates the relevance between a document and a category by taking a small set of seed words relevant to the category. With pre-trained word embeddings from a large external corpus, DAZER is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process. The gate mechanism controls which convolution filters output the relevance signals in a category dependent manner. Experiments on two document collections of two different tasks (i.e., topic categorization and sentiment analysis) demonstrate that DAZER significantly outperforms the existing alternative solutions, including the state-of-the-art deep relevance ranking models.",,,,ACL
215,2018,Disconnected Recurrent Neural Networks for Text Categorization,Baoxin Wang,"Recurrent neural network (RNN) has achieved remarkable performance in text categorization. RNN can model the entire sequence and capture long-term dependencies, but it does not do well in extracting key patterns. In contrast, convolutional neural network (CNN) is good at extracting local and position-invariant features. In this paper, we present a novel model named disconnected recurrent neural network (DRNN), which incorporates position-invariance into RNN. By limiting the distance of information flow in RNN, the hidden state at each time step is restricted to represent words near the current position. The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text categorization.",,,,ACL
216,2018,Joint Embedding of Words and Labels for Text Classification,"Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe Zhang","Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels. The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both accuracy and speed.",,,,ACL
217,2018,Neural Sparse Topical Coding,"Min Peng, Qianqian Xie, Yanchun Zhang, Hua Wang","Topic models with sparsity enhancement have been proven to be effective at learning discriminative and coherent latent topics of short texts, which is critical to many scientific and engineering applications. However, the extensions of these models require carefully tailored graphical models and re-deduced inference algorithms, limiting their variations and applications. We propose a novel sparsity-enhanced topic model, Neural Sparse Topical Coding (NSTC) base on a sparsity-enhanced topic model called Sparse Topical Coding (STC). It focuses on replacing the complex inference process with the back propagation, which makes the model easy to explore extensions. Moreover, the external semantic information of words in word embeddings is incorporated to improve the representation of short texts. To illustrate the flexibility offered by the neural network based framework, we present three extensions base on NSTC without re-deduced inference algorithms. Experiments on Web Snippet and 20Newsgroups datasets demonstrate that our models outperform existing methods.",,,,ACL
218,2018,Document Similarity for Texts of Varying Lengths via Hidden Topics,"Hongyu Gong, Tarek Sakakini, Suma Bhat, JinJun Xiong","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",,,,ACL
219,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,"Sandeep Mathias, Diptesh Kanojia, Kevin Patel, Samarth Agrawal","Predicting a reader’s rating of text quality is a challenging task that involves estimating different subjective aspects of the text, like structure, clarity, etc. Such subjective aspects are better handled using cognitive information. One such source of cognitive information is gaze behaviour. In this paper, we show that gaze behaviour does indeed help in effectively predicting the rating of text quality. To do this, we first we model text quality as a function of three properties - organization, coherence and cohesion. Then, we demonstrate how capturing gaze behaviour helps in predicting each of these properties, and hence the overall quality, by reporting improvements obtained by adding gaze features to traditional textual features for score prediction. We also hypothesize that if a reader has fully understood the text, the corresponding gaze behaviour would give a better indication of the assigned rating, as opposed to partial understanding. Our experiments validate this hypothesis by showing greater agreement between the given rating and the predicted rating when the reader has a full understanding of the text.",,,,ACL
220,2018,Multi-Input Attention for Unsupervised OCR Correction,"Rui Dong, David Smith","We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for unsupervised training and as a source of evidence when decoding. A sequence-to-sequence model with attention is applied for single-input correction, and a new decoder with multi-input attention averaging is developed to search for consensus among multiple sequences. We design two ways of training the correction model without human annotation, either training to match noisily observed textual variants or bootstrapping from a uniform error model. On two corpora of historical newspapers and books, we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and, with the addition of multi-input decoding, can rival supervised methods.",,,,ACL
221,2018,Building Language Models for Text with Named Entities,"Md Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang","Text in many domains involves a significant amount of named entities. Predicting the entity names is often challenging for a language model as they appear less frequent on the training corpus. In this paper, we propose a novel and effective approach to building a language model which can learn the entity names by leveraging their entity type information. We also introduce two benchmark datasets based on recipes and Java programming codes, on which we evaluate the proposed model. Experimental results show that our model achieves 52.2% better perplexity in recipe generation and 22.06% on code generation than state-of-the-art language models.",,,,ACL
222,2018,hyperdoc2vec: Distributed Representations of Hypertext Documents,"Jialong Han, Yan Song, Wayne Xin Zhao, Shuming Shi","Hypertext documents, such as web pages and academic papers, are of great importance in delivering information in our daily life. Although being effective on plain documents, conventional text embedding methods suffer from information loss if directly adapted to hyper-documents. In this paper, we propose a general embedding approach for hyper-documents, namely, hyperdoc2vec, along with four criteria characterizing necessary information that hyper-document embedding models should preserve. Systematic comparisons are conducted between hyperdoc2vec and several competitors on two tasks, i.e., paper classification and citation recommendation, in the academic paper domain. Analyses and experiments both validate the superiority of hyperdoc2vec to other models w.r.t. the four criteria.",,,,ACL
223,2018,Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval,"Zhenghao Liu, Chenyan Xiong, Maosong Sun, Zhiyuan Liu","This paper presents the Entity-Duet Neural Ranking Model (EDRM), which introduces knowledge graphs to neural search systems. EDRM represents queries and documents by their words and entity annotations. The semantics from knowledge graphs are integrated in the distributed representations of their entities, while the ranking is conducted by interaction-based neural ranking networks. The two components are learned end-to-end, making EDRM a natural combination of entity-oriented search and neural information retrieval. Our experiments on a commercial search log demonstrate the effectiveness of EDRM. Our analyses reveal that knowledge graph semantics significantly improve the generalization ability of neural ranking models.",,,,ACL
224,2018,Neural Natural Language Inference Models Enhanced with External Knowledge,"Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana Inkpen","Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these data? If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets.",,,,ACL
225,2018,AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples,"Dongyeop Kang, Tushar Khot, Ashish Sabharwal, Eduard Hovy","We consider the problem of learning textual entailment models with limited supervision (5K-10K training examples), and present two complementary approaches for it. First, we propose knowledge-guided adversarial example generators for incorporating large lexical resources in entailment models via only a handful of rule templates. Second, to make the entailment model—a discriminator—more robust, we propose the first GAN-style approach for training it using a natural language example generator that iteratively adjusts to the discriminator’s weaknesses. We demonstrate effectiveness using two entailment datasets, where the proposed methods increase accuracy by 4.7% on SciTail and by 2.8% on a 1% sub-sample of SNLI. Notably, even a single hand-written rule, negate, improves the accuracy of negation examples in SNLI by 6.1%.",,,,ACL
226,2018,Subword-level Word Vector Representations for Korean,"Sungjoon Park, Jeongmin Byun, Sion Baek, Yongseok Cho","Research on distributed word representations is focused on widely-used languages such as English. Although the same methods can be used for other languages, language-specific knowledge can enhance the accuracy and richness of word vector representations. In this paper, we look at improving distributed word representations for Korean using knowledge about the unique linguistic structure of Korean. Specifically, we decompose Korean words into the jamo-level, beyond the character-level, allowing a systematic use of subword information. To evaluate the vectors, we develop Korean test sets for word similarity and analogy and make them publicly available. The results show that our simple method outperforms word2vec and character-level Skip-Grams on semantic and syntactic similarity and analogy tasks and contributes positively toward downstream NLP tasks such as sentiment analysis.",,,,ACL
227,2018,Incorporating Chinese Characters of Words for Lexical Sememe Prediction,"Huiming Jin, Hao Zhu, Zhiyuan Liu, Ruobing Xie","Sememes are minimum semantic units of concepts in human languages, such that each word sense is composed of one or multiple sememes. Words are usually manually annotated with their sememes by linguists, and form linguistic common-sense knowledge bases widely used in various NLP tasks. Recently, the lexical sememe prediction task has been introduced. It consists of automatically recommending sememes for words, which is expected to improve annotation efficiency and consistency. However, existing methods of lexical sememe prediction typically rely on the external context of words to represent the meaning, which usually fails to deal with low-frequency and out-of-vocabulary words. To address this issue for Chinese, we propose a novel framework to take advantage of both internal character information and external context information of words. We experiment on HowNet, a Chinese sememe knowledge base, and demonstrate that our framework outperforms state-of-the-art baselines by a large margin, and maintains a robust performance even for low-frequency words.",,,,ACL
228,2018,SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment,"Jisun An, Haewoon Kwak, Yong-Yeol Ahn","Because word semantics can substantially change across communities and contexts, capturing domain-specific word semantics is an important challenge. Here, we propose SemAxis, a simple yet powerful framework to characterize word semantics using many semantic axes in word-vector spaces beyond sentiment. We demonstrate that SemAxis can capture nuanced semantic representations in multiple online communities. We also show that, when the sentiment axis is examined, SemAxis outperforms the state-of-the-art approaches in building domain-specific sentiment lexicons.",,,,ACL
229,2018,End-to-End Reinforcement Learning for Automatic Taxonomy Induction,"Yuning Mao, Xiang Ren, Jiaming Shen, Xiaotao Gu","We present a novel end-to-end reinforcement learning approach to automatic taxonomy induction from a set of terms. While prior methods treat the problem as a two-phase task (i.e.,, detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy), we argue that such two-phase methods may suffer from error propagation, and cannot effectively optimize metrics that capture the holistic structure of a taxonomy. In our approach, the representations of term pairs are learned using multiple sources of information and used to determine which term to select and where to place it on the taxonomy via a policy network. All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training taxonomies. Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6% on ancestor F1.",,,,ACL
230,2018,Incorporating Glosses into Neural Word Sense Disambiguation,"Fuli Luo, Tianyu Liu, Qiaolin Xia, Baobao Chang","Word Sense Disambiguation (WSD) aims to identify the correct meaning of polysemous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-the-art systems on several English all-words WSD datasets.",,,,ACL
231,2018,Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages,"Jeremy Barnes, Roman Klinger, Sabine Schulte im Walde","Sentiment analysis in low-resource languages suffers from a lack of annotated corpora to estimate high-performing models. Machine translation and bilingual word embeddings provide some relief through cross-lingual sentiment approaches. However, they either require large amounts of parallel data or do not sufficiently capture sentiment information. We introduce Bilingual Sentiment Embeddings (BLSE), which jointly represent sentiment information in a source and target language. This model only requires a small bilingual lexicon, a source-language corpus annotated for sentiment, and monolingual word embeddings for each language. We perform experiments on three language combinations (Spanish, Catalan, Basque) for sentence-level cross-lingual sentiment classification and find that our model significantly outperforms state-of-the-art methods on four out of six experimental setups, as well as capturing complementary information to machine translation. Our analysis of the resulting embedding space provides evidence that it represents sentiment information in the resource-poor target language without any annotated data in that language.",,,,ACL
232,2018,Learning Domain-Sensitive and Sentiment-Aware Word Embeddings,"Bei Shi, Zihao Fu, Lidong Bing, Wai Lam","Word embeddings have been widely used in sentiment classification because of their efficacy for semantic representations of words. Given reviews from different domains, some existing methods for word embeddings exploit sentiment information, but they cannot produce domain-sensitive embeddings. On the other hand, some other existing methods can generate domain-sensitive word embeddings, but they cannot distinguish words with similar contexts but opposite sentiment polarity. We propose a new method for learning domain-sensitive and sentiment-aware embeddings that simultaneously capture the information of sentiment semantics and domain sensitivity of individual words. Our method can automatically determine and produce domain-common embeddings and domain-specific embeddings. The differentiation of domain-common and domain-specific words enables the advantage of data augmentation of common semantics from multiple domains and capture the varied semantics of specific words from different domains at the same time. Experimental results show that our model provides an effective way to learn domain-sensitive and sentiment-aware word embeddings which benefit sentiment classification at both sentence level and lexicon term level.",,,,ACL
233,2018,Cross-Domain Sentiment Classification with Target Domain Specific Information,"Minlong Peng, Qi Zhang, Yu-gang Jiang, Xuanjing Huang","The task of adopting a model with good performance to a target domain that is different from the source domain used for training has received considerable attention in sentiment analysis. Most existing approaches mainly focus on learning representations that are domain-invariant in both the source and target domains. Few of them pay attention to domain-specific information, which should also be informative. In this work, we propose a method to simultaneously extract domain specific and invariant representations and train a classifier on each of the representation, respectively. And we introduce a few target domain labeled data for learning domain-specific information. To effectively utilize the target domain labeled data, we train the domain invariant representation based classifier with both the source and target domain labeled data and train the domain-specific representation based classifier with only the target domain labeled data. These two classifiers then boost each other in a co-training style. Extensive sentiment analysis experiments demonstrated that the proposed method could achieve better performance than state-of-the-art methods.",,,,ACL
234,2018,Aspect Based Sentiment Analysis with Gated Convolutional Networks,"Wei Xue, Tao Li","Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a model based on convolutional neural networks and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity. The architecture is much simpler than attention layer used in the existing models. Second, the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models.",,,,ACL
235,2018,A Helping Hand: Transfer Learning for Deep Sentiment Analysis,"Xin Dong, Gerard de Melo","Deep convolutional neural networks excel at sentiment polarity classification, but tend to require substantial amounts of training data, which moreover differs quite significantly between domains. In this work, we present an approach to feed generic cues into the training process of such networks, leading to better generalization abilities given limited training data. We propose to induce sentiment embeddings via supervision on extrinsic data, which are then fed into the model via a dedicated memory-based component. We observe significant gains in effectiveness on a range of different datasets in seven different languages.",,,,ACL
236,2018,Cold-Start Aware User and Product Attention for Sentiment Classification,"Reinald Kim Amplayo, Jihyeok Kim, Sua Sung, Seung-won Hwang","The use of user/product information in sentiment analysis is important, especially for cold-start users/products, whose number of reviews are very limited. However, current models do not deal with the cold-start problem which is typical in review websites. In this paper, we present Hybrid Contextualized Sentiment Classifier (HCSC), which contains two modules: (1) a fast word encoder that returns word vectors embedded with short and long range dependency features; and (2) Cold-Start Aware Attention (CSAA), an attention mechanism that considers the existence of cold-start problem when attentively pooling the encoded word vectors. HCSC introduces shared vectors that are constructed from similar users/products, and are used when the original distinct vectors do not have sufficient information (i.e. cold-start). This is decided by a frequency-guided selective gate vector. Our experiments show that in terms of RMSE, HCSC performs significantly better when compared with on famous datasets, despite having less complexity, and thus can be trained much faster. More importantly, our model performs significantly better than previous models when the training data is sparse and has cold-start problems.",,,,ACL
237,2018,Modeling Deliberative Argumentation Strategies on Wikipedia,"Khalid Al-Khatib, Henning Wachsmuth, Kevin Lang, Jakob Herpel","This paper studies how the argumentation strategies of participants in deliberative discussions can be supported computationally. Our ultimate goal is to predict the best next deliberative move of each participant. In this paper, we present a model for deliberative discussions and we illustrate its operationalization. Previous models have been built manually based on a small set of discussions, resulting in a level of abstraction that is not suitable for move recommendation. In contrast, we derive our model statistically from several types of metadata that can be used for move description. Applied to six million discussions from Wikipedia talk pages, our approach results in a model with 13 categories along three dimensions: discourse acts, argumentative relations, and frames. On this basis, we automatically generate a corpus with about 200,000 turns, labeled for the 13 categories. We then operationalize the model with three supervised classifiers and provide evidence that the proposed categories can be predicted.",,,,ACL
238,2018,"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning","Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut","We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",,,,ACL
239,2018,Learning Translations via Images with a Massively Multilingual Image Dataset,"John Hewitt, Daphne Ippolito, Brendan Callahan, Reno Kriz","We conduct the most comprehensive study to date into translating words via images. To facilitate research on the task, we introduce a large-scale multilingual corpus of images, each labeled with the word it represents. Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings. In contrast, we have collected by far the largest available dataset for this task, with images for approximately 10,000 words in each of 100 languages. We run experiments on a dozen high resource languages and 20 low resources languages, demonstrating the effect of word concreteness and part-of-speech on translation quality. %We find that while image features work best for concrete nouns, they are sometimes effective on other parts of speech. To improve image-based translation, we introduce a novel method of predicting word concreteness from images, which improves on a previous state-of-the-art unsupervised technique. This allows us to predict when image-based translation may be effective, enabling consistent improvements to a state-of-the-art text-based word translation system. Our code and the Massively Multilingual Image Dataset (MMID) are available at http://multilingual-images.org/.",,,,ACL
240,2018,On the Automatic Generation of Medical Imaging Reports,"Baoyu Jing, Pengtao Xie, Eric Xing","Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time-consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the reports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the prediction of tags and the generation of paragraphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available dataset.",,,,ACL
241,2018,Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning,"Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi","Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning. The proposed algorithm provides two evaluation approaches, which check if we can mislead neural image captioning systems to output some randomly chosen captions or keywords. Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords, and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.",,,,ACL
242,2018,Think Visually: Question Answering through Virtual Imagery,"Ankit Goyal, Jian Wang, Jia Deng","In this paper, we study the problem of geometric reasoning (a form of visual reasoning) in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture that specializes in answering questions that admit latent visual representations, and learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems. Experimental results validate the effectiveness of our proposed DSMN for visual thinking tasks.",,,,ACL
243,2018,Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game,"Haichao Zhang, Haonan Yu, Wei Xu","Building intelligent agents that can communicate with and learn from humans in natural language is of great value. Supervised language learning is limited by the ability of capturing mainly the statistics of training data, and is hardly adaptive to new scenarios or flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting. We highlight the perspective that conversational interaction serves as a natural interface both for language learning and for novel knowledge acquisition and propose a joint imitation and reinforcement approach for grounded language learning through an interactive conversational game. The agent trained with this approach is able to actively acquire information by asking questions about novel objects and use the just-learned knowledge in subsequent conversations in a one-shot fashion. Results compared with other methods verified the effectiveness of the proposed approach.",,,,ACL
244,2018,A Purely End-to-End System for Multi-speaker Speech Recognition,"Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Jonathan Le Roux","Recently, there has been growing interest in multi-speaker speech recognition, where the utterances of multiple speakers are recognized from their mixture. Promising techniques have been proposed for this task, but earlier works have required additional training data such as isolated source signals or senone alignments for effective learning. In this paper, we propose a new sequence-to-sequence framework to directly decode multiple label sequences from a single speech sequence by unifying source separation and speech recognition functions in an end-to-end manner. We further propose a new objective function to improve the contrast between the hidden vectors to avoid generating similar hypotheses. Experimental results show that the model is directly able to learn a mapping from a speech mixture to multiple label sequences, achieving 83.1% relative improvement compared to a model trained without the proposed objective. Interestingly, the results are comparable to those produced by previous end-to-end works featuring explicit separation and recognition modules.",,,,ACL
245,2018,A Structured Variational Autoencoder for Contextual Morphological Inflection,"Lawrence Wolf-Sonkin, Jason Naradowsky, Sebastian J. Mielke, Ryan Cotterell","Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases.",,,,ACL
246,2018,Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings,"Bernd Bohnet, Ryan McDonald, Gonçalo Simões, Daniel Andor","The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy. One characteristic common among these models is the presence of rich initial word encodings. These encodings typically are composed of a recurrent character-based representation with dynamically and pre-trained word embeddings. However, these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts. In this paper, we investigate models that use recurrent neural networks with sentence-level context for initial character and word-based representations. In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states.",,,,ACL
247,2018,Neural Factor Graph Models for Cross-lingual Morphological Tagging,"Chaitanya Malaviya, Matthew R. Gormley, Graham Neubig","Morphological analysis involves predicting the syntactic traits of a word (e.g. POS: Noun, Case: Acc, Gender: Fem). Previous work in morphological tagging improves performance for low-resource languages (LRLs) through cross-lingual training with a high-resource language (HRL) from the same family, but is limited by the strict, often false, assumption that tag sets exactly overlap between the HRL and LRL. In this paper we propose a method for cross-lingual morphological tagging that aims to improve information sharing between languages by relaxing this assumption. The proposed model uses factorial conditional random fields with neural network potentials, making it possible to (1) utilize the expressive power of neural network representations to smooth over superficial differences in the surface forms, (2) model pairwise and transitive relationships between tags, and (3) accurately generate tag sets that are unseen or rare in the training data. Experiments on four languages from the Universal Dependencies Treebank demonstrate superior tagging accuracies over existing cross-lingual approaches.",,,,ACL
248,2018,Global Transition-based Non-projective Dependency Parsing,"Carlos Gómez-Rodríguez, Tianze Shi, Lillian Lee","Shi, Huang, and Lee (2017a) obtained state-of-the-art results for English and Chinese dependency parsing by combining dynamic-programming implementations of transition-based dependency parsers with a minimal set of bidirectional LSTM features. However, their results were limited to projective parsing. In this paper, we extend their approach to support non-projectivity by providing the first practical implementation of the MH₄ algorithm, an O(n4) mildly nonprojective dynamic-programming parser with very high coverage on non-projective treebanks. To make MH₄ compatible with minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is effective than its projective counterpart in parsing a number of highly non-projective languages.",,,,ACL
249,2018,Constituency Parsing with a Self-Attentive Encoder,"Nikita Kitaev, Dan Klein","We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",,,,ACL
250,2018,Pre- and In-Parsing Models for Neural Empty Category Detection,"Yufei Chen, Yuanyuan Zhao, Weiwei Sun, Xiaojun Wan","Motivated by the positive impact of empty category on syntactic parsing, we study neural models for pre- and in-parsing detection of empty category, which has not previously been investigated. We find several non-obvious facts: (a) BiLSTM can capture non-local contextual information which is essential for detecting empty categories, (b) even with a BiLSTM, syntactic information is still able to enhance the detection, and (c) automatic detection of empty categories improves parsing quality for overt words. Our neural ECD models outperform the prior state-of-the-art by significant margins.",,,,ACL
251,2018,Composing Finite State Transducers on GPUs,"Arturo Argueta, David Chiang","Weighted finite state transducers (FSTs) are frequently used in language processing to handle tasks such as part-of-speech tagging and speech recognition. There has been previous work using multiple CPU cores to accelerate finite state algorithms, but limited attention has been given to parallel graphics processing unit (GPU) implementations. In this paper, we introduce the first (to our knowledge) GPU implementation of the FST composition operation, and we also discuss the optimizations used to achieve the best performance on this architecture. We show that our approach obtains speedups of up to 6 times over our serial implementation and 4.5 times over OpenFST.",,,,ACL
252,2018,Supervised Treebank Conversion: Data and Approaches,"Xinzhou Jiang, Zhenghua Li, Bo Zhang, Min Zhang","Treebank conversion is a straightforward and effective way to exploit various heterogeneous treebanks for boosting parsing performance. However, previous work mainly focuses on unsupervised treebank conversion and has made little progress due to the lack of manually labeled data where each sentence has two syntactic trees complying with two different guidelines at the same time, referred as bi-tree aligned data. In this work, we for the first time propose the task of supervised treebank conversion. First, we manually construct a bi-tree aligned dataset containing over ten thousand sentences. Then, we propose two simple yet effective conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two conversion approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multi-treebank exploitation and leads to significantly higher parsing accuracy.",,,,ACL
253,2018,Object-oriented Neural Programming (OONP) for Document Understanding,"Zhengdong Lu, Xianggen Liu, Haotian Cui, Yukun Yan","We propose Object-oriented Neural Programming (OONP), a framework for semantically parsing documents in specific domains. Basically, OONP reads a document and parses it into a predesigned object-oriented data structure that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the document, and builds and updates an intermediate ontology during the process to summarize its partial understanding of the text. OONP supports a big variety of forms (both symbolic and differentiable) for representing the state and the document, and a rich family of operations to compose the representation. An OONP parser can be trained with supervision of different forms and strength, including supervised learning (SL), reinforcement learning (RL) and hybrid of the two. Our experiments on both synthetic and real-world document parsing tasks have shown that OONP can learn to handle fairly complicated ontology with training data of modest sizes.",,,,ACL
254,2018,Finding syntax in human encephalography with beam search,"John Hale, Chris Dyer, Adhiguna Kuncoro, Jonathan Brennan","Recurrent neural network grammars (RNNGs) are generative models of (tree , string ) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.",,,,ACL
255,2018,Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information,"Sudha Rao, Hal Daumé III","Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. In this work, we build a neural network model for the task of ranking clarification questions. Our model is inspired by the idea of expected value of perfect information: a good question is one whose expected answer will be useful. We study this problem using data from StackExchange, a plentiful online resource in which people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster. We create a dataset of clarification questions consisting of 77K posts paired with a clarification question (and answer) from three domains of StackExchange: askubuntu, unix and superuser. We evaluate our model on 500 samples of this dataset against expert human judgments and demonstrate significant improvements over controlled baselines.",,,,ACL
256,2018,Let’s do it “again”: A First Computational Approach to Detecting Adverbial Presupposition Triggers,"Andre Cianflone, Yulan Feng, Jad Kabbara, Jackie Chi Kit Cheung","We introduce the novel task of predicting adverbial presupposition triggers, which is useful for natural language generation tasks such as summarization and dialogue systems. We introduce two new corpora, derived from the Penn Treebank and the Annotated English Gigaword dataset and investigate the use of a novel attention mechanism tailored to this task. Our attention mechanism augments a baseline recurrent neural network without the need for additional trainable parameters, minimizing the added computational cost of our mechanism. We demonstrate that this model statistically outperforms our baselines.",,,,ACL
1,2019,One Time of Interaction May Not Be Enough: Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues,"Chongyang Tao, Wei Wu, Can Xu, Wenpeng Hu, Dongyan Zhao","Currently, researchers have paid great attention to retrieval-based dialogues in open-domain. In particular, people study the problem by investigating context-response matching for multi-turn response selection based on publicly recognized benchmark data sets. State-of-the-art methods require a response to interact with each utterance in a context from the beginning, but the interaction is performed in a shallow way. In this work, we let utterance-response interaction go deep by proposing an interaction-over-interaction network (IoI). The model performs matching by stacking multiple interaction blocks in which residual information from one time of interaction initiates the interaction process again. Thus, matching information within an utterance-response pair is extracted from the interaction of the pair in an iterative fashion, and the information flows along the chain of the blocks via representations. Evaluation results on three benchmark data sets indicate that IoI can significantly outperform state-of-the-art methods in terms of various matching metrics. Through further analysis, we also unveil how the depth of interaction affects the performance of IoI.",,,,ACL
2,2019,Incremental Transformer with Deliberation Decoder for Document Grounded Conversations,"Zekang Li, Cheng Niu, Fandong Meng, Yang Feng, Qian Li","Document Grounded Conversations is a task to generate dialogue responses when chatting about the content of a given document. Obviously, document knowledge plays a critical role in Document Grounded Conversations, while existing dialogue models do not exploit this kind of knowledge effectively enough. In this paper, we propose a novel Transformer-based architecture for multi-turn document grounded conversations. In particular, we devise an Incremental Transformer to encode multi-turn utterances along with knowledge in related documents. Motivated by the human cognitive process, we design a two-pass decoder (Deliberation Decoder) to improve context coherence and knowledge correctness. Our empirical study on a real-world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance.",,,,ACL
3,2019,Improving Multi-turn Dialogue Modelling with Utterance ReWriter,"Hui Su, Xiaoyu Shen, Rongzhi Zhang, Fei Sun, Pengwei Hu","Recent research has achieved impressive results in single-turn dialogue modelling. In the multi-turn setting, however, current models are still far from satisfactory. One major challenge is the frequently occurred coreference and information omission in our daily conversation, making it hard for machines to understand the real intention. In this paper, we propose rewriting the human utterance as a pre-process to help multi-turn dialgoue modelling. Each utterance is first rewritten to recover all coreferred and omitted information. The next processing steps are then performed based on the rewritten utterance. To properly train the utterance rewriter, we collect a new dataset with human annotations and introduce a Transformer-based utterance rewriting architecture using the pointer network. We show the proposed architecture achieves remarkably good performance on the utterance rewriting task. The trained utterance rewriter can be easily integrated into online chatbots and brings general improvement over different domains.",,,,ACL
4,2019,Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study,"Chinnadhurai Sankar, Sandeep Subramanian, Chris Pal, Sarath Chandar, Yoshua Bengio","Neural generative models have been become increasingly popular when building conversational agents. They offer flexibility, can be easily adapted to new domains, and require minimal domain engineering. A common criticism of these systems is that they seldom understand or use the available dialog history effectively. In this paper, we take an empirical approach to understanding how these models use the available dialog history by studying the sensitivity of the models to artificially introduced unnatural changes or perturbations to their context at test time. We experiment with 10 different types of perturbations on 4 multi-turn dialog datasets and find that commonly used neural dialog architectures like recurrent and transformer-based seq2seq models are rarely sensitive to most perturbations such as missing or reordering utterances, shuffling words, etc. Also, by open-sourcing our code, we believe that it will serve as a useful diagnostic tool for evaluating dialog systems in the future.",,,,ACL
5,2019,Boosting Dialog Response Generation,"Wenchao Du, Alan W Black","Neural models have become one of the most important approaches to dialog response generation. However, they still tend to generate the most common and generic responses in the corpus all the time. To address this problem, we designed an iterative training process and ensemble method based on boosting. We combined our method with different training and decoding paradigms as the base model, including mutual-information-based decoding and reward-augmented maximum likelihood learning. Empirical results show that our approach can significantly improve the diversity and relevance of the responses generated by all base models, backed by objective measurements and human evaluation.",,,,ACL
6,2019,Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection,"Junyu Lu, Chenbin Zhang, Zeying Xie, Guang Ling, Tom Chao Zhou","Response selection plays an important role in fully automated dialogue systems. Given the dialogue context, the goal of response selection is to identify the best-matched next utterance (i.e., response) from multiple candidates. Despite the efforts of many previous useful models, this task remains challenging due to the huge semantic gap and also the large size of candidate set. To address these issues, we propose a Spatio-Temporal Matching network (STM) for response selection. In detail, soft alignment is first used to obtain the local relevance between the context and the response. And then, we construct spatio-temporal features by aggregating attention images in time dimension and make use of 3D convolution and pooling operations to extract matching information. Evaluation on two large-scale multi-turn response selection tasks has demonstrated that our proposed model significantly outperforms the state-of-the-art model. Particularly, visualization analysis shows that the spatio-temporal features enables matching information in segment pairs and time sequences, and have good interpretability for multi-turn text matching.",,,,ACL
7,2019,Semantic Parsing with Dual Learning,"Ruisheng Cao, Su Zhu, Chen Liu, Jieyu Li, Kai Yu","Semantic parsing converts natural language queries into structured logical forms. The lack of training data is still one of the most serious problems in this area. In this work, we develop a semantic parsing framework with the dual learning algorithm, which enables a semantic parser to make full use of data (labeled and even unlabeled) through a dual-learning game. This game between a primal model (semantic parsing) and a dual model (logical form to query) forces them to regularize each other, and can achieve feedback signals from some prior-knowledge. By utilizing the prior-knowledge of logical form structures, we propose a novel reward signal at the surface and semantic levels which tends to generate complete and reasonable logical forms. Experimental results show that our approach achieves new state-of-the-art performance on ATIS dataset and gets competitive performance on OVERNIGHT dataset.",,,,ACL
8,2019,Semantic Expressive Capacity with Bounded Memory,"Antoine Venant, Alexander Koller","We investigate the capacity of mechanisms for compositional semantic parsing to describe relations between sentences and semantic representations. We prove that in order to represent certain relations, mechanisms which are syntactically projective must be able to remember an unbounded number of locations in the semantic representations, where nonprojective mechanisms need not. This is the first result of this kind, and has consequences both for grammar-based and for neural systems.",,,,ACL
9,2019,AMR Parsing as Sequence-to-Graph Transduction,"Sheng Zhang, Xutai Ma, Kevin Duh, Benjamin Van Durme","We propose an attention-based model that treats AMR parsing as sequence-to-graph transduction. Unlike most AMR parsers that rely on pre-trained aligners, external semantic resources, or data augmentation, our proposed parser is aligner-free, and it can be effectively trained with limited amounts of labeled AMR data. Our experimental results outperform all previously reported SMATCH scores, on both AMR 2.0 (76.3% on LDC2017T10) and AMR 1.0 (70.2% on LDC2014T12).",,,,ACL
10,2019,Generating Logical Forms from Graph Representations of Text and Entities,"Peter Shaw, Philip Massey, Angelica Chen, Francesco Piccinno, Yasemin Altun","Structured information about entities is critical for many semantic parsing tasks. We present an approach that uses a Graph Neural Network (GNN) architecture to incorporate information about relevant entities and their relations during parsing. Combined with a decoder copy mechanism, this approach provides a conceptually simple mechanism to generate logical forms with entities. We demonstrate that this approach is competitive with the state-of-the-art across several tasks without pre-training, and outperforms existing approaches when combined with BERT pre-training.",,,,ACL
11,2019,Learning Compressed Sentence Representations for On-Device Text Processing,"Dinghan Shen, Pengyu Cheng, Dhanasekar Sundararaman, Xinyuan Zhang, Qian Yang","Vector representations of sentences, trained on massive text corpora, are widely used as generic sentence embeddings across a variety of NLP problems. The learned representations are generally assumed to be continuous and real-valued, giving rise to a large memory footprint and slow retrieval speed, which hinders their applicability to low-resource (memory and computation) platforms, such as mobile devices. In this paper, we propose four different strategies to transform continuous and generic sentence embeddings into a binarized form, while preserving their rich semantic information. The introduced methods are evaluated across a wide range of downstream tasks, where the binarized sentence embeddings are demonstrated to degrade performance by only about 2% relative to their continuous counterparts, while reducing the storage requirement by over 98%. Moreover, with the learned binary representations, the semantic relatedness of two sentences can be evaluated by simply calculating their Hamming distance, which is more computational efficient compared with the inner product operation between continuous embeddings. Detailed analysis and case study further validate the effectiveness of proposed methods.",,,,ACL
12,2019,The (Non-)Utility of Structural Features in BiLSTM-based Dependency Parsers,"Agnieszka Falenska, Jonas Kuhn","Classical non-neural dependency parsers put considerable effort on the design of feature functions. Especially, they benefit from information coming from structural features, such as features drawn from neighboring tokens in the dependency tree. In contrast, their BiLSTM-based successors achieve state-of-the-art performance without explicit information about the structural context. In this paper we aim to answer the question: How much structural context are the BiLSTM representations able to capture implicitly? We show that features drawn from partial subtrees become redundant when the BiLSTMs are used. We provide a deep insight into information flow in transition- and graph-based neural architectures to demonstrate where the implicit information comes from when the parsers make their decisions. Finally, with model ablations we demonstrate that the structural context is not only present in the models, but it significantly influences their performance.",,,,ACL
13,2019,Automatic Generation of High Quality CCGbanks for Parser Domain Adaptation,"Masashi Yoshikawa, Hiroshi Noji, Koji Mineshima, Daisuke Bekki","We propose a new domain adaptation method for Combinatory Categorial Grammar (CCG) parsing, based on the idea of automatic generation of CCG corpora exploiting cheaper resources of dependency trees. Our solution is conceptually simple, and not relying on a specific parser architecture, making it applicable to the current best-performing parsers. We conduct extensive parsing experiments with detailed discussion; on top of existing benchmark datasets on (1) biomedical texts and (2) question sentences, we create experimental datasets of (3) speech conversation and (4) math problems. When applied to the proposed method, an off-the-shelf CCG parser shows significant performance gains, improving from 90.7% to 96.6% on speech conversation, and from 88.5% to 96.8% on math problems.",,,,ACL
14,2019,A Joint Named-Entity Recognizer for Heterogeneous Tag-sets Using a Tag Hierarchy,"Genady Beryozkin, Yoel Drori, Oren Gilon, Tzvika Hartman, Idan Szpektor","We study a variant of domain adaptation for named-entity recognition where multiple, heterogeneously tagged training sets are available. Furthermore, the test tag-set is not identical to any individual training tag-set. Yet, the relations between all tags are provided in a tag hierarchy, covering the test tags as a combination of training tags. This setting occurs when various datasets are created using different annotation schemes. This is also the case of extending a tag-set with a new tag by annotating only the new tag in a new dataset. We propose to use the given tag hierarchy to jointly learn a neural network that shares its tagging layer among all tag-sets. We compare this model to combining independent models and to a model based on the multitasking approach. Our experiments show the benefit of the tag-hierarchy model, especially when facing non-trivial consolidation of tag-sets.",,,,ACL
15,2019,Massively Multilingual Transfer for NER,"Afshin Rahimi, Yuan Li, Trevor Cohn","In cross-lingual transfer, NLP models over one or more source languages are applied to a low-resource target language. While most prior work has used a single source model or a few carefully selected models, here we consider a “massive” setting with many such models. This setting raises the problem of poor transfer, particularly from distant languages. We propose two techniques for modulating the transfer, suitable for zero-shot or few-shot learning, respectively. Evaluating on named entity recognition, we show that our techniques are much more effective than strong baselines, including standard ensembling, and our unsupervised method rivals oracle selection of the single best individual model.",,,,ACL
16,2019,Reliability-aware Dynamic Feature Composition for Name Tagging,"Ying Lin, Liyuan Liu, Heng Ji, Dong Yu, Jiawei Han","Word embeddings are widely used on a variety of tasks and can substantially improve the performance. However, their quality is not consistent throughout the vocabulary due to the long-tail distribution of word frequency. Without sufficient contexts, rare word embeddings are usually less reliable than those of common words. However, current models typically trust all word embeddings equally regardless of their reliability and thus may introduce noise and hurt the performance. Since names often contain rare and uncommon words, this problem is particularly critical for name tagging. In this paper, we propose a novel reliability-aware name tagging model to tackle this issue. We design a set of word frequency-based reliability signals to indicate the quality of each word embedding. Guided by the reliability signals, the model is able to dynamically select and compose features such as word embedding and character-level representation using gating mechanisms. For example, if an input word is rare, the model relies less on its word embedding and assigns higher weights to its character and contextual features. Experiments on OntoNotes 5.0 show that our model outperforms the baseline model by 2.7% absolute gain in F-score. In cross-genre experiments on five genres in OntoNotes, our model improves the performance for most genre pairs and obtains up to 5% absolute F-score gain.",,,,ACL
17,2019,Unsupervised Pivot Translation for Distant Languages,"Yichong Leng, Xu Tan, Tao Qin, Xiang-Yang Li, Tie-Yan Liu","Unsupervised neural machine translation (NMT) has attracted a lot of attention recently. While state-of-the-art methods for unsupervised translation usually perform well between similar languages (e.g., English-German translation), they perform poorly between distant languages, because unsupervised alignment does not work well for distant languages. In this work, we introduce unsupervised pivot translation for distant languages, which translates a language to a distant language through multiple hops, and the unsupervised translation on each hop is relatively easier than the original direct translation. We propose a learning to route (LTR) method to choose the translation path between the source and target languages. LTR is trained on language pairs whose best translation path is available and is applied on the unseen language pairs for path selection. Experiments on 20 languages and 294 distant language pairs demonstrate the advantages of the unsupervised pivot translation for distant languages, as well as the effectiveness of the proposed LTR for path selection. Specifically, in the best case, LTR achieves an improvement of 5.58 BLEU points over the conventional direct unsupervised method.",,,,ACL
18,2019,Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces,"Barun Patra, Joel Ruben Antony Moniz, Sarthak Garg, Matthew R. Gormley, Graham Neubig","Recent work on bilingual lexicon induction (BLI) has frequently depended either on aligned bilingual lexicons or on distribution matching, often with an assumption about the isometry of the two spaces. We propose a technique to quantitatively estimate this assumption of the isometry between two embedding spaces and empirically show that this assumption weakens as the languages in question become increasingly etymologically distant. We then propose Bilingual Lexicon Induction with Semi-Supervision (BLISS) — a semi-supervised approach that relaxes the isometric assumption while leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don’t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.",,,,ACL
19,2019,An Effective Approach to Unsupervised Machine Translation,"Mikel Artetxe, Gorka Labaka, Eneko Agirre","While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014.",,,,ACL
20,2019,Effective Adversarial Regularization for Neural Machine Translation,"Motoki Sato, Jun Suzuki, Shun Kiyono","A regularization technique based on adversarial perturbation, which was initially developed in the field of image processing, has been successfully applied to text classification tasks and has yielded attractive improvements. We aim to further leverage this promising methodology into more sophisticated and critical neural models in the natural language processing field, i.e., neural machine translation (NMT) models. However, it is not trivial to apply this methodology to such models. Thus, this paper investigates the effectiveness of several possible configurations of applying the adversarial perturbation and reveals that the adversarial regularization technique can significantly and consistently improve the performance of widely used NMT models, such as LSTM-based and Transformer-based models.",,,,ACL
21,2019,Revisiting Low-Resource Neural Machine Translation: A Case Study,"Rico Sennrich, Biao Zhang","It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (PBSMT) and requiring large amounts of auxiliary data to achieve competitive results. In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. We discuss some pitfalls to be aware of when training low-resource NMT systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource NMT. In our experiments on German–English with different amounts of IWSLT14 training data, we show that, without the use of any auxiliary monolingual or multilingual data, an optimized NMT system can outperform PBSMT with far less data than previously claimed. We also apply these techniques to a low-resource Korean–English dataset, surpassing previously reported results by 4 BLEU.",,,,ACL
22,2019,Domain Adaptive Inference for Neural Machine Translation,"Danielle Saunders, Felix Stahlberg, Adrià de Gispert, Bill Byrne","We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and report strong improvements across test domains without access to the domain label.",,,,ACL
23,2019,Neural Relation Extraction for Knowledge Base Enrichment,"Bayu Distiawan Trisedya, Gerhard Weikum, Jianzhong Qi, Rui Zhang","We study relation extraction for knowledge base (KB) enrichment. Specifically, we aim to extract entities and their relationships from sentences in the form of triples and map the elements of the extracted triples to an existing KB in an end-to-end manner. Previous studies focus on the extraction itself and rely on Named Entity Disambiguation (NED) to map triples into the KB space. This way, NED errors may cause extraction errors that affect the overall precision and recall.To address this problem, we propose an end-to-end relation extraction model for KB enrichment based on a neural encoder-decoder model. We collect high-quality training data by distant supervision with co-reference resolution and paraphrase detection. We propose an n-gram based attention model that captures multi-word entity names in a sentence. Our model employs jointly learned word and entity embeddings to support named entity disambiguation. Finally, our model uses a modified beam search and a triple classifier to help generate high-quality triples. Our model outperforms state-of-the-art baselines by 15.51% and 8.38% in terms of F1 score on two real-world datasets.",,,,ACL
24,2019,Attention Guided Graph Convolutional Networks for Relation Extraction,"Zhijiang Guo, Yan Zhang, Wei Lu","Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches.",,,,ACL
25,2019,Spatial Aggregation Facilitates Discovery of Spatial Topics,"Aniruddha Maiti, Slobodan Vucetic","Spatial aggregation refers to merging of documents created at the same spatial location. We show that by spatial aggregation of a large collection of documents and applying a traditional topic discovery algorithm on the aggregated data we can efficiently discover spatially distinct topics. By looking at topic discovery through matrix factorization lenses we show that spatial aggregation allows low rank approximation of the original document-word matrix, in which spatially distinct topics are preserved and non-spatial topics are aggregated into a single topic. Our experiments on synthetic data confirm this observation. Our experiments on 4.7 million tweets collected during the Sandy Hurricane in 2012 show that spatial and temporal aggregation allows rapid discovery of relevant spatial and temporal topics during that period. Our work indicates that different forms of document aggregation might be effective in rapid discovery of various types of distinct topics from large collections of documents.",,,,ACL
26,2019,Relation Embedding with Dihedral Group in Knowledge Graph,"Canran Xu, Ruijiang Li","Link prediction is critical for the application of incomplete knowledge graph (KG) in the downstream tasks. As a family of effective approaches for link predictions, embedding methods try to learn low-rank representations for both entities and relations such that the bilinear form defined therein is a well-behaved scoring function. Despite of their successful performances, existing bilinear forms overlook the modeling of relation compositions, resulting in lacks of interpretability for reasoning on KG. To fulfill this gap, we propose a new model called DihEdral, named after dihedral symmetry group. This new model learns knowledge graph embeddings that can capture relation compositions by nature. Furthermore, our approach models the relation embeddings parametrized by discrete values, thereby decrease the solution space drastically. Our experiments show that DihEdral is able to capture all desired properties such as (skew-) symmetry, inversion and (non-) Abelian composition, and outperforms existing bilinear form based approach and is comparable to or better than deep learning models such as ConvE.",,,,ACL
27,2019,Sequence Tagging with Contextual and Non-Contextual Subword Representations: A Multilingual Evaluation,"Benjamin Heinzerling, Michael Strube","Pretrained contextual and non-contextual subword embeddings have become available in over 250 languages, allowing massively multilingual NLP. However, while there is no dearth of pretrained embeddings, the distinct lack of systematic evaluations makes it difficult for practitioners to choose between them. In this work, we conduct an extensive evaluation comparing non-contextual subword embeddings, namely FastText and BPEmb, and a contextual representation method, namely BERT, on multilingual named entity recognition and part-of-speech tagging. We find that overall, a combination of BERT, BPEmb, and character representations works best across languages and tasks. A more detailed analysis reveals different strengths and weaknesses: Multilingual BERT performs well in medium- to high-resource languages, but is outperformed by non-contextual subword embeddings in a low-resource setting.",,,,ACL
28,2019,Augmenting Neural Networks with First-order Logic,"Tao Li, Vivek Srikumar","Today, the dominant paradigm for training neural networks involves minimizing task loss on a large dataset. Using world knowledge to inform a model, and yet retain the ability to perform end-to-end training remains an open question. In this paper, we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction. Our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over baselines, especially in low-data regimes.",,,,ACL
29,2019,Self-Regulated Interactive Sequence-to-Sequence Learning,"Julia Kreutzer, Stefan Riezler","Not all types of supervision signals are created equal: Different types of feedback have different costs and effects on learning. We show how self-regulation strategies that decide when to ask for which kind of feedback from a teacher (or from oneself) can be cast as a learning-to-learn problem leading to improved cost-aware sequence-to-sequence learning. In experiments on interactive neural machine translation, we find that the self-regulator discovers an 𝜖-greedy strategy for the optimal cost-quality trade-off by mixing different feedback types including corrections, error markups, and self-supervision. Furthermore, we demonstrate its robustness under domain shift and identify it as a promising alternative to active learning.",,,,ACL
30,2019,You Only Need Attention to Traverse Trees,"Mahtab Ahmed, Muhammad Rifayat Samee, Robert E. Mercer","In recent NLP research, a topic of interest is universal sentence encoding, sentence representations that can be used in any supervised task. At the word sequence level, fully attention-based models suffer from two problems: a quadratic increase in memory consumption with respect to the sentence length and an inability to capture and use syntactic information. Recursive neural nets can extract very good syntactic information by traversing a tree structure. To this end, we propose Tree Transformer, a model that captures phrase level syntax for constituency trees as well as word-level dependencies for dependency trees by doing recursive traversal only with attention. Evaluation of this model on four tasks gets noteworthy results compared to the standard transformer and LSTM-based models as well as tree-structured LSTMs. Ablation studies to find whether positional information is inherently encoded in the trees and which type of attention is suitable for doing the recursive traversal are provided.",,,,ACL
31,2019,Cross-Domain Generalization of Neural Constituency Parsers,"Daniel Fried, Nikita Kitaev, Dan Klein","Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing—but to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.",,,,ACL
32,2019,Adaptive Attention Span in Transformers,"Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin","We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.",,,,ACL
33,2019,Neural News Recommendation with Long- and Short-term User Representations,"Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu","Personalized news recommendation is important to help users find their interested news and improve reading experience. A key problem in news recommendation is learning accurate user representations to capture their interests. Users usually have both long-term preferences and short-term interests. However, existing news recommendation methods usually learn single representations of users, which may be insufficient. In this paper, we propose a neural news recommendation approach which can learn both long- and short-term user representations. The core of our approach is a news encoder and a user encoder. In the news encoder, we learn representations of news from their titles and topic categories, and use attention network to select important words. In the user encoder, we propose to learn long-term user representations from the embeddings of their IDs.In addition, we propose to learn short-term user representations from their recently browsed news via GRU network. Besides, we propose two methods to combine long-term and short-term user representations. The first one is using the long-term user representation to initialize the hidden state of the GRU network in short-term user representation. The second one is concatenating both long- and short-term user representations as a unified user vector. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of neural news recommendation.",,,,ACL
34,2019,Automatic Domain Adaptation Outperforms Manual Domain Adaptation for Predicting Financial Outcomes,"Marina Sedinkina, Nikolas Breitkopf, Hinrich Schütze","In this paper, we automatically create sentiment dictionaries for predicting financial outcomes. We compare three approaches: (i) manual adaptation of the domain-general dictionary H4N, (ii) automatic adaptation of H4N and (iii) a combination consisting of first manual, then automatic adaptation. In our experiments, we demonstrate that the automatically adapted sentiment dictionary outperforms the previous state of the art in predicting the financial outcomes excess return and volatility. In particular, automatic adaptation performs better than manual adaptation. In our analysis, we find that annotation based on an expert’s a priori belief about a word’s meaning can be incorrect – annotation should be performed based on the word’s contexts in the target domain instead.",,,,ACL
35,2019,Manipulating the Difficulty of C-Tests,"Ji-Ung Lee, Erik Schwan, Christian M. Meyer","We propose two novel manipulation strategies for increasing and decreasing the difficulty of C-tests automatically. This is a crucial step towards generating learner-adaptive exercises for self-directed language learning and preparing language assessment tests. To reach the desired difficulty level, we manipulate the size and the distribution of gaps based on absolute and relative gap difficulty predictions. We evaluate our approach in corpus-based experiments and in a user study with 60 participants. We find that both strategies are able to generate C-tests with the desired difficulty level.",,,,ACL
36,2019,Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings,"Zied Haj-Yahia, Adrien Sieg, Léa A. Deleris","Text classification aims at mapping documents into a set of predefined categories. Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy. This is particularly true when the number of target categories is in the tens or the hundreds. In this work, we explore an unsupervised approach to classify documents into categories simply described by a label. The proposed method is inspired by the way a human proceeds in this situation: It draws on textual similarity between the most relevant words in each document and a dictionary of keywords for each category reflecting its semantics and lexical field. The novelty of our method hinges on the enrichment of the category labels through a combination of human expertise and language models, both generic and domain specific. Our experiments on 5 standard corpora show that the proposed method increases F1-score over relying solely on human expertise and can also be on par with simple supervised approaches. It thus provides a practical alternative to situations where low cost text categorization is needed, as we illustrate with our application to operational risk incidents classification.",,,,ACL
37,2019,Neural Text Simplification of Clinical Letters with a Domain Specific Phrase Table,"Matthew Shardlow, Raheel Nawaz","Clinical letters are infamously impenetrable for the lay patient. This work uses neural text simplification methods to automatically improve the understandability of clinical letters for patients. We take existing neural text simplification software and augment it with a new phrase table that links complex medical terminology to simpler vocabulary by mining SNOMED-CT. In an evaluation task using crowdsourcing, we show that the results of our new system are ranked easier to understand (average rank 1.93) than using the original system (2.34) without our phrase table. We also show improvement against baselines including the original text (2.79) and using the phrase table without the neural text simplification software (2.94). Our methods can easily be transferred outside of the clinical domain by using domain-appropriate resources to provide effective neural text simplification for any domain without the need for costly annotation.",,,,ACL
38,2019,What You Say and How You Say It Matters: Predicting Stock Volatility Using Verbal and Vocal Cues,"Yu Qin, Yi Yang","Predicting financial risk is an essential task in financial market. Prior research has shown that textual information in a firm’s financial statement can be used to predict its stock’s risk level. Nowadays, firm CEOs communicate information not only verbally through press releases and financial reports, but also nonverbally through investor meetings and earnings conference calls. There are anecdotal evidences that CEO’s vocal features, such as emotions and voice tones, can reveal the firm’s performance. However, how vocal features can be used to predict risk levels, and to what extent, is still unknown. To fill the gap, we obtain earnings call audio recordings and textual transcripts for S&P 500 companies in recent years. We propose a multimodal deep regression model (MDRM) that jointly model CEO’s verbal (from text) and vocal (from audio) information in a conference call. Empirical results show that our model that jointly considers verbal and vocal features achieves significant and substantial prediction error reduction. We also discuss several interesting findings and the implications to financial markets. The processed earnings conference calls data (text and audio) are released for readers who are interested in reproducing the results or designing trading strategy.",,,,ACL
39,2019,Detecting Concealed Information in Text and Speech,Shengli Hu,"Motivated by infamous cheating scandals in the media industry, the wine industry, and political campaigns, we address the problem of detecting concealed information in technical settings. In this work, we explore acoustic-prosodic and linguistic indicators of information concealment by collecting a unique corpus of professionals practicing for oral exams while concealing information. We reveal subtle signs of concealing information in speech and text, compare and contrast them with those in deception detection literature, uncovering the link between concealing information and deception. We then present a series of experiments that automatically detect concealed information from text and speech. We compare the use of acoustic-prosodic, linguistic, and individual feature sets, using different machine learning models. Finally, we present a multi-task learning framework with acoustic, linguistic, and individual features, that outperforms human performance by over 15%.",,,,ACL
40,2019,Evidence-based Trustworthiness,"Yi Zhang, Zachary Ives, Dan Roth","The information revolution brought with it information pollution. Information retrieval and extraction help us cope with abundant information from diverse sources. But some sources are of anonymous authorship, and some are of uncertain accuracy, so how can we determine what we should actually believe? Not all information sources are equally trustworthy, and simply accepting the majority view is often wrong. This paper develops a general framework for estimating the trustworthiness of information sources in an environment where multiple sources provide claims and supporting evidence, and each claim can potentially be produced by multiple sources. We consider two settings: one in which information sources directly assert claims, and a more realistic and challenging one, in which claims are inferred from evidence provided by sources, via (possibly noisy) NLP techniques. Our key contribution is to develop a family of probabilistic models that jointly estimate the trustworthiness of sources, and the credibility of claims they assert. This is done while accounting for the (possibly noisy) NLP needed to infer claims from evidence supplied by sources. We evaluate our framework on several datasets, showing strong results and significant improvement over baselines.",,,,ACL
41,2019,Disentangled Representation Learning for Non-Parallel Text Style Transfer,"Vineet John, Lili Mou, Hareesh Bahuleyan, Olga Vechtomova","This paper tackles the problem of disentangling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for style prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches.",,,,ACL
42,2019,Cross-Sentence Grammatical Error Correction,"Shamil Chollampatt, Weiqi Wang, Hwee Tou Ng","Automatic grammatical error correction (GEC) research has made remarkable progress in the past decade. However, all existing approaches to GEC correct errors by considering a single sentence alone and ignoring crucial cross-sentence context. Some errors can only be corrected reliably using cross-sentence context and models can also benefit from the additional contextual information in correcting other errors. In this paper, we address this serious limitation of existing approaches and improve strong neural encoder-decoder models by appropriately modeling wider contexts. We employ an auxiliary encoder that encodes previous sentences and incorporate the encoding in the decoder via attention and gating mechanisms. Our approach results in statistically significant improvements in overall GEC performance over strong baselines across multiple test sets. Analysis of our cross-sentence GEC model on a synthetic dataset shows high performance in verb tense corrections that require cross-sentence context.",,,,ACL
43,2019,This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation,"Rui Zhang, Joel Tetreault","Given the overwhelming number of emails, an effective subject line becomes essential to better inform the recipient of the email’s content. In this paper, we propose and study the task of email subject line generation: automatically generating an email subject line from the email body. We create the first dataset for this task and find that email subject line generation favor extremely abstractive summary which differentiates it from news headline generation or news single document summarization. We then develop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems. We also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric. Our system outperforms competitive baselines given both automatic and human evaluations. To our knowledge, this is the first work to tackle the problem of effective email subject line generation.",,,,ACL
44,2019,Time-Out: Temporal Referencing for Robust Modeling of Lexical Semantic Change,"Haim Dubossarsky, Simon Hengchen, Nina Tahmasebi, Dominik Schlechtweg","State-of-the-art models of lexical semantic change detection suffer from noise stemming from vector space alignment. We have empirically tested the Temporal Referencing method for lexical semantic change and show that, by avoiding alignment, it is less affected by this noise. We show that, trained on a diachronic corpus, the skip-gram with negative sampling architecture with temporal referencing outperforms alignment models on a synthetic task as well as a manual testset. We introduce a principled way to simulate lexical semantic change and systematically control for possible biases.",,,,ACL
45,2019,Adversarial Attention Modeling for Multi-dimensional Emotion Regression,"Suyang Zhu, Shoushan Li, Guodong Zhou","In this paper, we propose a neural network-based approach, namely Adversarial Attention Network, to the task of multi-dimensional emotion regression, which automatically rates multiple emotion dimension scores for an input text. Especially, to determine which words are valuable for a particular emotion dimension, an attention layer is trained to weight the words in an input sequence. Furthermore, adversarial training is employed between two attention layers to learn better word weights via a discriminator. In particular, a shared attention layer is incorporated to learn public word weights between two emotion dimensions. Empirical evaluation on the EMOBANK corpus shows that our approach achieves notable improvements in r-values on both EMOBANK Reader’s and Writer’s multi-dimensional emotion regression tasks in all domains over the state-of-the-art baselines.",,,,ACL
46,2019,"Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing","Sijie Mai, Haifeng Hu, Songlong Xing","We propose a general strategy named ‘divide, conquer and combine’ for multimodal fusion. Instead of directly fusing features at holistic level, we conduct fusion hierarchically so that both local and global interactions are considered for a comprehensive interpretation of multimodal embeddings. In the ‘divide’ and ‘conquer’ stages, we conduct local fusion by exploring the interaction of a portion of the aligned feature vectors across various modalities lying within a sliding window, which ensures that each part of multimodal embeddings are explored sufficiently. On its basis, global fusion is conducted in the ‘combine’ stage to explore the interconnection across local interactions, via an Attentive Bi-directional Skip-connected LSTM that directly connects distant local interactions and integrates two levels of attention mechanism. In this way, local interactions can exchange information sufficiently and thus obtain an overall view of multimodal information. Our method achieves state-of-the-art performance on multimodal affective computing with higher efficiency.",,,,ACL
47,2019,Modeling Financial Analysts’ Decision Making via the Pragmatics and Semantics of Earnings Calls,"Katherine Keith, Amanda Stent","Every fiscal quarter, companies hold earnings calls in which company executives respond to questions from analysts. After these calls, analysts often change their price target recommendations, which are used in equity re- search reports to help investors make deci- sions. In this paper, we examine analysts’ decision making behavior as it pertains to the language content of earnings calls. We identify a set of 20 pragmatic features of analysts’ questions which we correlate with analysts’ pre-call investor recommendations. We also analyze the degree to which semantic and pragmatic features from an earnings call complement market data in predicting analysts’ post-call changes in price targets. Our results show that earnings calls are moderately predictive of analysts’ decisions even though these decisions are influenced by a number of other factors including private communication with company executives and market conditions. A breakdown of model errors indicates disparate performance on calls from different market sectors.",,,,ACL
48,2019,An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis,"Ruidan He, Wee Sun Lee, Hwee Tou Ng, Daniel Dahlmeier","Aspect-based sentiment analysis produces a list of aspect terms and their corresponding sentiments for a natural language sentence. This task is usually done in a pipeline manner, with aspect term extraction performed first, followed by sentiment predictions toward the extracted aspect terms. While easier to develop, such an approach does not fully exploit joint information from the two subtasks and does not use all available sources of training information that might be helpful, such as document-level labeled sentiment corpus. In this paper, we propose an interactive multi-task learning network (IMN) which is able to jointly learn multiple related tasks simultaneously at both the token level as well as the document level. Unlike conventional multi-task learning methods that rely on learning common features for the different tasks, IMN introduces a message passing architecture where information is iteratively passed to different tasks through a shared set of latent variables. Experimental results demonstrate superior performance of the proposed method against multiple baselines on three benchmark datasets.",,,,ACL
49,2019,Decompositional Argument Mining: A General Purpose Approach for Argument Graph Construction,"Debela Gemechu, Chris Reed","This work presents an approach decomposing propositions into four functional components and identify the patterns linking those components to determine argument structure. The entities addressed by a proposition are target concepts and the features selected to make a point about the target concepts are aspects. A line of reasoning is followed by providing evidence for the points made about the target concepts via aspects. Opinions on target concepts and opinions on aspects are used to support or attack the ideas expressed by target concepts and aspects. The relations between aspects, target concepts, opinions on target concepts and aspects are used to infer the argument relations. Propositions are connected iteratively to form a graph structure. The approach is generic in that it is not tuned for a specific corpus and evaluated on three different corpora from the literature: AAEC, AMT, US2016G1tv and achieved an F score of 0.79, 0.77 and 0.64, respectively.",,,,ACL
50,2019,MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations,"Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria","Emotion recognition in conversations is a challenging task that has recently gained popularity due to its potential applications. Until now, however, a large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing. Thus, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends. Each utterance is annotated with emotion and sentiment labels, and encompasses audio, visual and textual modalities. We propose several strong multimodal baselines and show the importance of contextual and multimodal information for emotion recognition in conversations. The full dataset is available for use at http://affective-meld.github.io.",,,,ACL
51,2019,Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification,"Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, Yiwei Lv","Open-domain targeted sentiment analysis aims to detect opinion targets along with their sentiment polarities from a sentence. Prior work typically formulates this task as a sequence tagging problem. However, such formulation suffers from problems such as huge search space and sentiment inconsistency. To address these problems, we propose a span-based extract-then-classify framework, where multiple opinion targets are directly extracted from the sentence under the supervision of target span boundaries, and corresponding polarities are then classified using their span representations. We further investigate three approaches under this framework, namely the pipeline, joint, and collapsed models. Experiments on three benchmark datasets show that our approach consistently outperforms the sequence tagging baseline. Moreover, we find that the pipeline model achieves the best performance compared with the other two models.",,,,ACL
52,2019,Transfer Capsule Network for Aspect Level Sentiment Classification,"Zhuang Chen, Tieyun Qian","Aspect-level sentiment classification aims to determine the sentiment polarity of a sentence towards an aspect. Due to the high cost in annotation, the lack of aspect-level labeled data becomes a major obstacle in this area. On the other hand, document-level labeled data like reviews are easily accessible from online websites. These reviews encode sentiment knowledge in abundant contexts. In this paper, we propose a Transfer Capsule Network (TransCap) model for transferring document-level knowledge to aspect-level sentiment classification. To this end, we first develop an aspect routing approach to encapsulate the sentence-level semantic representations into semantic capsules from both the aspect-level and document-level data. We then extend the dynamic routing approach to adaptively couple the semantic capsules with the class capsules under the transfer learning framework. Experiments on SemEval datasets demonstrate the effectiveness of TransCap.",,,,ACL
53,2019,Progressive Self-Supervised Attention Learning for Aspect-Level Sentiment Analysis,"Jialong Tang, Ziyao Lu, Jinsong Su, Yubin Ge, Linfeng Song","In aspect-level sentiment classification (ASC), it is prevalent to equip dominant neural models with attention mechanisms, for the sake of acquiring the importance of each context word on the given aspect. However, such a mechanism tends to excessively focus on a few frequent words with sentiment polarities, while ignoring infrequent ones. In this paper, we propose a progressive self-supervised attention learning approach for neural ASC models, which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms. Specifically, we iteratively conduct sentiment predictions on all training instances. Particularly, at each iteration, the context word with the maximum attention weight is extracted as the one with active/misleading influence on the correct/incorrect prediction of every instance, and then the word itself is masked for subsequent iterations. Finally, we augment the conventional training objective with a regularization term, which enables ASC models to continue equally focusing on the extracted active context words while decreasing weights of those misleading ones. Experimental results on multiple datasets show that our proposed approach yields better attention mechanisms, leading to substantial improvements over the two state-of-the-art neural ASC models. Source code and trained models are available at https://github.com/DeepLearnXMU/PSSAttention.",,,,ACL
54,2019,Classification and Clustering of Arguments with Contextualized Word Embeddings,"Nils Reimers, Benjamin Schiller, Tilman Beck, Johannes Daxenberger, Christian Stab","We experiment with two recent contextualized word embedding methods (ELMo and BERT) in the context of open-domain argument search. For the first time, we show how to leverage the power of contextualized word embeddings to classify and cluster topic-dependent arguments, achieving impressive results on both tasks and across multiple datasets. For argument classification, we improve the state-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8 percentage points and for the IBM Debater - Evidence Sentences dataset by 7.4 percentage points. For the understudied task of argument clustering, we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the Argument Facet Similarity (AFS) Corpus.",,,,ACL
55,2019,Sentiment Tagging with Partial Labels using Modular Architectures,"Xiao Zhang, Dan Goldwasser","Many NLP learning tasks can be decomposed into several distinct sub-tasks, each associated with a partial label. In this paper we focus on a popular class of learning problems, sequence prediction applied to several sentiment analysis tasks, and suggest a modular learning approach in which different sub-tasks are learned using separate functional modules, combined to perform the final task while sharing information. Our experiments show this approach helps constrain the learning process and can alleviate some of the supervision efforts.",,,,ACL
56,2019,DOER: Dual Cross-Shared RNN for Aspect Term-Polarity Co-Extraction,"Huaishao Luo, Tianrui Li, Bing Liu, Junbo Zhang","This paper focuses on two related subtasks of aspect-based sentiment analysis, namely aspect term extraction and aspect sentiment classification, which we call aspect term-polarity co-extraction. The former task is to extract aspects of a product or service from an opinion document, and the latter is to identify the polarity expressed in the document about these extracted aspects. Most existing algorithms address them as two separate tasks and solve them one by one, or only perform one task, which can be complicated for real applications. In this paper, we treat these two tasks as two sequence labeling problems and propose a novel Dual crOss-sharEd RNN framework (DOER) to generate all aspect term-polarity pairs of the input sentence simultaneously. Specifically, DOER involves a dual recurrent neural network to extract the respective representation of each task, and a cross-shared unit to consider the relationship between them. Experimental results demonstrate that the proposed framework outperforms state-of-the-art baselines on three benchmark datasets.",,,,ACL
57,2019,A Corpus for Modeling User and Language Effects in Argumentation on Online Debating,"Esin Durmus, Claire Cardie","Existing argumentation datasets have succeeded in allowing researchers to develop computational methods for analyzing the content, structure and linguistic features of argumentative text. They have been much less successful in fostering studies of the effect of “user” traits — characteristics and beliefs of the participants — on the debate/argument outcome as this type of user information is generally not available. This paper presents a dataset of 78,376 debates generated over a 10-year period along with surprisingly comprehensive participant profiles. We also complete an example study using the dataset to analyze the effect of selected user traits on the debate outcome in comparison to the linguistic features typically employed in studies of this kind.",,,,ACL
58,2019,Topic Tensor Network for Implicit Discourse Relation Recognition in Chinese,"Sheng Xu, Peifeng Li, Fang Kong, Qiaoming Zhu, Guodong Zhou","In the literature, most of the previous studies on English implicit discourse relation recognition only use sentence-level representations, which cannot provide enough semantic information in Chinese due to its unique paratactic characteristics. In this paper, we propose a topic tensor network to recognize Chinese implicit discourse relations with both sentence-level and topic-level representations. In particular, besides encoding arguments (discourse units) using a gated convolutional network to obtain sentence-level representations, we train a simplified topic model to infer the latent topic-level representations. Moreover, we feed the two pairs of representations to two factored tensor networks, respectively, to capture both the sentence-level interactions and topic-level relevance using multi-slice tensors. Experimentation on CDTB, a Chinese discourse corpus, shows that our proposed model significantly outperforms several state-of-the-art baselines in both micro and macro F1-scores.",,,,ACL
59,2019,Learning from Omission,"Bill McDowell, Noah Goodman","Pragmatic reasoning allows humans to go beyond the literal meaning when interpret- ing language in context. Previous work has shown that such reasoning can improve the performance of already-trained language understanding systems. Here, we explore whether pragmatic reasoning during training can improve the quality of learned meanings. Our experiments on reference game data show that end-to-end pragmatic training produces more accurate utterance interpretation models, especially when data is sparse and language is complex.",,,,ACL
60,2019,Multi-Task Learning for Coherence Modeling,"Youmna Farag, Helen Yannakoudakis","We address the task of assessing discourse coherence, an aspect of text quality that is essential for many NLP tasks, such as summarization and language assessment. We propose a hierarchical neural network trained in a multi-task fashion that learns to predict a document-level coherence score (at the network’s top layers) along with word-level grammatical roles (at the bottom layers), taking advantage of inductive transfer between the two tasks. We assess the extent to which our framework generalizes to different domains and prediction tasks, and demonstrate its effectiveness not only on standard binary evaluation coherence tasks, but also on real-world tasks involving the prediction of varying degrees of coherence, achieving a new state of the art.",,,,ACL
61,2019,Data Programming for Learning Discourse Structure,"Sonia Badene, Kate Thompson, Jean-Pierre Lorré, Nicholas Asher","This paper investigates the advantages and limits of data programming for the task of learning discourse structure. The data programming paradigm implemented in the Snorkel framework allows a user to label training data using expert-composed heuristics, which are then transformed via the “generative step” into probability distributions of the class labels given the training candidates. These results are later generalized using a discriminative model. Snorkel’s attractive promise to create a large amount of annotated data from a smaller set of training data by unifying the output of a set of heuristics has yet to be used for computationally difficult tasks, such as that of discourse attachment, in which one must decide where a given discourse unit attaches to other units in a text in order to form a coherent discourse structure. Although approaching this problem using Snorkel requires significant modifications to the structure of the heuristics, we show that weak supervision methods can be more than competitive with classical supervised learning approaches to the attachment problem.",,,,ACL
62,2019,Evaluating Discourse in Structured Text Representations,"Elisa Ferracane, Greg Durrett, Junyi Jessy Li, Katrin Erk","Discourse structure is integral to understanding a text and is helpful in many NLP tasks. Learning latent representations of discourse is an attractive alternative to acquiring expensive labeled discourse data. Liu and Lapata (2018) propose a structured attention mechanism for text classification that derives a tree over a text, akin to an RST discourse tree. We examine this model in detail, and evaluate on additional discourse-relevant tasks and datasets, in order to assess whether the structured attention improves performance on the end task and whether it captures a text’s discourse structure. We find the learned latent trees have little to no structure and instead focus on lexical cues; even after obtaining more structured trees with proposed model modifications, the trees are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser. Finally, ablation studies show the structured attention provides little benefit, sometimes even hurting performance.",,,,ACL
63,2019,Know What You Don’t Know: Modeling a Pragmatic Speaker that Refers to Objects of Unknown Categories,"Sina Zarrieß, David Schlangen","Zero-shot learning in Language & Vision is the task of correctly labelling (or naming) objects of novel categories. Another strand of work in L&V aims at pragmatically informative rather than “correct” object descriptions, e.g. in reference games. We combine these lines of research and model zero-shot reference games, where a speaker needs to successfully refer to a novel object in an image. Inspired by models of “rational speech acts”, we extend a neural generator to become a pragmatic speaker reasoning about uncertain object categories. As a result of this reasoning, the generator produces fewer nouns and names of distractor categories as compared to a literal speaker. We show that this conversational strategy for dealing with novel objects often improves communicative success, in terms of resolution accuracy of an automatic listener.",,,,ACL
64,2019,End-to-end Deep Reinforcement Learning Based Coreference Resolution,"Hongliang Fei, Xu Li, Dingcheng Li, Ping Li","Recent neural network models have significantly advanced the task of coreference resolution. However, current neural coreference models are usually trained with heuristic loss functions that are computed over a sequence of local decisions. In this paper, we introduce an end-to-end reinforcement learning based coreference resolution model to directly optimize coreference evaluation metrics. Specifically, we modify the state-of-the-art higher-order mention ranking approach in Lee et al. (2018) to a reinforced policy gradient model by incorporating the reward associated with a sequence of coreference linking actions. Furthermore, we introduce maximum entropy regularization for adequate exploration to prevent the model from prematurely converging to a bad local optimum. Our proposed model achieves new state-of-the-art performance on the English OntoNotes v5.0 benchmark.",,,,ACL
65,2019,Implicit Discourse Relation Identification for Open-domain Dialogues,"Mingyu Derek Ma, Kevin Bowden, Jiaqi Wu, Wen Cui, Marilyn Walker","Discourse relation identification has been an active area of research for many years, and the challenge of identifying implicit relations remains largely an unsolved task, especially in the context of an open-domain dialogue system. Previous work primarily relies on a corpora of formal text which is inherently non-dialogic, i.e., news and journals. This data however is not suitable to handle the nuances of informal dialogue nor is it capable of navigating the plethora of valid topics present in open-domain dialogue. In this paper, we designed a novel discourse relation identification pipeline specifically tuned for open-domain dialogue systems. We firstly propose a method to automatically extract the implicit discourse relation argument pairs and labels from a dataset of dialogic turns, resulting in a novel corpus of discourse relation pairs; the first of its kind to attempt to identify the discourse relations connecting the dialogic turns in open-domain discourse. Moreover, we have taken the first steps to leverage the dialogue features unique to our task to further improve the identification of such relations by performing feature ablation and incorporating dialogue features to enhance the state-of-the-art model.",,,,ACL
66,2019,Coreference Resolution with Entity Equalization,"Ben Kantor, Amir Globerson","A key challenge in coreference resolution is to capture properties of entity clusters, and use those in the resolution process. Here we provide a simple and effective approach for achieving this, via an “Entity Equalization” mechanism. The Equalization approach represents each mention in a cluster via an approximation of the sum of all mentions in the cluster. We show how this can be done in a fully differentiable end-to-end manner, thus enabling high-order inferences in the resolution process. Our approach, which also employs BERT embeddings, results in new state-of-the-art results on the CoNLL-2012 coreference resolution task, improving average F1 by 3.6%.",,,,ACL
67,2019,A Cross-Domain Transferable Neural Coherence Model,"Peng Xu, Hamidreza Saghir, Jin Sung Kang, Teng Long, Avishek Joey Bose","Coherence is an important aspect of text quality and is crucial for ensuring its readability. One important limitation of existing coherence models is that training on one domain does not easily generalize to unseen categories of text. Previous work advocates for generative models for cross-domain generalization, because for discriminative models, the space of incoherent sentence orderings to discriminate against during training is prohibitively large. In this work, we propose a local discriminative neural model with a much smaller negative sampling space that can efficiently learn against incorrect orderings. The proposed coherence model is simple in structure, yet it significantly outperforms previous state-of-art methods on a standard benchmark dataset on the Wall Street Journal corpus, as well as in multiple new challenging settings of transfer to unseen categories of discourse on Wikipedia articles.",,,,ACL
68,2019,MOROCO: The Moldavian and Romanian Dialectal Corpus,"Andrei Butnaru, Radu Tudor Ionescu","In this work, we introduce the MOldavian and ROmanian Dialectal COrpus (MOROCO), which is freely available for download at https://github.com/butnaruandrei/MOROCO. The corpus contains 33564 samples of text (with over 10 million tokens) collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports and tech. The data set is divided into 21719 samples for training, 5921 samples for validation and another 5924 samples for testing. For each sample, we provide corresponding dialectal and category labels. This allows us to perform empirical studies on several classification tasks such as (i) binary discrimination of Moldavian versus Romanian text samples, (ii) intra-dialect multi-class categorization by topic and (iii) cross-dialect multi-class categorization by topic. We perform experiments using a shallow approach based on string kernels, as well as a novel deep approach based on character-level convolutional neural networks containing Squeeze-and-Excitation blocks. We also present and analyze the most discriminative features of our best performing model, before and after named entity removal.",,,,ACL
69,2019,Just “OneSeC” for Producing Multilingual Sense-Annotated Data,"Bianca Scarlini, Tommaso Pasini, Roberto Navigli","The well-known problem of knowledge acquisition is one of the biggest issues in Word Sense Disambiguation (WSD), where annotated data are still scarce in English and almost absent in other languages. In this paper we formulate the assumption of One Sense per Wikipedia Category and present OneSeC, a language-independent method for the automatic extraction of hundreds of thousands of sentences in which a target word is tagged with its meaning. Our automatically-generated data consistently lead a supervised WSD model to state-of-the-art performance when compared with other automatic and semi-automatic methods. Moreover, our approach outperforms its competitors on multilingual and domain-specific settings, where it beats the existing state of the art on all languages and most domains. All the training data are available for research purposes at http://trainomatic.org/onesec.",,,,ACL
70,2019,"How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions","Goran Glavaš, Robert Litschko, Sebastian Ruder, Ivan Vulić","Cross-lingual word embeddings (CLEs) facilitate cross-lingual transfer of NLP models. Despite their ubiquitous downstream usage, increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction (BLI). Even the BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we take the first step towards a comprehensive evaluation of CLE models: we thoroughly evaluate both supervised and unsupervised CLE models, for a large number of language pairs, on BLI and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI may hurt downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess simple baselines, which still display competitive performance across the board. We hope our work catalyzes further research on CLE evaluation and model analysis.",,,,ACL
71,2019,SP-10K: A Large-scale Evaluation Set for Selectional Preference Acquisition,"Hongming Zhang, Hantian Ding, Yangqiu Song","Selectional Preference (SP) is a commonly observed language phenomenon and proved to be useful in many natural language processing tasks. To provide a better evaluation method for SP models, we introduce SP-10K, a large-scale evaluation set that provides human ratings for the plausibility of 10,000 SP pairs over five SP relations, covering 2,500 most frequent verbs, nouns, and adjectives in American English. Three representative SP acquisition methods based on pseudo-disambiguation are evaluated with SP-10K. To demonstrate the importance of our dataset, we investigate the relationship between SP-10K and the commonsense knowledge in ConceptNet5 and show the potential of using SP to represent the commonsense knowledge. We also use the Winograd Schema Challenge to prove that the proposed new SP relations are essential for the hard pronoun coreference resolution problem.",,,,ACL
72,2019,A Wind of Change: Detecting and Evaluating Lexical Semantic Change across Times and Domains,"Dominik Schlechtweg, Anna Hätty, Marco Del Tredici, Sabine Schulte im Walde","We perform an interdisciplinary large-scale evaluation for detecting lexical semantic divergences in a diachronic and in a synchronic task: semantic sense changes across time, and semantic sense changes across domains. Our work addresses the superficialness and lack of comparison in assessing models of diachronic lexical change, by bringing together and extending benchmark models on a common state-of-the-art evaluation task. In addition, we demonstrate that the same evaluation task and modelling approaches can successfully be utilised for the synchronic detection of domain-specific sense divergences in the field of term extraction.",,,,ACL
73,2019,"Errudite: Scalable, Reproducible, and Testable Error Analysis","Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, Daniel Weld","Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.",,,,ACL
74,2019,DocRED: A Large-Scale Document-Level Relation Extraction Dataset,"Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin","Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.",,,,ACL
75,2019,ChID: A Large-scale Chinese IDiom Dataset for Cloze Test,"Chujie Zheng, Minlie Huang, Aixin Sun","Cloze-style reading comprehension in Chinese is still limited due to the lack of various corpora. In this paper we propose a large-scale Chinese cloze test dataset ChID, which studies the comprehension of idiom, a unique language phenomenon in Chinese. In this corpus, the idioms in a passage are replaced by blank symbols and the correct answer needs to be chosen from well-designed candidate idioms. We carefully study how the design of candidate idioms and the representation of idioms affect the performance of state-of-the-art models. Results show that the machine accuracy is substantially worse than that of human, indicating a large space for further research.",,,,ACL
76,2019,Automatic Evaluation of Local Topic Quality,"Jeffrey Lund, Piper Armstrong, Wilson Fearn, Stephen Cowley, Courtni Byun","Topic models are typically evaluated with respect to the global topic distributions that they generate, using metrics such as coherence, but without regard to local (token-level) topic assignments. Token-level assignments are important for downstream tasks such as classification. Even recent models, which aim to improve the quality of these token-level topic assignments, have been evaluated only with respect to global metrics. We propose a task designed to elicit human judgments of token-level topic assignments. We use a variety of topic model types and parameters and discover that global metrics agree poorly with human assignments. Since human evaluation is expensive we propose a variety of automated metrics to evaluate topic models at a local level. Finally, we correlate our proposed metrics with human judgments from the task on several datasets. We show that an evaluation based on the percent of topic switches correlates most strongly with human judgment of local topic quality. We suggest that this new metric, which we call consistency, be adopted alongside global metrics such as topic coherence when evaluating new topic models.",,,,ACL
77,2019,Crowdsourcing and Aggregating Nested Markable Annotations,"Chris Madge, Juntao Yu, Jon Chamberlain, Udo Kruschwitz, Silviu Paun","One of the key steps in language resource creation is the identification of the text segments to be annotated, or markables, which depending on the task may vary from nominal chunks for named entity resolution to (potentially nested) noun phrases in coreference resolution (or mentions) to larger text segments in text segmentation. Markable identification is typically carried out semi-automatically, by running a markable identifier and correcting its output by hand–which is increasingly done via annotators recruited through crowdsourcing and aggregating their responses. In this paper, we present a method for identifying markables for coreference annotation that combines high-performance automatic markable detectors with checking with a Game-With-A-Purpose (GWAP) and aggregation using a Bayesian annotation model. The method was evaluated both on news data and data from a variety of other genres and results in an improvement on F1 of mention boundaries of over seven percentage points when compared with a state-of-the-art, domain-independent automatic mention detector, and almost three points over an in-domain mention detector. One of the key contributions of our proposal is its applicability to the case in which markables are nested, as is the case with coreference markables; but the GWAP and several of the proposed markable detectors are task and language-independent and are thus applicable to a variety of other annotation scenarios.",,,,ACL
78,2019,Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems,"Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher","Over-dependence on domain ontology and lack of sharing knowledge across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short when tracking unknown slot values during inference and often have difficulties in adapting to new domains. In this paper, we propose a Transferable Dialogue State Generator (TRADE) that generates dialogue states from utterances using copy mechanism, facilitating transfer when predicting (domain, slot, value) triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art 48.62% joint goal accuracy for the five domains of MultiWOZ, a human-human dialogue dataset. In addition, we show the transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves 60.58% joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains.",,,,ACL
79,2019,"Multi-Task Networks with Universe, Group, and Task Feature Learning","Shiva Pentyala, Mengwen Liu, Markus Dreyer","We present methods for multi-task learning that take advantage of natural groupings of related tasks. Task groups may be defined along known properties of the tasks, such as task domain or language. Such task groups represent supervised information at the inter-task level and can be encoded into the model. We investigate two variants of neural network architectures that accomplish this, learning different feature spaces at the levels of individual tasks, task groups, as well as the universe of all tasks: (1) parallel architectures encode each input simultaneously into feature spaces at different levels; (2) serial architectures encode each input successively into feature spaces at different levels in the task hierarchy. We demonstrate the methods on natural language understanding (NLU) tasks, where a grouping of tasks into different task domains leads to improved performance on ATIS, Snips, and a large in-house dataset.",,,,ACL
80,2019,Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue,"Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, Rajen Subba","Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Avenues like the E2E NLG Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem. The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset.",,,,ACL
81,2019,OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs,"Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba","We study a conversational reasoning model that strategically traverses through a large-scale common fact knowledge graph (KG) to introduce engaging and contextually diverse entities and attributes. For this study, we collect a new Open-ended Dialog <-> KG parallel corpus called OpenDialKG, where each utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. We then propose the DialKG Walker model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder. Automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-of-the-art baselines or rule-based models, in both in-domain and cross-domain tasks. The proposed model also generates a KG walk path for each entity retrieved, providing a natural way to explain conversational reasoning.",,,,ACL
82,2019,Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing,"Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, Jian Yin","In this paper, we present an approach to incorporate retrieved datapoints as supporting evidence for context-dependent semantic parsing, such as generating source code conditioned on the class environment. Our approach naturally combines a retrieval model and a meta-learner, where the former learns to find similar datapoints from the training data, and the latter considers retrieved datapoints as a pseudo task for fast adaptation. Specifically, our retriever is a context-aware encoder-decoder model with a latent variable which takes context environment into consideration, and our meta-learner learns to utilize retrieved datapoints in a model-agnostic meta-learning paradigm for fast adaptation. We conduct experiments on CONCODE and CSQA datasets, where the context refers to class environment in JAVA codes and conversational history, respectively. We use sequence-to-action model as the base semantic parser, which performs the state-of-the-art accuracy on both datasets. Results show that both the context-aware retriever and the meta-learning strategy improve accuracy, and our approach performs better than retrieve-and-edit baselines.",,,,ACL
83,2019,Knowledge-aware Pronoun Coreference Resolution,"Hongming Zhang, Yan Song, Yangqiu Song, Dong Yu","Resolving pronoun coreference requires knowledge support, especially for particular domains (e.g., medicine). In this paper, we explore how to leverage different types of knowledge to better resolve pronoun coreference with a neural model. To ensure the generalization ability of our model, we directly incorporate knowledge in the format of triplets, which is the most common format of modern knowledge graphs, instead of encoding it with features or rules as that in conventional approaches. Moreover, since not all knowledge is helpful in certain contexts, to selectively use them, we propose a knowledge attention module, which learns to select and use informative knowledge based on contexts, to enhance our model. Experimental results on two datasets from different domains prove the validity and effectiveness of our model, where it outperforms state-of-the-art baselines by a large margin. Moreover, since our model learns to use external knowledge rather than only fitting the training data, it also demonstrates superior performance to baselines in the cross-domain setting.",,,,ACL
84,2019,Don’t Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference,"Yonatan Belinkov, Adam Poliak, Stuart Shieber, Benjamin Van Durme, Alexander Rush","Natural Language Inference (NLI) datasets often contain hypothesis-only biases—artifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis. We propose two probabilistic methods to build models that are more robust to such biases and better transfer across datasets. In contrast to standard approaches to NLI, our methods predict the probability of a premise given a hypothesis and NLI label, discouraging models from ignoring the premise. We evaluate our methods on synthetic and existing NLI datasets by training on datasets containing biases and testing on datasets containing no (or different) hypothesis-only biases. Our results indicate that these methods can make NLI models more robust to dataset-specific artifacts, transferring better than a baseline architecture in 9 out of 12 NLI datasets. Additionally, we provide an extensive analysis of the interplay of our methods with known biases in NLI datasets, as well as the effects of encouraging models to ignore biases and fine-tuning on target datasets.",,,,ACL
85,2019,GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification,"Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang","Fact verification (FV) is a challenging task which requires to retrieve relevant evidence from plain text and use the evidence to verify given claims. Many claims require to simultaneously integrate and reason over several pieces of evidence for verification. However, previous work employs simple models to extract information from evidence without letting evidence communicate with each other, e.g., merely concatenate the evidence for processing. Therefore, these methods are unable to grasp sufficient relational and logical information among the evidence. To alleviate this issue, we propose a graph-based evidence aggregating and reasoning (GEAR) framework which enables information to transfer on a fully-connected evidence graph and then utilizes different aggregators to collect multi-evidence information. We further employ BERT, an effective pre-trained language representation model, to improve the performance. Experimental results on a large-scale benchmark dataset FEVER have demonstrated that GEAR could leverage multi-evidence information for FV and thus achieves the promising result with a test FEVER score of 67.10%. Our code is available at https://github.com/thunlp/GEAR.",,,,ACL
86,2019,SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference,"Martin Schmitt, Hinrich Schütze","We present SherLIiC, a testbed for lexical inference in context (LIiC), consisting of 3985 manually annotated inference rule candidates (InfCands), accompanied by (i) ~960k unlabeled InfCands, and (ii) ~190k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09. Each InfCand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more Freebase types. Due to our candidate selection process based on strong distributional evidence, SherLIiC is much harder than existing testbeds because distributional evidence is of little utility in the classification of InfCands. We also show that, due to its construction, many of SherLIiC’s correct InfCands are novel and missing from existing rule bases. We evaluate a large number of strong baselines on SherLIiC, ranging from semantic vector space models to state of the art neural models of natural language inference (NLI). We show that SherLIiC poses a tough challenge to existing NLI systems.",,,,ACL
87,2019,Extracting Symptoms and their Status from Clinical Conversations,"Nan Du, Kai Chen, Anjuli Kannan, Linh Tran, Yuhui Chen","This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models.",,,,ACL
88,2019,What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations,"Verónica Pérez-Rosas, Xinyi Wu, Kenneth Resnicow, Rada Mihalcea","The quality of a counseling intervention relies highly on the active collaboration between clients and counselors. In this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants’ turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.",,,,ACL
89,2019,Finding Your Voice: The Linguistic Development of Mental Health Counselors,"Justine Zhang, Robert Filbin, Christine Morrison, Jaclyn Weiser, Cristian Danescu-Niculescu-Mizil","Mental health counseling is an enterprise with profound societal importance where conversations play a primary role. In order to acquire the conversational skills needed to face a challenging range of situations, mental health counselors must rely on training and on continued experience with actual clients. However, in the absence of large scale longitudinal studies, the nature and significance of this developmental process remain unclear. For example, prior literature suggests that experience might not translate into consequential changes in counselor behavior. This has led some to even argue that counseling is a profession without expertise. In this work, we develop a computational framework to quantify the extent to which individuals change their linguistic behavior with experience and to study the nature of this evolution. We use our framework to conduct a large longitudinal study of mental health counseling conversations, tracking over 3,400 counselors across their tenure. We reveal that overall, counselors do indeed change their conversational behavior to become more diverse across interactions, developing an individual voice that distinguishes them from other counselors. Furthermore, a finer-grained investigation shows that the rate and nature of this diversification vary across functionally different conversational components.",,,,ACL
90,2019,Towards Automating Healthcare Question Answering in a Noisy Multilingual Low-Resource Setting,"Jeanne E. Daniel, Willie Brink, Ryan Eloff, Charles Copley","We discuss ongoing work into automating a multilingual digital helpdesk service available via text messaging to pregnant and breastfeeding mothers in South Africa. Our anonymized dataset consists of short informal questions, often in low-resource languages, with unreliable language labels, spelling errors and code-mixing, as well as template answers with some inconsistencies. We explore cross-lingual word embeddings, and train parametric and non-parametric models on 90K samples for answer selection from a set of 126 templates. Preliminary results indicate that LSTMs trained end-to-end perform best, with a test accuracy of 62.13% and a recall@5 of 89.56%, and demonstrate that we can accelerate response time by several orders of magnitude.",,,,ACL
91,2019,Joint Entity Extraction and Assertion Detection for Clinical Text,"Parminder Bhatia, Busra Celikkaya, Mohammed Khalilia","Negative medical findings are prevalent in clinical reports, yet discriminating them from positive findings remains a challenging task for in-formation extraction. Most of the existing systems treat this task as a pipeline of two separate tasks, i.e., named entity recognition (NER)and rule-based negation detection. We consider this as a multi-task problem and present a novel end-to-end neural model to jointly extract entities and negations. We extend a standard hierarchical encoder-decoder NER model and first adopt a shared encoder followed by separate decoders for the two tasks. This architecture performs considerably better than the previous rule-based and machine learning-based systems. To overcome the problem of increased parameter size especially for low-resource settings, we propose the Conditional Softmax Shared Decoder architecture which achieves state-of-art results for NER and negation detection on the 2010 i2b2/VA challenge dataset and a proprietary de-identified clinical dataset.",,,,ACL
92,2019,HEAD-QA: A Healthcare Dataset for Complex Reasoning,"David Vilares, Carlos Gómez-Rodríguez","We present HEAD-QA, a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. We then consider monolingual (Spanish) and cross-lingual (to English) experiments with information retrieval and neural techniques. We show that: (i) HEAD-QA challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work.",,,,ACL
93,2019,Are You Convinced? Choosing the More Convincing Evidence with a Siamese Network,"Martin Gleize, Eyal Shnarch, Leshem Choshen, Lena Dankin, Guy Moshkowich","With the advancement in argument detection, we suggest to pay more attention to the challenging task of identifying the more convincing arguments. Machines capable of responding and interacting with humans in helpful ways have become ubiquitous. We now expect them to discuss with us the more delicate questions in our world, and they should do so armed with effective arguments. But what makes an argument more persuasive? What will convince you? In this paper, we present a new data set, IBM-EviConv, of pairs of evidence labeled for convincingness, designed to be more challenging than existing alternatives. We also propose a Siamese neural network architecture shown to outperform several baselines on both a prior convincingness data set and our own. Finally, we provide insights into our experimental results and the various kinds of argumentative value our method is capable of detecting.",,,,ACL
94,2019,From Surrogacy to Adoption; From Bitcoin to Cryptocurrency: Debate Topic Expansion,"Roy Bar-Haim, Dalia Krieger, Orith Toledo-Ronen, Lilach Edelstein, Yonatan Bilu","When debating a controversial topic, it is often desirable to expand the boundaries of discussion. For example, we may consider the pros and cons of possible alternatives to the debate topic, make generalizations, or give specific examples. We introduce the task of Debate Topic Expansion - finding such related topics for a given debate topic, along with a novel annotated dataset for the task. We focus on relations between Wikipedia concepts, and show that they differ from well-studied lexical-semantic relations such as hypernyms, hyponyms and antonyms. We present algorithms for finding both consistent and contrastive expansions and demonstrate their effectiveness empirically. We suggest that debate topic expansion may have various use cases in argumentation mining.",,,,ACL
95,2019,Multimodal and Multi-view Models for Emotion Recognition,"Gustavo Aguilar, Viktor Rozgic, Weiran Wang, Chao Wang","Studies on emotion recognition (ER) show that combining lexical and acoustic information results in more robust and accurate models. The majority of the studies focus on settings where both modalities are available in training and evaluation. However, in practice, this is not always the case; getting ASR output may represent a bottleneck in a deployment pipeline due to computational complexity or privacy-related constraints. To address this challenge, we study the problem of efficiently combining acoustic and lexical modalities during training while still providing a deployable acoustic model that does not require lexical inputs. We first experiment with multimodal models and two attention mechanisms to assess the extent of the benefits that lexical information can provide. Then, we frame the task as a multi-view learning problem to induce semantic information from a multimodal model into our acoustic-only network using a contrastive loss function. Our multimodal model outperforms the previous state of the art on the USC-IEMOCAP dataset reported on lexical and acoustic information. Additionally, our multi-view-trained acoustic network significantly surpasses models that have been exclusively trained with acoustic features.",,,,ACL
96,2019,Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts,"Rui Xia, Zixiang Ding","Emotion cause extraction (ECE), the task aimed at extracting the potential causes behind certain emotions in text, has gained much attention in recent years due to its wide applications. However, it suffers from two shortcomings: 1) the emotion must be annotated before cause extraction in ECE, which greatly limits its applications in real-world scenarios; 2) the way to first annotate emotion and then extract the cause ignores the fact that they are mutually indicative. In this work, we propose a new task: emotion-cause pair extraction (ECPE), which aims to extract the potential pairs of emotions and corresponding causes in a document. We propose a 2-step approach to address this new ECPE task, which first performs individual emotion extraction and cause extraction via multi-task learning, and then conduct emotion-cause pairing and filtering. The experimental results on a benchmark emotion cause corpus prove the feasibility of the ECPE task as well as the effectiveness of our approach.",,,,ACL
97,2019,Argument Invention from First Principles,"Yonatan Bilu, Ariel Gera, Daniel Hershcovich, Benjamin Sznajder, Dan Lahav","Competitive debaters often find themselves facing a challenging task – how to debate a topic they know very little about, with only minutes to prepare, and without access to books or the Internet? What they often do is rely on ”first principles”, commonplace arguments which are relevant to many topics, and which they have refined in past debates. In this work we aim to explicitly define a taxonomy of such principled recurring arguments, and, given a controversial topic, to automatically identify which of these arguments are relevant to the topic. As far as we know, this is the first time that this approach to argument invention is formalized and made explicit in the context of NLP. The main goal of this work is to show that it is possible to define such a taxonomy. While the taxonomy suggested here should be thought of as a ”first attempt” it is nonetheless coherent, covers well the relevant topics and coincides with what professional debaters actually argue in their speeches, and facilitates automatic argument invention for new topics.",,,,ACL
98,2019,Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization,"Sangwoo Cho, Logan Lebanoff, Hassan Foroosh, Fei Liu","The most important obstacles facing multi-document summarization include excessive redundancy in source descriptions and the looming shortage of training data. These obstacles prevent encoder-decoder models from being used directly, but optimization-based methods such as determinantal point processes (DPPs) are known to handle them well. In this paper we seek to strengthen a DPP-based method for extractive multi-document summarization by presenting a novel similarity measure inspired by capsule networks. The approach measures redundancy between a pair of sentences based on surface form and semantic information. We show that our DPP system with improved similarity measure performs competitively, outperforming strong summarization baselines on benchmark datasets. Our findings are particularly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions.",,,,ACL
99,2019,Global Optimization under Length Constraint for Neural Text Summarization,"Takuya Makino, Tomoya Iwakura, Hiroya Takamura, Manabu Okumura","We propose a global optimization method under length constraint (GOLC) for neural text summarization models. GOLC increases the probabilities of generating summaries that have high evaluation scores, ROUGE in this paper, within a desired length. We compared GOLC with two optimization methods, a maximum log-likelihood and a minimum risk training, on CNN/Daily Mail and a Japanese single document summarization data set of The Mainichi Shimbun Newspapers. The experimental results show that a state-of-the-art neural summarization model optimized with GOLC generates fewer overlength summaries while maintaining the fastest processing speed; only 6.70% overlength summaries on CNN/Daily and 7.8% on long summary of Mainichi, compared to the approximately 20% to 50% on CNN/Daily Mail and 10% to 30% on Mainichi with the other optimization methods. We also demonstrate the importance of the generation of in-length summaries for post-editing with the dataset Mainich that is created with strict length constraints. The ex- perimental results show approximately 30% to 40% improved post-editing time by use of in-length summaries.",,,,ACL
100,2019,Searching for Effective Neural Extractive Summarization: What Works and What’s Next,"Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu, Xuanjing Huang","The recent years have seen remarkable success in the use of deep neural networks on text summarization. However, there is no clear understanding of why they perform so well, or how they might be improved. In this paper, we seek to better understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Besides, we find an effective way to improve the current framework and achieve the state-of-the-art result on CNN/DailyMail by a large margin based on our observations and analysis. Hopefully, our work could provide more hints for future research on extractive summarization.",,,,ACL
101,2019,A Simple Theoretical Model of Importance for Summarization,Maxime Peyrard,"Research on summarization has mainly been driven by empirical approaches, crafting systems to perform well on standard datasets with the notion of information Importance remaining latent. We argue that establishing theoretical models of Importance will advance our understanding of the task and help to further improve summarization systems. To this end, we propose simple but rigorous definitions of several concepts that were previously used only intuitively in summarization: Redundancy, Relevance, and Informativeness. Importance arises as a single quantity naturally unifying these concepts. Additionally, we provide intuitions to interpret the proposed quantities and experiments to demonstrate the potential of the framework to inform and guide subsequent works.",,,,ACL
102,2019,Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model,"Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir Radev","Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",,,,ACL
103,2019,Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency,"Shuhuai Ren, Yihe Deng, Kun He, Wanxiang Che","We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples.",,,,ACL
104,2019,Heuristic Authorship Obfuscation,"Janek Bevendorff, Martin Potthast, Matthias Hagen, Benno Stein","Authorship verification is the task of determining whether two texts were written by the same author. We deal with the adversary task, called authorship obfuscation: preventing verification by altering a to-be-obfuscated text. Our new obfuscation approach (1) models writing style difference as the Jensen-Shannon distance between the character n-gram distributions of texts, and (2) manipulates an author’s subconsciously encoded writing style in a sophisticated manner using heuristic search. To obfuscate, we analyze the huge space of textual variants for a paraphrased version of the to-be-obfuscated text that has a sufficient Jensen-Shannon distance at minimal costs in terms of text quality. We analyze, quantify, and illustrate the rationale of this approach, define paraphrasing operators, derive obfuscation thresholds, and develop an effective obfuscation framework. Our authorship obfuscation approach defeats state-of-the-art verification approaches, including unmasking and compression models, while keeping text changes at a minimum.",,,,ACL
105,2019,Text Categorization by Learning Predominant Sense of Words as Auxiliary Task,"Kazuya Shimura, Jiyi Li, Fumiyo Fukumoto","Distributions of the senses of words are often highly skewed and give a strong influence of the domain of a document. This paper follows the assumption and presents a method for text categorization by leveraging the predominant sense of words depending on the domain, i.e., domain-specific senses. The key idea is that the features learned from predominant senses are possible to discriminate the domain of the document and thus improve the overall performance of text categorization. We propose multi-task learning framework based on the neural network model, transformer, which trains a model to simultaneously categorize documents and predicts a predominant sense for each word. The experimental results using four benchmark datasets show that our method is comparable to the state-of-the-art categorization approach, especially our model works well for categorization of multi-label documents.",,,,ACL
106,2019,DeepSentiPeer: Harnessing Sentiment in Review Texts to Recommend Peer Review Decisions,"Tirthankar Ghosal, Rajeev Verma, Asif Ekbal, Pushpak Bhattacharyya","Automatically validating a research artefact is one of the frontiers in Artificial Intelligence (AI) that directly brings it close to competing with human intellect and intuition. Although criticised sometimes, the existing peer review system still stands as the benchmark of research validation. The present-day peer review process is not straightforward and demands profound domain knowledge, expertise, and intelligence of human reviewer(s), which is somewhat elusive with the current state of AI. However, the peer review texts, which contains rich sentiment information of the reviewer, reflecting his/her overall attitude towards the research in the paper, could be a valuable entity to predict the acceptance or rejection of the manuscript under consideration. Here in this work, we investigate the role of reviewer sentiment embedded within peer review texts to predict the peer review outcome. Our proposed deep neural architecture takes into account three channels of information: the paper, the corresponding reviews, and review’s polarity to predict the overall recommendation score as well as the final decision. We achieve significant performance improvement over the baselines (∼ 29% error reduction) proposed in a recently released dataset of peer reviews. An AI of this kind could assist the editors/program chairs as an additional layer of confidence, especially when non-responding/missing reviewers are frequent in present day peer review.",,,,ACL
107,2019,Gated Embeddings in End-to-End Speech Recognition for Conversational-Context Fusion,"Suyoun Kim, Siddharth Dalmia, Florian Metze","We present a novel conversational-context aware end-to-end speech recognizer based on a gated neural network that incorporates conversational-context/word/speech embeddings. Unlike conventional speech recognition models, our model learns longer conversational-context information that spans across sentences and is consequently better at recognizing long conversations. Specifically, we propose to use text-based external word and/or sentence embeddings (i.e., fastText, BERT) within an end-to-end framework, yielding significant improvement in word error rate with better conversational-context representation. We evaluated the models on the Switchboard conversational speech corpus and show that our model outperforms standard end-to-end speech recognition models.",,,,ACL
108,2019,Figurative Usage Detection of Symptom Words to Improve Personal Health Mention Detection,"Adith Iyer, Aditya Joshi, Sarvnaz Karimi, Ross Sparks, Cecile Paris","Personal health mention detection deals with predicting whether or not a given sentence is a report of a health condition. Past work mentions errors in this prediction when symptom words, i.e., names of symptoms of interest, are used in a figurative sense. Therefore, we combine a state-of-the-art figurative usage detection with CNN-based personal health mention detection. To do so, we present two methods: a pipeline-based approach and a feature augmentation-based approach. The introduction of figurative usage detection results in an average improvement of 2.21% F-score of personal health mention detection, in the case of the feature augmentation-based approach. This paper demonstrates the promise of using figurative usage detection to improve personal health mention detection.",,,,ACL
109,2019,Complex Word Identification as a Sequence Labelling Task,"Sian Gooding, Ekaterina Kochmar","Complex Word Identification (CWI) is concerned with detection of words in need of simplification and is a crucial first step in a simplification pipeline. It has been shown that reliable CWI systems considerably improve text simplification. However, most CWI systems to date address the task on a word-by-word basis, not taking the context into account. In this paper, we present a novel approach to CWI based on sequence modelling. Our system is capable of performing CWI in context, does not require extensive feature engineering and outperforms state-of-the-art systems on this task.",,,,ACL
110,2019,Neural News Recommendation with Topic-Aware News Representation,"Chuhan Wu, Fangzhao Wu, Mingxiao An, Yongfeng Huang, Xing Xie","News recommendation can help users find interested news and alleviate information overload. The topic information of news is critical for learning accurate news and user representations for news recommendation. However, it is not considered in many existing news recommendation methods. In this paper, we propose a neural news recommendation approach with topic-aware news representations. The core of our approach is a topic-aware news encoder and a user encoder. In the news encoder we learn representations of news from their titles via CNN networks and apply attention networks to select important words. In addition, we propose to learn topic-aware news representations by jointly training the news encoder with an auxiliary topic classification task. In the user encoder we learn the representations of users from their browsed news and use attention networks to select informative news for user representation learning. Extensive experiments on a real-world dataset validate the effectiveness of our approach.",,,,ACL
111,2019,Poetry to Prose Conversion in Sanskrit as a Linearisation Task: A Case for Low-Resource Languages,"Amrith Krishna, Vishnu Sharma, Bishal Santra, Aishik Chakraborty, Pavankumar Satuluri","The word ordering in a Sanskrit verse is often not aligned with its corresponding prose order. Conversion of the verse to its corresponding prose helps in better comprehension of the construction. Owing to the resource constraints, we formulate this task as a word ordering (linearisation) task. In doing so, we completely ignore the word arrangement at the verse side. kāvya guru, the approach we propose, essentially consists of a pipeline of two pretraining steps followed by a seq2seq model. The first pretraining step learns task-specific token embeddings from pretrained embeddings. In the next step, we generate multiple possible hypotheses for possible word arrangements of the input %using another pretraining step. We then use them as inputs to a neural seq2seq model for the final prediction. We empirically show that the hypotheses generated by our pretraining step result in predictions that consistently outperform predictions based on the original order in the verse. Overall, kāvya guru outperforms current state of the art models in linearisation for the poetry to prose conversion task in Sanskrit.",,,,ACL
112,2019,Learning Emphasis Selection for Written Text in Visual Media from Crowd-Sourced Label Distributions,"Amirreza Shirani, Franck Dernoncourt, Paul Asente, Nedim Lipka, Seokhwan Kim","In visual communication, text emphasis is used to increase the comprehension of written text to convey the author’s intent. We study the problem of emphasis selection, i.e. choosing candidates for emphasis in short written text, to enable automated design assistance in authoring. Without knowing the author’s intent and only considering the input text, multiple emphasis selections are valid. We propose a model that employs end-to-end label distribution learning (LDL) on crowd-sourced data and predicts a selection distribution, capturing the inter-subjectivity (common-sense) in the audience as well as the ambiguity of the input. We compare the model with several baselines in which the problem is transformed to single-label learning by mapping label distributions to absolute labels via majority voting.",,,,ACL
113,2019,"Rumor Detection by Exploiting User Credibility Information, Attention and Multi-task Learning","Quanzhi Li, Qiong Zhang, Luo Si","In this study, we propose a new multi-task learning approach for rumor detection and stance classification tasks. This neural network model has a shared layer and two task specific layers. We incorporate the user credibility information into the rumor detection layer, and we also apply attention mechanism in the rumor detection process. The attended information include not only the hidden states in the rumor detection layer, but also the hidden states from the stance detection layer. The experiments on two datasets show that our proposed model outperforms the state-of-the-art rumor detection approaches.",,,,ACL
114,2019,Context-specific Language Modeling for Human Trafficking Detection from Online Advertisements,"Saeideh Shahrokh Esfahani, Michael J. Cafarella, Maziyar Baran Pouyan, Gregory DeAngelo, Elena Eneva","Human trafficking is a worldwide crisis. Traffickers exploit their victims by anonymously offering sexual services through online advertisements. These ads often contain clues that law enforcement can use to separate out potential trafficking cases from volunteer sex advertisements. The problem is that the sheer volume of ads is too overwhelming for manual processing. Ideally, a centralized semi-automated tool can be used to assist law enforcement agencies with this task. Here, we present an approach using natural language processing to identify trafficking ads on these websites. We propose a classifier by integrating multiple text feature sets, including the publicly available pre-trained textual language model Bi-directional Encoder Representation from transformers (BERT). In this paper, we demonstrate that a classifier using this composite feature set has significantly better performance compared to any single feature set alone.",,,,ACL
115,2019,Self-Attentional Models for Lattice Inputs,"Matthias Sperber, Graham Neubig, Ngoc-Quan Pham, Alex Waibel","Lattices are an efficient and effective method to encode ambiguity of upstream systems in natural language processing tasks, for example to compactly capture multiple speech recognition hypotheses, or to represent multiple linguistic analyses. Previous work has extended recurrent neural networks to model lattice inputs and achieved improvements in various tasks, but these models suffer from very slow computation speeds. This paper extends the recently proposed paradigm of self-attention to handle lattice inputs. Self-attention is a sequence modeling technique that relates inputs to one another by computing pairwise similarities and has gained popularity for both its strong results and its computational efficiency. To extend such models to handle lattices, we introduce probabilistic reachability masks that incorporate lattice structure into the model and support lattice scores if available. We also propose a method for adapting positional embeddings to lattice structures. We apply the proposed model to a speech translation task and find that it outperforms all examined baselines while being much faster to compute than previous neural lattice models during both training and inference.",,,,ACL
116,2019,"When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion","Elena Voita, Rico Sennrich, Ivan Titov","Though machine translation errors caused by the lack of context beyond one sentence have long been acknowledged, the development of context-aware NMT systems is hampered by several problems. Firstly, standard metrics are not sensitive to improvements in consistency in document-level translations. Secondly, previous work on context-aware NMT assumed that the sentence-aligned parallel data consisted of complete documents while in most practical scenarios such document-level data constitutes only a fraction of the available parallel data. To address the first issue, we perform a human study on an English-Russian subtitles dataset and identify deixis, ellipsis and lexical cohesion as three main sources of inconsistency. We then create test sets targeting these phenomena. To address the second shortcoming, we consider a set-up in which a much larger amount of sentence-level data is available compared to that aligned at the document level. We introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU.",,,,ACL
117,2019,A Compact and Language-Sensitive Multilingual Translation Method,"Yining Wang, Long Zhou, Jiajun Zhang, Feifei Zhai, Jingfang Xu","Multilingual neural machine translation (Multi-NMT) with one encoder-decoder model has made remarkable progress due to its simple deployment. However, this multilingual translation paradigm does not make full use of language commonality and parameter sharing between encoder and decoder. Furthermore, this kind of paradigm cannot outperform the individual models trained on bilingual corpus in most cases. In this paper, we propose a compact and language-sensitive method for multilingual translation. To maximize parameter sharing, we first present a universal representor to replace both encoder and decoder models. To make the representor sensitive for specific languages, we further introduce language-sensitive embedding, attention, and discriminator with the ability to enhance model performance. We verify our methods on various translation scenarios, including one-to-many, many-to-many and zero-shot. Extensive experiments demonstrate that our proposed methods remarkably outperform strong standard multilingual translation systems on WMT and IWSLT datasets. Moreover, we find that our model is especially helpful in low-resource and zero-shot translation scenarios.",,,,ACL
118,2019,Unsupervised Parallel Sentence Extraction with Parallel Segment Detection Helps Machine Translation,"Viktor Hangya, Alexander Fraser","Mining parallel sentences from comparable corpora is important. Most previous work relies on supervised systems, which are trained on parallel data, thus their applicability is problematic in low-resource scenarios. Recent developments in building unsupervised bilingual word embeddings made it possible to mine parallel sentences based on cosine similarities of source and target language words. We show that relying only on this information is not enough, since sentences often have similar words but different meanings. We detect continuous parallel segments in sentence pair candidates and rely on them when mining parallel sentences. We show better mining accuracy on three language pairs in a standard shared task on artificial data. We also provide the first experiments showing that parallel sentences mined from real life sources improve unsupervised MT. Our code is available, we hope it will be used to support low-resource MT research.",,,,ACL
119,2019,Unsupervised Bilingual Word Embedding Agreement for Unsupervised Neural Machine Translation,"Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita","Unsupervised bilingual word embedding (UBWE), together with other technologies such as back-translation and denoising, has helped unsupervised neural machine translation (UNMT) achieve remarkable results in several language pairs. In previous methods, UBWE is first trained using non-parallel monolingual corpora and then this pre-trained UBWE is used to initialize the word embedding in the encoder and decoder of UNMT. That is, the training of UBWE and UNMT are separate. In this paper, we first empirically investigate the relationship between UBWE and UNMT. The empirical findings show that the performance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT.",,,,ACL
120,2019,Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies,"Yunsu Kim, Yingbo Gao, Hermann Ney","Transfer learning or multilingual model is essential for low-resource neural machine translation (NMT), but the applicability is limited to cognate languages by sharing their vocabularies. This paper shows effective techniques to transfer a pretrained NMT model to a new, unrelated language without shared vocabularies. We relieve the vocabulary mismatch by using cross-lingual word embedding, train a more language-agnostic encoder by injecting artificial noises, and generate synthetic data easily from the pretraining data without back-translation. Our methods do not require restructuring the vocabulary or retraining the model. We improve plain NMT transfer by up to +5.1% BLEU in five low-resource translation tasks, outperforming multilingual joint training by a large margin. We also provide extensive ablation studies on pretrained embedding, synthetic data, vocabulary size, and parameter freezing for a better understanding of NMT transfer.",,,,ACL
121,2019,Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations,"Jiatao Gu, Yong Wang, Kyunghyun Cho, Victor O.K. Li","Zero-shot translation, translating between language pairs on which a Neural Machine Translation (NMT) system has never been trained, is an emergent property when training the system in multilingual settings. However, naive training for zero-shot NMT easily fails, and is sensitive to hyper-parameter setting. The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot. In this work, we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the mutual information between language IDs of the source and decoded sentences. Inspired by this analysis, we propose to use two simple but effective approaches: (1) decoder pre-training; (2) back-translation. These methods show significant improvement (4 22 BLEU points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach.",,,,ACL
122,2019,Syntactically Supervised Transformers for Faster Neural Machine Translation,"Nader Akoury, Kalpesh Krishna, Mohit Iyyer","Standard decoders for neural machine translation autoregressively generate a single target token per timestep, which slows inference especially for long outputs. While architectural advances such as the Transformer fully parallelize the decoder computations at training time, inference still proceeds sequentially. Recent developments in non- and semi-autoregressive decoding produce multiple tokens per timestep independently of the others, which improves inference speed but deteriorates translation quality. In this work, we propose the syntactically supervised Transformer (SynST), which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse. A series of controlled experiments demonstrates that SynST decodes sentences ~5x faster than the baseline autoregressive Transformer while achieving higher BLEU scores than most competing methods on En-De and En-Fr datasets.",,,,ACL
123,2019,Dynamically Composing Domain-Data Selection with Clean-Data Selection by “Co-Curricular Learning” for Neural Machine Translation,"Wei Wang, Isaac Caswell, Ciprian Chelba","Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a “co-curricular learning” method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the “co-curriculum”. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.",,,,ACL
124,2019,On the Word Alignment from Neural Machine Translation,"Xintong Li, Guanlin Li, Lemao Liu, Max Meng, Shuming Shi","Prior researches suggest that neural machine translation (NMT) captures word alignment through its attention mechanism, however, this paper finds attention may almost fail to capture word alignment for some NMT models. This paper thereby proposes two methods to induce word alignment which are general and agnostic to specific NMT models. Experiments show that both methods induce much better word alignment than attention. This paper further visualizes the translation through the word alignment induced by NMT. In particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics.",,,,ACL
125,2019,Imitation Learning for Non-Autoregressive Neural Machine Translation,"Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, Xu Sun","Non-autoregressive translation models (NAT) have achieved impressive inference speedup. A potential issue of the existing NAT algorithms, however, is that the decoding is conducted in parallel, without directly considering previous context. In this paper, we propose an imitation learning framework for non-autoregressive machine translation, which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart. We conduct experiments on the IWSLT16, WMT14 and WMT16 datasets. Our proposed model achieves a significant speedup over the autoregressive models, while keeping the translation quality comparable to the autoregressive models. By sampling sentence length in parallel at inference time, we achieve the performance of 31.85 BLEU on WMT16 Ro→En and 30.68 BLEU on IWSLT16 En→De.",,,,ACL
126,2019,Monotonic Infinite Lookback Attention for Simultaneous Machine Translation,"Naveen Arivazhagan, Colin Cherry, Wolfgang Macherey, Chung-Cheng Chiu, Semih Yavuz","Simultaneous machine translation begins to translate each source sentence before the source speaker is finished speaking, with applications to live and streaming scenarios. Simultaneous systems must carefully schedule their reading of the source sentence to balance quality against latency. We present the first simultaneous translation system to learn an adaptive schedule jointly with a neural machine translation (NMT) model that attends over all source tokens read thus far. We do so by introducing Monotonic Infinite Lookback (MILk) attention, which maintains both a hard, monotonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source. We show that MILk’s adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values.",,,,ACL
127,2019,Global Textual Relation Embedding for Relational Understanding,"Zhiyu Chen, Hanwen Zha, Honglei Liu, Wenhu Chen, Xifeng Yan","Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding. Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding. The data and code can be found at https://github.com/czyssrs/GloREPlus",,,,ACL
128,2019,Graph Neural Networks with Generated Parameters for Relation Extraction,"Hao Zhu, Yankai Lin, Zhiyuan Liu, Jie Fu, Tat-Seng Chua","In this paper, we propose a novel graph neural network with generated parameters (GP-GNNs). The parameters in the propagation module, i.e. the transition matrices used in message passing procedure, are produced by a generator taking natural language sentences as inputs. We verify GP-GNNs in relation extraction from text, both on bag- and instance-settings. Experimental results on a human-annotated dataset and two distantly supervised datasets show that multi-hop reasoning mechanism yields significant improvements. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.",,,,ACL
129,2019,Entity-Relation Extraction as Multi-Turn Question Answering,"Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan","In this paper, we propose a new paradigm for the task of entity-relation extraction. We cast the task as a multi-turn question answering problem, i.e., the extraction of entities and elations is transformed to the task of identifying answer spans from the context. This multi-turn QA formalization comes with several key advantages: firstly, the question query encodes important information for the entity/relation class we want to identify; secondly, QA provides a natural way of jointly modeling entity and relation; and thirdly, it allows us to exploit the well developed machine reading comprehension (MRC) models. Experiments on the ACE and the CoNLL04 corpora demonstrate that the proposed paradigm significantly outperforms previous best models. We are able to obtain the state-of-the-art results on all of the ACE04, ACE05 and CoNLL04 datasets, increasing the SOTA results on the three datasets to 49.6 (+1.2), 60.3 (+0.7) and 69.2 (+1.4), respectively. Additionally, we construct and will release a newly developed dataset RESUME, which requires multi-step reasoning to construct entity dependencies, as opposed to the single-step dependency extraction in the triplet exaction in previous datasets. The proposed multi-turn QA model also achieves the best performance on the RESUME dataset.",,,,ACL
130,2019,Exploiting Entity BIO Tag Embeddings and Multi-task Learning for Relation Extraction with Imbalanced Data,"Wei Ye, Bo Li, Rui Xie, Zhonghao Sheng, Long Chen","In practical scenario, relation extraction needs to first identify entity pairs that have relation and then assign a correct relation class. However, the number of non-relation entity pairs in context (negative instances) usually far exceeds the others (positive instances), which negatively affects a model’s performance. To mitigate this problem, we propose a multi-task architecture which jointly trains a model to perform relation identification with cross-entropy loss and relation classification with ranking loss. Meanwhile, we observe that a sentence may have multiple entities and relation mentions, and the patterns in which the entities appear in a sentence may contain useful semantic information that can be utilized to distinguish between positive and negative instances. Thus we further incorporate the embeddings of character-wise/word-wise BIO tag from the named entity recognition task into character/word embeddings to enrich the input representation. Experiment results show that our proposed approach can significantly improve the performance of a baseline model with more than 10% absolute increase in F1-score, and outperform the state-of-the-art models on ACE 2005 Chinese and English corpus. Moreover, BIO tag embeddings are particularly effective and can be used to improve other models as well.",,,,ACL
131,2019,Joint Type Inference on Entities and Relations via Graph Convolutional Networks,"Changzhi Sun, Yeyun Gong, Yuanbin Wu, Ming Gong, Daxin Jiang","We develop a new paradigm for the task of joint entity relation extraction. It first identifies entity spans, then performs a joint inference on entity types and relation types. To tackle the joint type inference task, we propose a novel graph convolutional network (GCN) running on an entity-relation bipartite graph. By introducing a binary relation classification task, we are able to utilize the structure of entity-relation bipartite graph in a more efficient and interpretable way. Experiments on ACE05 show that our model outperforms existing joint models in entity performance and is competitive with the state-of-the-art in relation performance.",,,,ACL
132,2019,Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers,"Haoyu Wang, Ming Tan, Mo Yu, Shiyu Chang, Dakuo Wang","Many approaches to extract multiple relations from a paragraph require multiple passes over the paragraph. In practice, multiple passes are computationally expensive and this makes difficult to scale to longer paragraphs and larger text corpora. In this work, we focus on the task of multiple relation extractions by encoding the paragraph only once. We build our solution upon the pre-trained self-attentive models (Transformer), where we first add a structured prediction layer to handle extraction between multiple entity pairs, then enhance the paragraph embedding to capture multiple relational information associated with each entity with entity-aware attention. We show that our approach is not only scalable but can also perform state-of-the-art on the standard benchmark ACE 2005.",,,,ACL
133,2019,Unsupervised Information Extraction: Regularizing Discriminative Approaches with Relation Distribution Losses,"Étienne Simon, Vincent Guigue, Benjamin Piwowarski","Unsupervised relation extraction aims at extracting relations between entities in text. Previous unsupervised approaches are either generative or discriminative. In a supervised setting, discriminative approaches, such as deep neural network classifiers, have demonstrated substantial improvement. However, these models are hard to train without supervision, and the currently proposed solutions are unstable. To overcome this limitation, we introduce a skewness loss which encourages the classifier to predict a relation with confidence given a sentence, and a distribution distance loss enforcing that all relations are predicted in average. These losses improve the performance of discriminative based models, and enable us to train deep neural networks satisfactorily, surpassing current state of the art on three different datasets.",,,,ACL
134,2019,Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction,"Christoph Alt, Marc Hübner, Leonhard Hennig","Distantly supervised relation extraction is widely used to extract relational facts from text, but suffers from noisy labels. Current relation extraction methods try to alleviate the noise by multi-instance learning and by providing supporting linguistic and contextual information to more efficiently guide the relation classification. While achieving state-of-the-art results, we observed these models to be biased towards recognizing a limited set of relations with high precision, while ignoring those in the long tail. To address this gap, we utilize a pre-trained language model, the OpenAI Generative Pre-trained Transformer (GPT) (Radford et al., 2018). The GPT and similar models have been shown to capture semantic and syntactic features, and also a notable amount of “common-sense” knowledge, which we hypothesize are important features for recognizing a more diverse set of relations. By extending the GPT to the distantly supervised setting, and fine-tuning it on the NYT10 dataset, we show that it predicts a larger set of distinct relation types with high confidence. Manual and automated evaluation of our model shows that it achieves a state-of-the-art AUC score of 0.422 on the NYT10 dataset, and performs especially well at higher recall levels.",,,,ACL
135,2019,ARNOR: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification,"Wei Jia, Dai Dai, Xinyan Xiao, Hua Wu","Distant supervision is widely used in relation classification in order to create large-scale training data by aligning a knowledge base with an unlabeled corpus. However, it also introduces amounts of noisy labels where a contextual sentence actually does not express the labeled relation. In this paper, we propose ARNOR, a novel Attention Regularization based NOise Reduction framework for distant supervision relation classification. ARNOR assumes that a trustable relation label should be explained by the neural attention model. Specifically, our ARNOR framework iteratively learns an interpretable model and utilizes it to select trustable instances. We first introduce attention regularization to force the model to pay attention to the patterns which explain the relation labels, so as to make the model more interpretable. Then, if the learned model can clearly locate the relation patterns of a candidate instance in the training set, we will select it as a trustable instance for further training step. According to the experiments on NYT data, our ARNOR framework achieves significant improvements over state-of-the-art methods in both relation classification performance and noise reduction effect.",,,,ACL
136,2019,GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction,"Tsu-Jui Fu, Peng-Hsuan Li, Wei-Yun Ma","In this paper, we present GraphRel, an end-to-end relation extraction model which uses graph convolutional networks (GCNs) to jointly learn named entities and relations. In contrast to previous baselines, we consider the interaction between named entities and relations via a 2nd-phase relation-weighted GCN to better extract relations. Linear and dependency structures are both used to extract both sequential and regional features of the text, and a complete word graph is further utilized to extract implicit features among all word pairs of the text. With the graph-based approach, the prediction for overlapping relations is substantially improved over previous sequential approaches. We evaluate GraphRel on two public datasets: NYT and WebNLG. Results show that GraphRel maintains high precision while increasing recall substantially. Also, GraphRel outperforms previous work by 3.2% and 5.8% (F1 score), achieving a new state-of-the-art for relation extraction.",,,,ACL
137,2019,DIAG-NRE: A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction,"Shun Zheng, Xu Han, Yankai Lin, Peilin Yu, Lu Chen","Pattern-based labeling methods have achieved promising results in alleviating the inevitable labeling noises of distantly supervised neural relation extraction. However, these methods require significant expert labor to write relation-specific patterns, which makes them too sophisticated to generalize quickly. To ease the labor-intensive workload of pattern writing and enable the quick generalization to new relation types, we propose a neural pattern diagnosis framework, DIAG-NRE, that can automatically summarize and refine high-quality relational patterns from noise data with human experts in the loop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two real-world datasets and present both significant and interpretable improvements over state-of-the-art methods.",,,,ACL
138,2019,Multi-grained Named Entity Recognition,"Congying Xia, Chenwei Zhang, Tao Yang, Yaliang Li, Nan Du","This paper presents a novel framework, MGNER, for Multi-Grained Named Entity Recognition where multiple entities or entity mentions in a sentence could be non-overlapping or totally nested. Different from traditional approaches regarding NER as a sequential labeling task and annotate entities consecutively, MGNER detects and recognizes entities on multiple granularities: it is able to recognize named entities without explicitly assuming non-overlapping or totally nested structures. MGNER consists of a Detector that examines all possible word segments and a Classifier that categorizes entities. In addition, contextual information and a self-attention mechanism are utilized throughout the framework to improve the NER performance. Experimental results show that MGNER outperforms current state-of-the-art baselines up to 4.4% in terms of the F1 score among nested/non-overlapping NER tasks.",,,,ACL
139,2019,ERNIE: Enhanced Language Representation with Informative Entities,"Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun","Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.",,,,ACL
140,2019,Multi-Channel Graph Neural Network for Entity Alignment,"Yixin Cao, Zhiyuan Liu, Chengjiang Li, Zhiyuan Liu, Juanzi Li","Entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments. In this paper, we propose a novel Multi-channel Graph Neural Network model (MuGNN) to learn alignment-oriented knowledge graph (KG) embeddings by robustly encoding two KGs via multiple channels. Each channel encodes KGs via different relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive entities respectively, which are further combined via pooling techniques. Moreover, we also infer and transfer rule knowledge for completing two KGs consistently. MuGNN is expected to reconcile the structural differences of two KGs, and thus make better use of seed alignments. Extensive experiments on five publicly available datasets demonstrate our superior performance (5% Hits@1 up on average). Source code and data used in the experiments can be accessed at https://github.com/thunlp/MuGNN .",,,,ACL
141,2019,A Neural Multi-digraph Model for Chinese NER with Gazetteers,"Ruixue Ding, Pengjun Xie, Xiaoyan Zhang, Wei Lu, Linlin Li","Gazetteers were shown to be useful resources for named entity recognition (NER). Many existing approaches to incorporating gazetteers into machine learning based NER systems rely on manually defined selection strategies or handcrafted templates, which may not always lead to optimal effectiveness, especially when multiple gazetteers are involved. This is especially the case for the task of Chinese NER, where the words are not naturally tokenized, leading to additional ambiguities. To automatically learn how to incorporate multiple gazetteers into an NER system, we propose a novel approach based on graph neural networks with a multi-digraph structure that captures the information that the gazetteers offer. Experiments on various datasets show that our model is effective in incorporating rich gazetteer information while resolving ambiguities, outperforming previous approaches.",,,,ACL
142,2019,Improved Language Modeling by Decoding the Past,Siddhartha Brahma,"Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method improves perplexity on the Penn Treebank dataset by up to 1.8 points and by up to 2.3 points on the WikiText-2 dataset, over strong regularized baselines using a single softmax. With a mixture-of-softmax model, we show gains of up to 1.0 perplexity points on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.",,,,ACL
143,2019,Training Hybrid Language Models by Marginalizing over Segmentations,"Edouard Grave, Sainbayar Sukhbaatar, Piotr Bojanowski, Armand Joulin","In this paper, we study the problem of hybrid language modeling, that is using models which can predict both characters and larger units such as character ngrams or words. Using such models, multiple potential segmentations usually exist for a given string, for example one using words and one using characters only. Thus, the probability of a string is the sum of the probabilities of all the possible segmentations. Here, we show how it is possible to marginalize over the segmentations efficiently, in order to compute the true probability of a sequence. We apply our technique on three datasets, comprising seven languages, showing improvements over a strong character level language model.",,,,ACL
144,2019,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future","Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass","Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.",,,,ACL
145,2019,Lightweight and Efficient Neural Natural Language Processing with Quaternion Networks,"Yi Tay, Aston Zhang, Anh Tuan Luu, Jinfeng Rao, Shuai Zhang","Many state-of-the-art neural models for NLP are heavily parameterized and thus memory inefficient. This paper proposes a series of lightweight and memory efficient neural architectures for a potpourri of natural language processing (NLP) tasks. To this end, our models exploit computation using Quaternion algebra and hypercomplex spaces, enabling not only expressive inter-component interactions but also significantly (75%) reduced parameter size due to lesser degrees of freedom in the Hamilton product. We propose Quaternion variants of models, giving rise to new architectures such as the Quaternion attention Model and Quaternion Transformer. Extensive experiments on a battery of NLP tasks demonstrates the utility of proposed Quaternion-inspired models, enabling up to 75% reduction in parameter size without significant loss in performance.",,,,ACL
146,2019,Sparse Sequence-to-Sequence Models,"Ben Peters, Vlad Niculae, André F. T. Martins","Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of 𝛼-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any 𝛼 > 1. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models.",,,,ACL
147,2019,On the Robustness of Self-Attentive Models,"Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei Wei, Wen-Lian Hsu","This work examines the robustness of self-attentive neural networks against adversarial input perturbations. Specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks. We also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans. Experimental results show that, compared to recurrent neural models, self-attentive models are more robust against adversarial perturbation. In addition, we provide theoretical explanations for their superior robustness to support our claims.",,,,ACL
148,2019,Exact Hard Monotonic Attention for Character-Level Transduction,"Shijie Wu, Ryan Cotterell","Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.",,,,ACL
149,2019,A Lightweight Recurrent Network for Sequence Modeling,"Biao Zhang, Rico Sennrich","Recurrent networks have achieved great success on various sequential tasks with the assistance of complex recurrent units, but suffer from severe computational inefficiency due to weak parallelization. One direction to alleviate this issue is to shift heavy computations outside the recurrence. In this paper, we propose a lightweight recurrent network, or LRN. LRN uses input and forget gates to handle long-range dependencies as well as gradient vanishing and explosion, with all parameter related calculations factored outside the recurrence. The recurrence in LRN only manipulates the weight assigned to each token, tightly connecting LRN with self-attention networks. We apply LRN as a drop-in replacement of existing recurrent units in several neural sequential models. Extensive experiments on six NLP tasks show that LRN yields the best running efficiency with little or no loss in model performance.",,,,ACL
150,2019,Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications,"Wei Zhao, Haiyun Peng, Steffen Eger, Erik Cambria, Min Yang","Obstacles hindering the development of capsule networks for challenging NLP applications include poor scalability to large output spaces and less reliable routing processes. In this paper, we introduce: (i) an agreement score to evaluate the performance of routing processes at instance-level; (ii) an adaptive optimizer to enhance the reliability of routing; (iii) capsule compression and partial routing to improve the scalability of capsule networks. We validate our approach on two NLP tasks, namely: multi-label text classification and question answering. Experimental results show that our approach considerably improves over strong competitors on both tasks. In addition, we gain the best results in low-resource settings with few training instances.",,,,ACL
151,2019,Soft Representation Learning for Sparse Transfer,"Haeju Park, Jinyoung Yeo, Gengyu Wang, Seung-won Hwang","Transfer learning is effective for improving the performance of tasks that are related, and Multi-task learning (MTL) and Cross-lingual learning (CLL) are important instances. This paper argues that hard-parameter sharing, of hard-coding layers shared across different tasks or languages, cannot generalize well, when sharing with a loosely related task. Such case, which we call sparse transfer, might actually hurt performance, a phenomenon known as negative transfer. Our contribution is using adversarial training across tasks, to “soft-code” shared and private spaces, to avoid the shared space gets too sparse. In CLL, our proposed architecture considers another challenge of dealing with low-quality input.",,,,ACL
152,2019,Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization,"Paul Pu Liang, Zhun Liu, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan Salakhutdinov","There has been an increased interest in multimodal language processing including multimodal dialog, question answering, sentiment analysis, and speech recognition. However, naturally occurring multimodal data is often imperfect as a result of imperfect modalities, missing entries or noise corruption. To address these concerns, we present a regularization method based on tensor rank minimization. Our method is based on the observation that high-dimensional multimodal time series data often exhibit correlations across time and modalities which leads to low-rank tensor representations. However, the presence of noise or incomplete values breaks these correlations and results in tensor representations of higher rank. We design a model to learn such tensor representations and effectively regularize their rank. Experiments on multimodal language data show that our model achieves good results across various levels of imperfection.",,,,ACL
153,2019,Towards Lossless Encoding of Sentences,"Gabriele Prato, Mathieu Duchesneau, Sarath Chandar, Alain Tapp","A lot of work has been done in the field of image compression via machine learning, but not much attention has been given to the compression of natural language. Compressing text into lossless representations while making features easily retrievable is not a trivial task, yet has huge benefits. Most methods designed to produce feature rich sentence embeddings focus solely on performing well on downstream tasks and are unable to properly reconstruct the original sequence from the learned embedding. In this work, we propose a near lossless method for encoding long sequences of texts as well as all of their sub-sequences into feature rich representations. We test our method on sentiment analysis and show good performance across all sub-sentence and sentence embeddings.",,,,ACL
154,2019,Open Vocabulary Learning for Neural Chinese Pinyin IME,"Zhuosheng Zhang, Yafang Huang, Hai Zhao","Pinyin-to-character (P2C) conversion is the core component of pinyin-based Chinese input method engine (IME). However, the conversion is seriously compromised by the ambiguities of Chinese characters corresponding to pinyin as well as the predefined fixed vocabularies. To alleviate such inconveniences, we propose a neural P2C conversion model augmented by an online updated vocabulary with a sampling mechanism to support open vocabulary learning during IME working. Our experiments show that the proposed method outperforms commercial IMEs and state-of-the-art traditional models on standard corpus and true inputting history dataset in terms of multiple metrics and thus the online updated vocabulary indeed helps our IME effectively follows user inputting behavior.",,,,ACL
155,2019,Using LSTMs to Assess the Obligatoriness of Phonological Distinctive Features for Phonotactic Learning,"Nicole Mirea, Klinton Bicknell","To ascertain the importance of phonetic information in the form of phonological distinctive features for the purpose of segment-level phonotactic acquisition, we compare the performance of two recurrent neural network models of phonotactic learning: one that has access to distinctive features at the start of the learning process, and one that does not. Though the predictions of both models are significantly correlated with human judgments of non-words, the feature-naive model significantly outperforms the feature-aware one in terms of probability assigned to a held-out test set of English words, suggesting that distinctive features are not obligatory for learning phonotactic patterns at the segment level.",,,,ACL
156,2019,Better Character Language Modeling through Morphology,"Terra Blevins, Luke Zettlemoyer","We incorporate morphological supervision into character language models (CLMs) via multitasking and show that this addition improves bits-per-character (BPC) performance across 24 languages, even when the morphology data and language modeling data are disjoint. Analyzing the CLMs shows that inflected words benefit more from explicitly modeling morphology than uninflected words, and that morphological supervision improves performance even as the amount of language modeling data grows. We then transfer morphological supervision across languages to improve performance in the low-resource setting.",,,,ACL
157,2019,Historical Text Normalization with Delayed Rewards,"Simon Flachs, Marcel Bollmann, Anders Søgaard","Training neural sequence-to-sequence models with simple token-level log-likelihood is now a standard approach to historical text normalization, albeit often outperformed by phrase-based models. Policy gradient training enables direct optimization for exact matches, and while the small datasets in historical text normalization are prohibitive of from-scratch reinforcement learning, we show that policy gradient fine-tuning leads to significant improvements across the board. Policy gradient training, in particular, leads to more accurate normalizations for long or unseen words.",,,,ACL
158,2019,Stochastic Tokenization with a Language Model for Neural Text Classification,"Tatsuya Hiraoka, Hiroyuki Shindo, Yuji Matsumoto","For unsegmented languages such as Japanese and Chinese, tokenization of a sentence has a significant impact on the performance of text classification. Sentences are usually segmented with words or subwords by a morphological analyzer or byte pair encoding and then encoded with word (or subword) representations for neural networks. However, segmentation is potentially ambiguous, and it is unclear whether the segmented tokens achieve the best performance for the target task. In this paper, we propose a method to simultaneously learn tokenization and text classification to address these problems. Our model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods.",,,,ACL
159,2019,Mitigating Gender Bias in Natural Language Processing: Literature Review,"Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief","As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.",,,,ACL
160,2019,Gender-preserving Debiasing for Pre-trained Word Embeddings,"Masahiro Kaneko, Danushka Bollegala","Word embeddings learnt from massive text collections have demonstrated significant levels of discriminative biases such as gender, racial or ethnic biases, which in turn bias the down-stream NLP applications that use those word embeddings. Taking gender-bias as a working example, we propose a debiasing method that preserves non-discriminative gender-related information, while removing stereotypical discriminative gender biases from pre-trained word embeddings. Specifically, we consider four types of information: feminine, masculine, gender-neutral and stereotypical, which represent the relationship between gender vs. bias, and propose a debiasing method that (a) preserves the gender-related information in feminine and masculine words, (b) preserves the neutrality in gender-neutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information.",,,,ACL
161,2019,Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology,"Ran Zmigrod, Sabrina J. Mielke, Hanna Wallach, Ryan Cotterell","Gender stereotypes are manifest in most of the world’s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.",,,,ACL
162,2019,A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings,"Chris Sweeney, Maryam Najafian","Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.",,,,ACL
163,2019,The Risk of Racial Bias in Hate Speech Detection,"Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, Noah A. Smith","We investigate how annotators’ insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet’s dialect they are significantly less likely to label the tweet as offensive.",,,,ACL
164,2019,Evaluating Gender Bias in Machine Translation,"Gabriel Stanovsky, Noah A. Smith, Luke Zettlemoyer","We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., “The doctor asked the nurse to help her in the operation”). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word “doctor”). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are publicly available at https://github.com/gabrielStanovsky/mt_gender.",,,,ACL
165,2019,LSTMEmbed: Learning Word and Sense Representations from a Large Semantically Annotated Corpus with Long Short-Term Memories,"Ignacio Iacobacci, Roberto Navigli","While word embeddings are now a de facto standard representation of words in most NLP tasks, recently the attention has been shifting towards vector representations which capture the different meanings, i.e., senses, of words. In this paper we explore the capabilities of a bidirectional LSTM model to learn representations of word senses from semantically annotated corpora. We show that the utilization of an architecture that is aware of word order, like an LSTM, enables us to create better representations. We assess our proposed model on various standard benchmarks for evaluating semantic representations, reaching state-of-the-art performance on the SemEval-2014 word-to-sense similarity task. We release the code and the resulting word and sense embeddings at http://lcl.uniroma1.it/LSTMEmbed.",,,,ACL
166,2019,Understanding Undesirable Word Embedding Associations,"Kawin Ethayarajh, David Duvenaud, Graeme Hirst","Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.",,,,ACL
167,2019,Unsupervised Discovery of Gendered Language through Latent-Variable Modeling,"Alexander Miserlis Hoyle, Lawrence Wolf-Sonkin, Hanna Wallach, Isabelle Augenstein, Ryan Cotterell","Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.",,,,ACL
168,2019,Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings,"Vihari Piratla, Sunita Sarawagi, Soumen Chakrabarti","Given a small corpus D_T pertaining to a limited set of focused topics, our goal is to train embeddings that accurately capture the sense of words in the topic in spite of the limited size of D_T. These embeddings may be used in various tasks involving D_T. A popular strategy in limited data settings is to adapt pretrained embeddings E trained on a large corpus. To correct for sense drift, fine-tuning, regularization, projection, and pivoting have been proposed recently. Among these, regularization informed by a word’s corpus frequency performed well, but we improve upon it using a new regularizer based on the stability of its cooccurrence with other words. However, a thorough comparison across ten topics, spanning three tasks, with standardized settings of hyper-parameters, reveals that even the best embedding adaptation strategies provide small gains beyond well-tuned baselines, which many earlier comparisons ignored. In a bold departure from adapting pretrained embeddings, we propose using D_T to probe, attend to, and borrow fragments from any large, topic-rich source corpus (such as Wikipedia), which need not be the corpus used to pretrain embeddings. This step is made scalable and practical by suitable indexing. We reach the surprising conclusion that even limited corpus augmentation is more useful than adapting embeddings, which suggests that non-dominant sense information may be irrevocably obliterated from pretrained embeddings and cannot be salvaged by adaptation.",,,,ACL
169,2019,SphereRE: Distinguishing Lexical Relations with Hyperspherical Relation Embeddings,"Chengyu Wang, Xiaofeng He, Aoying Zhou","Lexical relations describe how meanings of terms relate to each other. Typical examples include hypernymy, synonymy, meronymy, etc. Automatic distinction of lexical relations is vital for NLP applications, and also challenging due to the lack of contextual signals to discriminate between such relations. In this work, we present a neural representation learning model to distinguish lexical relations among term pairs based on Hyperspherical Relation Embeddings (SphereRE). Rather than learning embeddings for individual terms, the model learns representations of relation triples by mapping them to the hyperspherical embedding space, where relation triples of different lexical relations are well separated. Experiments over several benchmarks confirm SphereRE outperforms state-of-the-arts.",,,,ACL
170,2019,Multilingual Factor Analysis,"Francisco Vargas, Kamen Brestnichki, Alex Papadopoulos Korfiatis, Nils Hammerla",In this work we approach the task of learning multilingual word representations in an offline manner by fitting a generative latent variable model to a multilingual dictionary. We model equivalent words in different languages as different views of the same word generated by a common latent variable representing their latent lexical meaning. We explore the task of alignment by querying the fitted model for multilingual embeddings achieving competitive results across a variety of tasks. The proposed model is robust to noise in the embedding space making it a suitable method for distributed representations learned from noisy corpora.,,,,ACL
171,2019,Meaning to Form: Measuring Systematicity as Information,"Tiago Pimentel, Arya D. McCarthy, Damian Blasi, Brian Roark, Ryan Cotterell","A longstanding debate in semiotics centers on the relationship between linguistic signs and their corresponding semantics: is there an arbitrary relationship between a word form and its meaning, or does some systematic phenomenon pervade? For instance, does the character bigram ‘gl’ have any systematic relationship to the meaning of words like ‘glisten’, ‘gleam’ and ‘glow’? In this work, we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks. We employ these in a data-driven and massively multilingual approach to the question, examining 106 languages. We find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation. Encouragingly, we also recover well-attested English examples of systematic affixes. We conclude with the meta-point: Our approximate effect size (measured in bits) is quite small—despite some amount of systematicity between form and meaning, an arbitrary relationship and its resulting benefits dominate human language.",,,,ACL
172,2019,Learning Morphosyntactic Analyzers from the Bible via Iterative Annotation Projection across 26 Languages,"Garrett Nicolai, David Yarowsky","A large percentage of computational tools are concentrated in a very small subset of the planet’s languages. Compounding the issue, many languages lack the high-quality linguistic annotation necessary for the construction of such tools with current machine learning methods. In this paper, we address both issues simultaneously: leveraging the high accuracy of English taggers and parsers, we project morphological information onto translations of the Bible in 26 varied test languages. Using an iterative discovery, constraint, and training process, we build inflectional lexica in the target languages. Through a combination of iteration, ensembling, and reranking, we see double-digit relative error reductions in lemmatization and morphological analysis over a strong initial system.",,,,ACL
173,2019,Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling,"Nasser Zalmout, Nizar Habash","Morphological tagging is challenging for morphologically rich languages due to the large target space and the need for more training data to minimize model sparsity. Dialectal variants of morphologically rich languages suffer more as they tend to be more noisy and have less resources. In this paper we explore the use of multitask learning and adversarial training to address morphological richness and dialectal variations in the context of full morphological tagging. We use multitask learning for joint morphological modeling for the features within two dialects, and as a knowledge-transfer scheme for cross-dialectal modeling. We use adversarial training to learn dialect invariant features that can help the knowledge-transfer scheme from the high to low-resource variants. We work with two dialectal variants: Modern Standard Arabic (high-resource “dialect’”) and Egyptian Arabic (low-resource dialect) as a case study. Our models achieve state-of-the-art results for both. Furthermore, adversarial training provides more significant improvement when using smaller training datasets in particular.",,,,ACL
174,2019,Neural Machine Translation with Reordering Embeddings,"Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita","The reordering model plays an important role in phrase-based statistical machine translation. However, there are few works that exploit the reordering information in neural machine translation. In this paper, we propose a reordering mechanism to learn the reordering embedding of a word based on its contextual information. These learned reordering embeddings are stacked together with self-attention networks to learn sentence representation for machine translation. The reordering mechanism can be easily integrated into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT’14 English-to-German, NIST Chinese-to-English, and WAT Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer.",,,,ACL
175,2019,Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation,"Bram Bulte, Arda Tezcan","We present a simple yet powerful data augmentation method for boosting Neural Machine Translation (NMT) performance by leveraging information retrieved from a Translation Memory (TM). We propose and test two methods for augmenting NMT training data with fuzzy TM matches. Tests on the DGT-TM data set for two language pairs show consistent and substantial improvements over a range of baseline systems. The results suggest that this method is promising for any translation environment in which a sizeable TM is available and a certain amount of repetition across translations is to be expected, especially considering its ease of implementation.",,,,ACL
176,2019,Learning Deep Transformer Models for Machine Translation,"Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li","Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT’16 English-German and NIST OpenMT’12 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big.",,,,ACL
177,2019,Generating Diverse Translations with Sentence Codes,"Raphael Shu, Hideki Nakayama, Kyunghyun Cho","Users of machine translation systems may desire to obtain multiple candidates translated in different ways. In this work, we attempt to obtain diverse translations by using sentence codes to condition the sentence generation. We describe two methods to extract the codes, either with or without the help of syntax information. For diverse generation, we sample multiple candidates, each of which conditioned on a unique code. Experiments show that the sampled translations have much higher diversity scores when using reasonable sentence codes, where the translation quality is still on par with the baselines even under strong constraint imposed by the codes. In qualitative analysis, we show that our method is able to generate paraphrase translations with drastically different structures. The proposed approach can be easily adopted to existing translation systems as no modification to the model is required.",,,,ACL
178,2019,Self-Supervised Neural Machine Translation,"Dana Ruiter, Cristina España-Bonet, Josef van Genabith","We present a simple new method where an emergent NMT system is used for simultaneously selecting training data and learning internal NMT representations. This is done in a self-supervised way without parallel data, in such a way that both tasks enhance each other during training. The method is language independent, introduces no additional hyper-parameters, and achieves BLEU scores of 29.21 (en2fr) and 27.36 (fr2en) on newstest2014 using English and French Wikipedia data for training.",,,,ACL
179,2019,Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation,"Elizabeth Salesky, Matthias Sperber, Alan W Black","Previous work on end-to-end translation from speech has primarily used frame-level features as speech representations, which creates longer, sparser sequences than text. We show that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features. Specifically, we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter, higher-level source sequences for translation. We see improvements of up to 5 BLEU on both our high and low resource language pairs, with a reduction in training time of 60%. Our improvements hold across multiple data sizes and two language pairs.",,,,ACL
180,2019,Visually Grounded Neural Syntax Acquisition,"Haoyue Shi, Jiayuan Mao, Kevin Gimpel, Karen Livescu","We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without any explicit supervision. The model learns by looking at natural images and reading paired captions. VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. We define concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F1 scores against gold parse trees. We find that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data. We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists. Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches.",,,,ACL
181,2019,Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation,"Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie","Advances in learning and representations have reinvigorated work that connects language to other modalities. A particularly exciting direction is Vision-and-Language Navigation(VLN), in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. Despite recent progress, current research leaves unclear how much of a role language under-standing plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. Here, we highlight shortcomings of current metrics for the Room-to-Room dataset (Anderson et al.,2018b) and propose a new metric, Coverage weighted by Length Score (CLS). We also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. We join existing short paths to form more challenging extended paths to create a new data set, Room-for-Room (R4R). Using R4R and CLS, we show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion.",,,,ACL
182,2019,Expressing Visual Relationships via Language,"Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, Mohit Bansal","Describing images with text is a fundamental problem in vision-language research. Current studies in this domain mostly focus on single image captioning. However, in various real applications (e.g., image editing, difference interpretation, and retrieval), generating relational captions for two images, can also be very useful. This important problem has not been explored mostly due to lack of datasets and effective models. To push forward the research in this direction, we first introduce a new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. We then propose a new relational speaker model based on an encoder-decoder architecture with static relational attention and sequential multi-head attention. We also extend the model with dynamic relational attention, which calculates visual alignment while decoding. Our models are evaluated on our newly collected and two public datasets consisting of image pairs annotated with relationship sentences. Experimental results, based on both automatic and human evaluation, demonstrate that our model outperforms all baselines and existing methods on all the datasets.",,,,ACL
183,2019,Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video,"Zhenfang Chen, Lin Ma, Wenhan Luo, Kwan-Yee Kenneth Wong","In this paper, we address a novel task, namely weakly-supervised spatio-temporally grounding natural sentence in video. Specifically, given a natural sentence and a video, we localize a spatio-temporal tube in the video that semantically corresponds to the given sentence, with no reliance on any spatio-temporal annotations during training. First, a set of spatio-temporal tubes, referred to as instances, are extracted from the video. We then encode these instances and the sentence using our newly proposed attentive interactor which can exploit their fine-grained relationships to characterize their matching behaviors. Besides a ranking loss, a novel diversity loss is introduced to train our attentive interactor to strengthen the matching behaviors of reliable instance-sentence pairs and penalize the unreliable ones. We also contribute a dataset, called VID-sentence, based on the ImageNet video object detection dataset, to serve as a benchmark for our task. Results from extensive experiments demonstrate the superiority of our model over the baseline approaches.",,,,ACL
184,2019,The PhotoBook Dataset: Building Common Ground through Visually-Grounded Dialogue,"Janosch Haber, Tim Baumgärtner, Ece Takmaz, Lieke Gelderloos, Elia Bruni","This paper introduces the PhotoBook dataset, a large-scale collection of visually-grounded, task-oriented dialogues in English designed to investigate shared dialogue history accumulating during conversation. Taking inspiration from seminal work on dialogue analysis, we propose a data-collection task formulated as a collaborative game prompting two online participants to refer to images utilising both their visual context as well as previously established referring expressions. We provide a detailed description of the task setup and a thorough analysis of the 2,500 dialogues collected. To further illustrate the novel features of the dataset, we propose a baseline model for reference resolution which uses a simple method to take into account shared information accumulated in a reference chain. Our results show that this information is particularly important to resolve later descriptions and underline the need to develop more sophisticated models of common ground in dialogue interaction.",,,,ACL
185,2019,Continual and Multi-Task Architecture Search,"Ramakanth Pasunuru, Mohit Bansal","Architecture search is the process of automatically learning the neural model or cell structure that best suits the given task. Recently, this approach has shown promising performance improvements (on language modeling and image classification) with reasonable training speed, using a weight sharing strategy called Efficient Neural Architecture Search (ENAS). In our work, we first introduce a novel continual architecture search (CAS) approach, so as to continually evolve the model parameters during the sequential training of several tasks, without losing performance on previously learned tasks (via block-sparsity and orthogonality constraints), thus enabling life-long learning. Next, we explore a multi-task architecture search (MAS) approach over ENAS for finding a unified, single cell structure that performs well across multiple tasks (via joint controller rewards), and hence allows more generalizable transfer of the cell structure knowledge to an unseen new task. We empirically show the effectiveness of our sequential continual learning and parallel multi-task learning based architecture search approaches on diverse sentence-pair classification tasks (GLUE) and multimodal-generation based video captioning tasks. Further, we present several ablations and analyses on the learned cell structures.",,,,ACL
186,2019,Semi-supervised Stochastic Multi-Domain Learning using Variational Inference,"Yitong Li, Timothy Baldwin, Trevor Cohn","Supervised models of NLP rely on large collections of text which closely resemble the intended testing setting. Unfortunately matching text is often not available in sufficient quantity, and moreover, within any domain of text, data is often highly heterogenous. In this paper we propose a method to distill the important domain signal as part of a multi-domain learning system, using a latent variable model in which parts of a neural model are stochastically gated based on the inferred domain. We compare the use of discrete versus continuous latent variables, operating in a domain-supervised or a domain semi-supervised setting, where the domain is known only for a subset of training inputs. We show that our model leads to substantial performance improvements over competitive benchmark domain adaptation methods, including methods using adversarial learning.",,,,ACL
187,2019,Boosting Entity Linking Performance by Leveraging Unlabeled Documents,"Phong Le, Ivan Titov","Modern entity linking systems rely on large collections of documents specifically annotated for the task (e.g., AIDA CoNLL). In contrast, we propose an approach which exploits only naturally occurring information: unlabeled documents and Wikipedia. Our approach consists of two stages. First, we construct a high recall list of candidate entities for each mention in an unlabeled document. Second, we use the candidate lists as weak supervision to constrain our document-level entity linking model. The model treats entities as latent variables and, when estimated on a collection of unlabelled texts, learns to choose entities relying both on local context of each mention and on coherence with other entities in the document. The resulting approach rivals fully-supervised state-of-the-art systems on standard test sets. It also approaches their performance in the very challenging setting: when tested on a test set sampled from the data used to estimate the supervised systems. By comparing to Wikipedia-only training of our model, we demonstrate that modeling unlabeled documents is beneficial.",,,,ACL
188,2019,Pre-Learning Environment Representations for Data-Efficient Neural Instruction Following,"David Gaddy, Dan Klein",We consider the problem of learning to map from natural language instructions to state transitions (actions) in a data-efficient manner. Our method takes inspiration from the idea that it should be easier to ground language to concepts that have already been formed through pre-linguistic observation. We augment a baseline instruction-following learner with an initial environment-learning phase that uses observations of language-free state transitions to induce a suitable latent representation of actions before processing the instruction-following training data. We show that mapping to pre-learned representations substantially improves performance over systems whose representations are learned from limited instructional data alone.,,,,ACL
189,2019,Reinforced Training Data Selection for Domain Adaptation,"Miaofeng Liu, Yan Song, Hongbin Zou, Tong Zhang","Supervised models suffer from the problem of domain shifting where distribution mismatch in the data across domains greatly affect model performance. To solve the problem, training data selection (TDS) has been proven to be a prospective solution for domain adaptation in leveraging appropriate data. However, conventional TDS methods normally requires a predefined threshold which is neither easy to set nor can be applied across tasks, and models are trained separately with the TDS process. To make TDS self-adapted to data and task, and to combine it with model training, in this paper, we propose a reinforcement learning (RL) framework that synchronously searches for training instances relevant to the target domain and learns better representations for them. A selection distribution generator (SDG) is designed to perform the selection and is updated according to the rewards computed from the selected data, where a predictor is included in the framework to ensure a task-specific model can be trained on the selected data and provides feedback to rewards. Experimental results from part-of-speech tagging, dependency parsing, and sentiment analysis, as well as ablation studies, illustrate that the proposed framework is not only effective in data selection and representation, but also generalized to accommodate different NLP tasks.",,,,ACL
190,2019,Generating Long and Informative Reviews with Aspect-Aware Coarse-to-Fine Decoding,"Junyi Li, Wayne Xin Zhao, Ji-Rong Wen, Yang Song","Generating long and informative review text is a challenging natural language generation task. Previous work focuses on word-level generation, neglecting the importance of topical and syntactic characteristics from natural languages. In this paper, we propose a novel review generation model by characterizing an elaborately designed aspect-aware coarse-to-fine generation process. First, we model the aspect transitions to capture the overall content flow. Then, to generate a sentence, an aspect-aware sketch will be predicted using an aspect-aware decoder. Finally, another decoder fills in the semantic slots by generating corresponding words. Our approach is able to jointly utilize aspect semantics, syntactic sketch, and context information. Extensive experiments results have demonstrated the effectiveness of the proposed model.",,,,ACL
191,2019,PaperRobot: Incremental Draft Generation of Scientific Ideas,"Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji","We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30%, 24% and 12% of the time, respectively.",,,,ACL
192,2019,Rhetorically Controlled Encoder-Decoder for Modern Chinese Poetry Generation,"Zhiqiang Liu, Zuohui Fu, Jie Cao, Gerard de Melo, Yik-Cheung Tam","Rhetoric is a vital element in modern poetry, and plays an essential role in improving its aesthetics. However, to date, it has not been considered in research on automatic poetry generation. In this paper, we propose a rhetorically controlled encoder-decoder for modern Chinese poetry generation. Our model relies on a continuous latent variable as a rhetoric controller to capture various rhetorical patterns in an encoder, and then incorporates rhetoric-based mixtures while generating modern Chinese poetry. For metaphor and personification, an automated evaluation shows that our model outperforms state-of-the-art baselines by a substantial margin, while human evaluation shows that our model generates better poems than baseline methods in terms of fluency, coherence, meaningfulness, and rhetorical aesthetics.",,,,ACL
193,2019,Enhancing Topic-to-Essay Generation with External Commonsense Knowledge,"Pengcheng Yang, Lei Li, Fuli Luo, Tianyu Liu, Xu Sun","Automatic topic-to-essay generation is a challenging task since it requires generating novel, diverse, and topic-consistent paragraph-level text with a set of topics as input. Previous work tends to perform essay generation based solely on the given topics while ignoring massive commonsense knowledge. However, this commonsense knowledge provides additional background information, which can help to generate essays that are more novel and diverse. Towards filling this gap, we propose to integrate commonsense from the external knowledge base into the generator through dynamic memory mechanism. Besides, the adversarial training based on a multi-label discriminator is employed to further improve topic-consistency. We also develop a series of automatic evaluation metrics to comprehensively assess the quality of the generated essay. Experiments show that with external commonsense knowledge and adversarial training, the generated essays are more novel, diverse, and topic-consistent than existing methods in terms of both automatic and human evaluation.",,,,ACL
194,2019,Towards Fine-grained Text Sentiment Transfer,"Fuli Luo, Peng Li, Pengcheng Yang, Jie Zhou, Yutong Tan","In this paper, we focus on the task of fine-grained text sentiment transfer (FGST). This task aims to revise an input sequence to satisfy a given sentiment intensity, while preserving the original semantic content. Different from the conventional sentiment transfer task that only reverses the sentiment polarity (positive/negative) of text, the FTST task requires more nuanced and fine-grained control of sentiment. To remedy this, we propose a novel Seq2SentiSeq model. Specifically, the numeric sentiment intensity value is incorporated into the decoder via a Gaussian kernel layer to finely control the sentiment intensity of the output. Moreover, to tackle the problem of lacking parallel data, we propose a cycle reinforcement learning algorithm to guide the model training. In this framework, the elaborately designed rewards can balance both sentiment transformation and content preservation, while not requiring any ground truth output. Experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation.",,,,ACL
195,2019,Data-to-text Generation with Entity Modeling,"Ratish Puduppully, Li Dong, Mirella Lapata","Recent approaches to data-to-text generation have shown great promise thanks to the use of large-scale datasets and the application of neural network architectures which are trained end-to-end. These models rely on representation learning to select content appropriately, structure it coherently, and verbalize it grammatically, treating entities as nothing more than vocabulary tokens. In this work we propose an entity-centric neural architecture for data-to-text generation. Our model creates entity-specific representations which are dynamically updated. Text is generated conditioned on the data input and entity memory representations using hierarchical attention at each time step. We present experiments on the RotoWire benchmark and a (five times larger) new dataset on the baseball domain which we create. Our results show that the proposed model outperforms competitive baselines in automatic and human evaluation.",,,,ACL
196,2019,Ensuring Readability and Data-fidelity using Head-modifier Templates in Deep Type Description Generation,"Jiangjie Chen, Ao Wang, Haiyun Jiang, Suo Feng, Chenguang Li","A type description is a succinct noun compound which helps human and machines to quickly grasp the informative and distinctive information of an entity. Entities in most knowledge graphs (KGs) still lack such descriptions, thus calling for automatic methods to supplement such information. However, existing generative methods either overlook the grammatical structure or make factual mistakes in generated texts. To solve these problems, we propose a head-modifier template based method to ensure the readability and data fidelity of generated type descriptions. We also propose a new dataset and two metrics for this task. Experiments show that our method improves substantially compared with baselines and achieves state-of-the-art performance on both datasets.",,,,ACL
197,2019,Key Fact as Pivot: A Two-Stage Model for Low Resource Table-to-Text Generation,"Shuming Ma, Pengcheng Yang, Tianyu Liu, Peng Li, Jie Zhou","Table-to-text generation aims to translate the structured data into the unstructured text. Most existing methods adopt the encoder-decoder framework to learn the transformation, which requires large-scale training samples. However, the lack of large parallel data is a major practical problem for many domains. In this work, we consider the scenario of low resource table-to-text generation, where only limited parallel data is available. We propose a novel model to separate the generation into two stages: key fact prediction and surface realization. It first predicts the key facts from the tables, and then generates the text with the key facts. The training of key fact prediction needs much fewer annotated data, while surface realization can be trained with pseudo parallel corpus. We evaluate our model on a biography generation dataset. Our model can achieve 27.34 BLEU score with only 1,000 parallel data, while the baseline model only obtain the performance of 9.71 BLEU score.",,,,ACL
198,2019,Unsupervised Neural Text Simplification,"Sai Surya, Abhijit Mishra, Anirban Laha, Parag Jain, Karthik Sankaranarayanan","The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabeled text corpora. The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based losses and denoising. The framework is trained using unlabeled text collected from en-Wikipedia dump. Our analysis (both quantitative and qualitative involving human evaluators) on public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels, competitive to existing supervised methods. It also outperforms viable unsupervised baselines. Adding a few labeled pairs helps improve the performance further.",,,,ACL
199,2019,Syntax-Infused Variational Autoencoder for Text Generation,"Xinyuan Zhang, Yi Yang, Siyang Yuan, Dinghan Shen, Lawrence Carin","We present a syntax-infused variational autoencoder (SIVAE), that integrates sentences with their syntactic trees to improve the grammar of generated sentences. Distinct from existing VAE-based text generative models, SIVAE contains two separate latent spaces, for sentences and syntactic trees. The evidence lower bound objective is redesigned correspondingly, by optimizing a joint distribution that accommodates two encoders and two decoders. SIVAE works with long short-term memory architectures to simultaneously generate sentences and syntactic trees. Two versions of SIVAE are proposed: one captures the dependencies between the latent variables through a conditional prior network, and the other treats the latent variables independently such that syntactically-controlled sentence generation can be performed. Experimental results demonstrate the generative superiority of SIVAE on both reconstruction and targeted syntactic evaluations. Finally, we show that the proposed models can be used for unsupervised paraphrasing given different syntactic tree templates.",,,,ACL
200,2019,Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models,"Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun Chen, Xin Wang","Variational autoencoders (VAEs) have received much attention recently as an end-to-end architecture for text generation with latent variables. However, previous works typically focus on synthesizing relatively short sentences (up to 20 words), and the posterior collapse issue has been widely identified in text-VAEs. In this paper, we propose to leverage several multi-level structures to learn a VAE model for generating long, and coherent text. In particular, a hierarchy of stochastic layers between the encoder and decoder networks is employed to abstract more informative and semantic-rich latent codes. Besides, we utilize a multi-level decoder structure to capture the coherent long-term structure inherent in long-form texts, by generating intermediate sentence representations as high-level plan vectors. Extensive experimental results demonstrate that the proposed multi-level VAE model produces more coherent and less repetitive long text compared to baselines as well as can mitigate the posterior-collapse issue.",,,,ACL
201,2019,Jointly Learning Semantic Parser and Natural Language Generator via Dual Information Maximization,"Hai Ye, Wenjie Li, Lu Wang","Semantic parsing aims to transform natural language (NL) utterances into formal meaning representations (MRs), whereas an NL generator achieves the reverse: producing an NL description for some given MRs. Despite this intrinsic connection, the two tasks are often studied separately in prior work. In this paper, we model the duality of these two tasks via a joint learning framework, and demonstrate its effectiveness of boosting the performance on both tasks. Concretely, we propose a novel method of dual information maximization (DIM) to regularize the learning process, where DIM empirically maximizes the variational lower bounds of expected joint distributions of NL and MRs. We further extend DIM to a semi-supervision setup (SemiDIM), which leverages unlabeled data of both tasks. Experiments on three datasets of dialogue management and code generation (and summarization) show that performance on both semantic parsing and NL generation can be consistently improved by DIM, in both supervised and semi-supervised setups.",,,,ACL
202,2019,"Learning to Select, Track, and Generate for Data-to-Text","Hayate Iso, Yui Uehara, Tatsuya Ishigaki, Hiroshi Noji, Eiji Aramaki","We propose a data-to-text generation model with two modules, one for tracking and the other for text generation. Our tracking module selects and keeps track of salient information and memorizes which record has been mentioned. Our generation module generates a summary conditioned on the state of tracking module. Our proposed model is considered to simulate the human-like writing process that gradually selects the information by determining the intermediate variables while writing the summary. In addition, we also explore the effectiveness of the writer information for generations. Experimental results show that our proposed model outperforms existing models in all evaluation metrics even without writer information. Incorporating writer information further improves the performance, contributing to content planning and surface realization.",,,,ACL
203,2019,Reinforced Dynamic Reasoning for Conversational Question Generation,"Boyuan Pan, Hao Li, Ziyu Yao, Deng Cai, Huan Sun","This paper investigates a new task named Conversational Question Generation (CQG) which is to generate a question based on a passage and a conversation history (i.e., previous turns of question-answer pairs). CQG is a crucial task for developing intelligent agents that can drive question-answering style conversations or test user understanding of a given passage. Towards that end, we propose a new approach named Reinforced Dynamic Reasoning network, which is based on the general encoder-decoder framework but incorporates a reasoning procedure in a dynamic manner to better understand what has been asked and what to ask next about the passage into the general encoder-decoder framework. To encourage producing meaningful questions, we leverage a popular question answering (QA) model to provide feedback and fine-tune the question generator using a reinforcement learning mechanism. Empirical results on the recently released CoQA dataset demonstrate the effectiveness of our method in comparison with various baselines and model variants. Moreover, to show the applicability of our method, we also apply it to create multi-turn question-answering conversations for passages in SQuAD.",,,,ACL
204,2019,TalkSumm: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks,"Guy Lev, Michal Shmueli-Scheuer, Jonathan Herzig, Achiya Jerbi, David Konopnicki","Currently, no large-scale training data is available for the task of scientific paper summarization. In this paper, we propose a novel method that automatically generates summaries for scientific papers, by utilizing videos of talks at scientific conferences. We hypothesize that such talks constitute a coherent and concise description of the papers’ content, and can form the basis for good summaries. We collected 1716 papers and their corresponding videos, and created a dataset of paper summaries. A model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually. In addition, we validated the quality of our summaries by human experts.",,,,ACL
205,2019,Improving Abstractive Document Summarization with Salient Information Modeling,"Yongjian You, Weijia Jia, Tianyi Liu, Wenmian Yang","Comprehensive document encoding and salient information selection are two major difficulties for generating summaries with adequate salient information. To tackle the above difficulties, we propose a Transformer-based encoder-decoder framework with two novel extensions for abstractive document summarization. Specifically, (1) to encode the documents comprehensively, we design a focus-attention mechanism and incorporate it into the encoder. This mechanism models a Gaussian focal bias on attention scores to enhance the perception of local context, which contributes to producing salient and informative summaries. (2) To distinguish salient information precisely, we design an independent saliency-selection network which manages the information flow from encoder to decoder. This network effectively reduces the influences of secondary information on the generated summaries. Experimental results on the popular CNN/Daily Mail benchmark demonstrate that our model outperforms other state-of-the-art baselines on the ROUGE metrics.",,,,ACL
206,2019,Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking,"Masaru Isonuma, Junichiro Mori, Ichiro Sakata","This paper focuses on the end-to-end abstractive summarization of a single product review without supervision. We assume that a review can be described as a discourse tree, in which the summary is the root, and the child sentences explain their parent in detail. By recursively estimating a parent from its children, our model learns the latent discourse tree without an external parser and generates a concise summary. We also introduce an architecture that ranks the importance of each sentence on the tree to support summary generation focusing on the main review point. The experimental results demonstrate that our model is competitive with or outperforms other unsupervised approaches. In particular, for relatively long reviews, it achieves a competitive or better performance than supervised models. The induced tree shows that the child sentences provide additional information about their parent, and the generated summary abstracts the entire review.",,,,ACL
207,2019,BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization,"Kai Wang, Xiaojun Quan, Rui Wang","The success of neural summarization models stems from the meticulous encodings of source articles. To overcome the impediments of limited and sometimes noisy training data, one promising direction is to make better use of the available training data by applying filters during summarization. In this paper, we propose a novel Bi-directional Selective Encoding with Template (BiSET) model, which leverages template discovered from training data to softly select key information from each source article to guide its summarization process. Extensive experiments on a standard summarization dataset are conducted and the results show that the template-equipped BiSET model manages to improve the summarization performance significantly with a new state of the art.",,,,ACL
208,2019,Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards,"Hou Pong Chan, Wang Chen, Lu Wang, Irwin King","Generating keyphrases that summarize the main points of a document is a fundamental task in natural language processing. Although existing generative models are capable of predicting multiple keyphrases for an input document as well as determining the number of keyphrases to generate, they still suffer from the problem of generating too few keyphrases. To address this problem, we propose a reinforcement learning (RL) approach for keyphrase generation, with an adaptive reward function that encourages a model to generate both sufficient and accurate keyphrases. Furthermore, we introduce a new evaluation method that incorporates name variations of the ground-truth keyphrases using the Wikipedia knowledge base. Thus, our evaluation method can more robustly evaluate the quality of predicted keyphrases. Extensive experiments on five real-world datasets of different scales demonstrate that our RL approach consistently and significantly improves the performance of the state-of-the-art generative models with both conventional and new evaluation methods.",,,,ACL
209,2019,Scoring Sentence Singletons and Pairs for Abstractive Summarization,"Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon Kim, Seokhwan Kim","When writing a summary, humans tend to choose content from one or two sentences and merge them into a single summary sentence. However, the mechanisms behind the selection of one or multiple source sentences remain poorly understood. Sentence fusion assumes multi-sentence input; yet sentence selection methods only work with single sentences and not combinations of them. There is thus a crucial gap between sentence selection and fusion to support summarizing by both compressing single sentences and fusing pairs. This paper attempts to bridge the gap by ranking sentence singletons and pairs together in a unified space. Our proposed framework attempts to model human methodology by selecting either a single sentence or a pair of sentences, then compressing or fusing the sentence(s) to produce a summary sentence. We conduct extensive experiments on both single- and multi-document summarization datasets and report findings on sentence selection and abstraction.",,,,ACL
210,2019,Keep Meeting Summaries on Topic: Abstractive Multi-Modal Meeting Summarization,"Manling Li, Lingyu Zhang, Heng Ji, Richard J. Radke","Transcripts of natural, multi-person meetings differ significantly from documents like news articles, which can make Natural Language Generation models for generating summaries unfocused. We develop an abstractive meeting summarizer from both videos and audios of meeting recordings. Specifically, we propose a multi-modal hierarchical attention across three levels: segment, utterance and word. To narrow down the focus into topically-relevant segments, we jointly model topic segmentation and summarization. In addition to traditional text features, we introduce new multi-modal features derived from visual focus of attention, based on the assumption that the utterance is more important if the speaker receives more attention. Experiments show that our model significantly outperforms the state-of-the-art with both BLEU and ROUGE measures.",,,,ACL
211,2019,Adversarial Domain Adaptation Using Artificial Titles for Abstractive Title Generation,"Francine Chen, Yan-Ying Chen","A common issue in training a deep learning, abstractive summarization model is lack of a large set of training summaries. This paper examines techniques for adapting from a labeled source domain to an unlabeled target domain in the context of an encoder-decoder model for text generation. In addition to adversarial domain adaptation (ADA), we introduce the use of artificial titles and sequential training to capture the grammatical style of the unlabeled target domain. Evaluation on adapting to/from news articles and Stack Exchange posts indicates that the use of these techniques can boost performance for both unsupervised adaptation as well as fine-tuning with limited target data.",,,,ACL
212,2019,BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization,"Eva Sharma, Chen Li, Lu Wang","Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article’s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.",,,,ACL
213,2019,Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference,"Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, Iryna Gurevych","While recent progress on abstractive summarization has led to remarkably fluent summaries, factual errors in generated summaries still severely limit their use in practice. In this paper, we evaluate summaries produced by state-of-the-art models via crowdsourcing and show that such errors occur frequently, in particular with more abstractive models. We study whether textual entailment predictions can be used to detect such errors and if they can be reduced by reranking alternative predicted summaries. That leads to an interesting downstream application for entailment models. In our experiments, we find that out-of-the-box entailment models trained on NLI datasets do not yet offer the desired performance for the downstream task and we therefore release our annotations as additional test data for future extrinsic evaluations of NLI.",,,,ACL
214,2019,Self-Supervised Learning for Contextualized Extractive Summarization,"Hong Wang, Xin Wang, Wenhan Xiong, Mo Yu, Xiaoxiao Guo","Existing models for extractive summarization are usually trained from scratch with a cross-entropy loss, which does not explicitly capture the global context at the document level. In this paper, we aim to improve this task by introducing three auxiliary pre-training tasks that learn to capture the document-level context in a self-supervised fashion. Experiments on the widely-used CNN/DM dataset validate the effectiveness of the proposed auxiliary tasks. Furthermore, we show that after pre-training, a clean model with simple building blocks is able to outperform previous state-of-the-art that are carefully designed.",,,,ACL
215,2019,On the Summarization of Consumer Health Questions,"Asma Ben Abacha, Dina Demner-Fushman","Question understanding is one of the main challenges in question answering. In real world applications, users often submit natural language questions that are longer than needed and include peripheral information that increases the complexity of the question, leading to substantially more false positives in answer retrieval. In this paper, we study neural abstractive models for medical question summarization. We introduce the MeQSum corpus of 1,000 summarized consumer health questions. We explore data augmentation methods and evaluate state-of-the-art neural abstractive models on this new task. In particular, we show that semantic augmentation from question datasets improves the overall performance, and that pointer-generator networks outperform sequence-to-sequence attentional models on this task, with a ROUGE-1 score of 44.16%. We also present a detailed error analysis and discuss directions for improvement that are specific to question summarization.",,,,ACL
216,2019,Unsupervised Rewriter for Multi-Sentence Compression,"Yang Zhao, Xiaoyu Shen, Wei Bi, Akiko Aizawa","Multi-sentence compression (MSC) aims to generate a grammatical but reduced compression from multiple input sentences while retaining their key information. Previous dominating approach for MSC is the extraction-based word graph approach. A few variants further leveraged lexical substitution to yield more abstractive compression. However, two limitations exist. First, the word graph approach that simply concatenates fragments from multiple sentences may yield non-fluent or ungrammatical compression. Second, lexical substitution is often inappropriate without the consideration of context information. To tackle the above-mentioned issues, we present a neural rewriter for multi-sentence compression that does not need any parallel corpus. Empirical studies have shown that our approach achieves comparable results upon automatic evaluation and improves the grammaticality of compression based on human evaluation. A parallel corpus with more than 140,000 (sentence group, compression) pairs is also constructed as a by-product for future research.",,,,ACL
217,2019,Inferential Machine Comprehension: Answering Questions by Recursively Deducing the Evidence Chain from Text,"Jianxing Yu, Zhengjun Zha, Jian Yin","This paper focuses on the topic of inferential machine comprehension, which aims to fully understand the meanings of given text to answer generic questions, especially the ones needed reasoning skills. In particular, we first encode the given document, question and options in a context aware way. We then propose a new network to solve the inference problem by decomposing it into a series of attention-based reasoning steps. The result of the previous step acts as the context of next step. To make each step can be directly inferred from the text, we design an operational cell with prior structure. By recursively linking the cells, the inferred results are synthesized together to form the evidence chain for reasoning, where the reasoning direction can be guided by imposing structural constraints to regulate interactions on the cells. Moreover, a termination mechanism is introduced to dynamically determine the uncertain reasoning depth, and the network is trained by reinforcement learning. Experimental results on 3 popular data sets, including MCTest, RACE and MultiRC, demonstrate the effectiveness of our approach.",,,,ACL
218,2019,Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension,"Yimeng Zhuang, Huadong Wang","Multi-passage reading comprehension requires the ability to combine cross-passage information and reason over multiple passages to infer the answer. In this paper, we introduce the Dynamic Self-attention Network (DynSAN) for multi-passage reading comprehension task, which processes cross-passage information at token-level and meanwhile avoids substantial computational costs. The core module of the dynamic self-attention is a proposed gated token selection mechanism, which dynamically selects important tokens from a sequence. These chosen tokens will attend to each other via a self-attention mechanism to model long-range dependencies. Besides, convolutional layers are combined with the dynamic self-attention to enhance the model’s capacity of extracting local semantic. The experimental results show that the proposed DynSAN achieves new state-of-the-art performance on the SearchQA, Quasar-T and WikiHop datasets. Further ablation study also validates the effectiveness of our model components.",,,,ACL
219,2019,Explicit Utilization of General Knowledge in Machine Reading Comprehension,"Chao Wang, Hui Jiang","To bridge the gap between Machine Reading Comprehension (MRC) models and human beings, which is mainly reflected in the hunger for data and the robustness to noise, in this paper, we explore how to integrate the neural networks of MRC models with the general knowledge of human beings. On the one hand, we propose a data enrichment method, which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage-question pair. On the other hand, we propose an end-to-end MRC model named as Knowledge Aided Reader (KAR), which explicitly uses the above extracted general knowledge to assist its attention mechanisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them. When only a subset (20%-80%) of the training examples are available, KAR outperforms the state-of-the-art MRC models by a large margin, and is still reasonably robust to noise.",,,,ACL
220,2019,Multi-style Generative Reading Comprehension,"Kyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda, Atsushi Otsuka","This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We propose a multi-style abstractive summarization model for question answering, called Masque. The proposed model has two key characteristics. First, unlike most studies on RC that have focused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications. Second, whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model, our approach learns multi-style answers within a model to improve the NLG capability for all styles involved. This also enables our model to give an answer in the target style. Experiments show that our model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the transfer of the style-independent NLG capability to the target style is the key to its success.",,,,ACL
221,2019,"Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension","Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li","This paper considers the reading comprehension task in which multiple documents are given as input. Prior work has shown that a pipeline of retriever, reader, and reranker can improve the overall performance. However, the pipeline system is inefficient since the input is re-encoded within each module, and is unable to leverage upstream components to help downstream training. In this work, we present RE3QA, a unified question answering model that combines context retrieving, reading comprehension, and answer reranking to predict the final answer. Unlike previous pipelined approaches, RE3QA shares contextualized text representation across different components, and is carefully designed to use high-quality upstream outputs (e.g., retrieved context or candidate answers) for directly supervising downstream modules (e.g., the reader or the reranker). As a result, the whole network can be trained end-to-end to avoid the context inconsistency problem. Experiments show that our model outperforms the pipelined baseline and achieves state-of-the-art results on two versions of TriviaQA and two variants of SQuAD.",,,,ACL
222,2019,Multi-Hop Paragraph Retrieval for Open-Domain Question Answering,"Yair Feldman, Ran El-Yaniv","This paper is concerned with the task of multi-hop open-domain Question Answering (QA). This task is particularly challenging since it requires the simultaneous performance of textual reasoning and efficient searching. We present a method for retrieving multiple supporting paragraphs, nested amidst a large knowledge base, which contain the necessary evidence to answer a given question. Our method iteratively retrieves supporting paragraphs by forming a joint vector representation of both a question and a paragraph. The retrieval is performed by considering contextualized sentence-level representations of the paragraphs in the knowledge source. Our method achieves state-of-the-art performance over two well-known datasets, SQuAD-Open and HotpotQA, which serve as our single- and multi-hop open-domain QA benchmarks, respectively.",,,,ACL
223,2019,E3: Entailment-driven Extracting and Editing for Conversational Machine Reading,"Victor Zhong, Luke Zettlemoyer","Conversational machine reading systems help users answer high-level questions (e.g. determine if they qualify for particular government benefits) when they do not know the exact rules by which the determination is made (e.g. whether they need certain income levels or veteran status). The key challenge is that these rules are only provided in the form of a procedural text (e.g. guidelines from government website) which the system must read to figure out what to ask the user. We present a new conversational machine reading model that jointly extracts a set of decision rules from the procedural text while reasoning about which are entailed by the conversational history and which still need to be edited to create questions for the user. On the recently introduced ShARC conversational machine reading dataset, our Entailment-driven Extract and Edit network (E3) achieves a new state-of-the-art, outperforming existing systems as well as a new BERT-based baseline. In addition, by explicitly highlighting which information still needs to be gathered, E3 provides a more explainable alternative to prior work. We release source code for our models and experiments at https://github.com/vzhong/e3.",,,,ACL
224,2019,Generating Question-Answer Hierarchies,"Kalpesh Krishna, Mohit Iyyer","The process of knowledge acquisition can be viewed as a question-answer game between a student and a teacher in which the student typically starts by asking broad, open-ended questions before drilling down into specifics (Hintikka, 1981; Hakkarainen and Sintonen, 2002). This pedagogical perspective motivates a new way of representing documents. In this paper, we present SQUASH (Specificity-controlled Question-Answer Hierarchies), a novel and challenging text generation task that converts an input document into a hierarchy of question-answer pairs. Users can click on high-level questions (e.g., “Why did Frodo leave the Fellowship?”) to reveal related but more specific questions (e.g., “Who did Frodo leave with?”). Using a question taxonomy loosely based on Lehnert (1978), we classify questions in existing reading comprehension datasets as either GENERAL or SPECIFIC . We then use these labels as input to a pipelined system centered around a conditional neural language model. We extensively evaluate the quality of the generated QA hierarchies through crowdsourced experiments and report strong empirical results.",,,,ACL
225,2019,Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction,"Kosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito","Question answering (QA) using textual sources for purposes such as reading comprehension (RC) has attracted much attention. This study focuses on the task of explainable multi-hop QA, which requires the system to return the answer with evidence sentences by reasoning and gathering disjoint pieces of the reference texts. It proposes the Query Focused Extractor (QFE) model for evidence extraction and uses multi-task learning with the QA model. QFE is inspired by extractive summarization models; compared with the existing method, which extracts each evidence sentence independently, it sequentially extracts evidence sentences by using an RNN with an attention mechanism on the question sentence. It enables QFE to consider the dependency among the evidence sentences and cover important information in the question sentence. Experimental results show that QFE with a simple RC baseline model achieves a state-of-the-art evidence extraction score on HotpotQA. Although designed for RC, it also achieves a state-of-the-art evidence extraction score on FEVER, which is a recognizing textual entailment task on a large textual database.",,,,ACL
226,2019,Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension,"An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu","Machine reading comprehension (MRC) is a crucial and challenging task in NLP. Recently, pre-trained language models (LMs), especially BERT, have achieved remarkable success, presenting new state-of-the-art results in MRC. In this work, we investigate the potential of leveraging external knowledge bases (KBs) to further improve BERT for MRC. We introduce KT-NET, which employs an attention mechanism to adaptively select desired knowledge from KBs, and then fuses selected knowledge with BERT to enable context- and knowledge-aware predictions. We believe this would combine the merits of both deep LMs and curated KBs towards better MRC. Experimental results indicate that KT-NET offers significant and consistent improvements over BERT, outperforming competitive baselines on ReCoRD and SQuAD1.1 benchmarks. Notably, it ranks the 1st place on the ReCoRD leaderboard, and is also the best single model on the SQuAD1.1 leaderboard at the time of submission (March 4th, 2019).",,,,ACL
227,2019,XQA: A Cross-lingual Open-domain Question Answering Dataset,"Jiahua Liu, Yankai Lin, Zhiyuan Liu, Maosong Sun","Open-domain question answering (OpenQA) aims to answer questions through text retrieval and reading comprehension. Recently, lots of neural network-based models have been proposed and achieved promising results in OpenQA. However, the success of these models relies on a massive volume of training data (usually in English), which is not available in many other languages, especially for those low-resource languages. Therefore, it is essential to investigate cross-lingual OpenQA. In this paper, we construct a novel dataset XQA for cross-lingual OpenQA research. It consists of a training set in English as well as development and test sets in eight other languages. Besides, we provide several baseline systems for cross-lingual OpenQA, including two machine translation-based methods and one zero-shot cross-lingual method (multilingual BERT). Experimental results show that the multilingual BERT model achieves the best results in almost all target languages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA.",,,,ACL
228,2019,Compound Probabilistic Context-Free Grammars for Grammar Induction,"Yoon Kim, Chris Dyer, Alexander Rush","We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models.",,,,ACL
229,2019,Semi-supervised Domain Adaptation for Dependency Parsing,"Zhenghua Li, Xue Peng, Min Zhang, Rui Wang, Luo Si","During the past decades, due to the lack of sufficient labeled data, most studies on cross-domain parsing focus on unsupervised domain adaptation, assuming there is no target-domain training data. However, unsupervised approaches make limited progress so far due to the intrinsic difficulty of both domain adaptation and parsing. This paper tackles the semi-supervised domain adaptation problem for Chinese dependency parsing, based on two newly-annotated large-scale domain-aware datasets. We propose a simple domain embedding approach to merge the source- and target-domain training data, which is shown to be more effective than both direct corpus concatenation and multi-task learning. In order to utilize unlabeled target-domain data, we employ the recent contextualized word representations and show that a simple fine-tuning procedure can further boost cross-domain parsing accuracy by large margin.",,,,ACL
230,2019,Head-Driven Phrase Structure Grammar Parsing on Penn Treebank,"Junru Zhou, Hai Zhao","Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings. This paper makes the first attempt to formulate a simplified HPSG by integrating constituent and dependency formal representations into head-driven phrase structure. Then two parsing algorithms are respectively proposed for two converted tree representations, division span and joint span. As HPSG encodes both constituent and dependency structure information, the proposed HPSG parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees. Our parser achieves new state-of-the-art performance for both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank, verifying the effectiveness of joint learning constituent and dependency structures. In details, we report 95.84 F1 of constituent parsing and 97.00% UAS of dependency parsing on PTB.",,,,ACL
231,2019,Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning,"Minlong Peng, Xiaoyu Xing, Qi Zhang, Jinlan Fu, Xuanjing Huang","In this work, we explore the way to perform named entity recognition (NER) using only unlabeled data and named entity dictionaries. To this end, we formulate the task as a positive-unlabeled (PU) learning problem and accordingly propose a novel PU learning algorithm to perform the task. We prove that the proposed algorithm can unbiasedly and consistently estimate the task loss as if there is fully labeled data. A key feature of the proposed method is that it does not require the dictionaries to label every entity within a sentence, and it even does not require the dictionaries to label all of the words constituting an entity. This greatly reduces the requirement on the quality of the dictionaries and makes our method generalize well with quite simple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at https://github.com/v-mipeng/LexiconNER.",,,,ACL
232,2019,Multi-Task Semantic Dependency Parsing with Policy Gradient for Learning Easy-First Strategies,"Shuhei Kurita, Anders Søgaard","In Semantic Dependency Parsing (SDP), semantic relations form directed acyclic graphs, rather than trees. We propose a new iterative predicate selection (IPS) algorithm for SDP. Our IPS algorithm combines the graph-based and transition-based parsing approaches in order to handle multiple semantic head words. We train the IPS model using a combination of multi-task learning and task-specific policy gradient training. Trained this way, IPS achieves a new state of the art on the SemEval 2015 Task 18 datasets. Furthermore, we observe that policy gradient training learns an easy-first strategy.",,,,ACL
233,2019,GCDT: A Global Context Enhanced Deep Transition Architecture for Sequence Labeling,"Yijin Liu, Fandong Meng, Jinchao Zhang, Jinan Xu, Yufeng Chen","Current state-of-the-art systems for sequence labeling are typically based on the family of Recurrent Neural Networks (RNNs). However, the shallow connections between consecutive hidden states of RNNs and insufficient modeling of global information restrict the potential performance of those models. In this paper, we try to address these issues, and thus propose a Global Context enhanced Deep Transition architecture for sequence labeling named GCDT. We deepen the state transition path at each position in a sentence, and further assign every token with a global representation learned from the entire sentence. Experiments on two standard sequence labeling tasks show that, given only training data and the ubiquitous word embeddings (Glove), our GCDT achieves 91.96 F1 on the CoNLL03 NER task and 95.43 F1 on the CoNLL2000 Chunking task, which outperforms the best reported results under the same settings. Furthermore, by leveraging BERT as an additional resource, we establish new state-of-the-art results with 93.47 F1 on NER and 97.30 F1 on Chunking.",,,,ACL
234,2019,Unsupervised Learning of PCFGs with Normalizing Flow,"Lifeng Jin, Finale Doshi-Velez, Timothy Miller, Lane Schwartz, William Schuler","Unsupervised PCFG inducers hypothesize sets of compact context-free rules as explanations for sentences. PCFG induction not only provides tools for low-resource languages, but also plays an important role in modeling language acquisition (Bannard et al., 2009; Abend et al. 2017). However, current PCFG induction models, using word tokens as input, are unable to incorporate semantics and morphology into induction, and may encounter issues of sparse vocabulary when facing morphologically rich languages. This paper describes a neural PCFG inducer which employs context embeddings (Peters et al., 2018) in a normalizing flow model (Dinh et al., 2015) to extend PCFG induction to use semantic and morphological information. Linguistically motivated sparsity and categorical distance constraints are imposed on the inducer as regularization. Experiments show that the PCFG induction model with normalizing flow produces grammars with state-of-the-art accuracy on a variety of different languages. Ablation further shows a positive effect of normalizing flow, context embeddings and proposed regularizers.",,,,ACL
235,2019,Variance of Average Surprisal: A Better Predictor for Quality of Grammar from Unsupervised PCFG Induction,"Lifeng Jin, William Schuler","In unsupervised grammar induction, data likelihood is known to be only weakly correlated with parsing accuracy, especially at convergence after multiple runs. In order to find a better indicator for quality of induced grammars, this paper correlates several linguistically- and psycholinguistically-motivated predictors to parsing accuracy on a large multilingual grammar induction evaluation data set. Results show that variance of average surprisal (VAS) better correlates with parsing accuracy than data likelihood and that using VAS instead of data likelihood for model selection provides a significant accuracy boost. Further evidence shows VAS to be a better candidate than data likelihood for predicting word order typology classification. Analyses show that VAS seems to separate content words from function words in natural language grammars, and to better arrange words with different frequencies into separate classes that are more consistent with linguistic theory.",,,,ACL
236,2019,Cross-Domain NER using Cross-Domain Language Modeling,"Chen Jia, Xiaobo Liang, Yue Zhang","Due to limitation of labeled resources, cross-domain named entity recognition (NER) has been a challenging task. Most existing work considers a supervised setting, making use of labeled data for both the source and target domains. A disadvantage of such methods is that they cannot train for domains without NER data. To address this issue, we consider using cross-domain LM as a bridge cross-domains for NER domain adaptation, performing cross-domain and cross-task knowledge transfer by designing a novel parameter generation network. Results show that our method can effectively extract domain differences from cross-domain LM contrast, allowing unsupervised domain adaptation while also giving state-of-the-art results among supervised domain adaptation methods.",,,,ACL
237,2019,Graph-based Dependency Parsing with Graph Neural Networks,"Tao Ji, Yuanbin Wu, Man Lan","We investigate the problem of efficiently incorporating high-order features into neural graph-based dependency parsing. Instead of explicitly extracting high-order features from intermediate parse trees, we develop a more powerful dependency tree node representation which captures high-order information concisely and efficiently. We use graph neural networks (GNNs) to learn the representations and discuss several new configurations of GNN’s updating and aggregation functions. Experiments on PTB show that our parser achieves the best UAS and LAS on PTB (96.0%, 94.3%) among systems without using any external resources.",,,,ACL
238,2019,Wide-Coverage Neural A* Parsing for Minimalist Grammars,"John Torr, Milos Stanojevic, Mark Steedman, Shay B. Cohen","Minimalist Grammars (Stabler, 1997) are a computationally oriented, and rigorous formalisation of many aspects of Chomsky’s (1995) Minimalist Program. This paper presents the first ever application of this formalism to the task of realistic wide-coverage parsing. The parser uses a linguistically expressive yet highly constrained grammar, together with an adaptation of the A* search algorithm currently used in CCG parsing (Lewis and Steedman, 2014; Lewis et al., 2016), with supertag probabilities provided by a bi-LSTM neural network supertagger trained on MGbank, a corpus of MG derivation trees. We report on some promising initial experimental results for overall dependency recovery as well as on the recovery of certain unbounded long distance dependencies. Finally, although like other MG parsers, ours has a high order polynomial worst case time complexity, we show that in practice its expected time complexity is cubic in the length of the sentence. The parser is publicly available.",,,,ACL
239,2019,Multi-Modal Sarcasm Detection in Twitter with Hierarchical Fusion Model,"Yitao Cai, Huiyu Cai, Xiaojun Wan","Sarcasm is a subtle form of language in which people express the opposite of what is implied. Previous works of sarcasm detection focused on texts. However, more and more social media platforms like Twitter allow users to create multi-modal messages, including texts, images, and videos. It is insufficient to detect sarcasm from multi-model messages based only on texts. In this paper, we focus on multi-modal sarcasm detection for tweets consisting of texts and images in Twitter. We treat text features, image features and image attributes as three modalities and propose a multi-modal hierarchical fusion model to address this task. Our model first extracts image features and attribute features, and then leverages attribute features and bidirectional LSTM network to extract text features. Features of three modalities are then reconstructed and fused into one feature vector for prediction. We create a multi-modal sarcasm detection dataset based on Twitter. Evaluation results on the dataset demonstrate the efficacy of our proposed model and the usefulness of the three modalities.",,,,ACL
240,2019,Topic-Aware Neural Keyphrase Generation for Social Media Language,"Yue Wang, Jing Li, Hou Pong Chan, Irwin King, Michael R. Lyu","A huge volume of user-generated content is daily produced on social media. To facilitate automatic language understanding, we study keyphrase prediction, distilling salient information from massive posts. While most existing methods extract words from source posts to form keyphrases, we propose a sequence-to-sequence (seq2seq) based neural keyphrase generation framework, enabling absent keyphrases to be created. Moreover, our model, being topic-aware, allows joint modeling of corpus-level latent topic representations, which helps alleviate data sparsity widely exhibited in social media language. Experiments on three datasets collected from English and Chinese social media platforms show that our model significantly outperforms both extraction and generation models without exploiting latent topics. Further discussions show that our model learns meaningful topics, which interprets its superiority in social media keyphrase generation.",,,,ACL
241,2019,#YouToo? Detection of Personal Recollections of Sexual Harassment on Social Media,"Arijit Ghosh Chowdhury, Ramit Sawhney, Rajiv Ratn Shah, Debanjan Mahata","The availability of large-scale online social data, coupled with computational methods can help us answer fundamental questions relat- ing to our social lives, particularly our health and well-being. The #MeToo trend has led to people talking about personal experiences of harassment more openly. This work at- tempts to aggregate such experiences of sex- ual abuse to facilitate a better understanding of social media constructs and to bring about social change. It has been found that disclo- sure of abuse has positive psychological im- pacts. Hence, we contend that such informa- tion can leveraged to create better campaigns for social change by analyzing how users react to these stories and to obtain a better insight into the consequences of sexual abuse. We use a three part Twitter-Specific Social Media Lan- guage Model to segregate personal recollec- tions of sexual harassment from Twitter posts. An extensive comparison with state-of-the-art generic and specific models along with a de- tailed error analysis explores the merit of our proposed model.",,,,ACL
242,2019,Multi-task Pairwise Neural Ranking for Hashtag Segmentation,"Mounica Maddela, Wei Xu, Daniel Preoţiuc-Pietro","Hashtags are often employed on social media and beyond to add metadata to a textual utterance with the goal of increasing discoverability, aiding search, or providing additional semantics. However, the semantic content of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation by framing it as a pairwise ranking problem between candidate segmentations. Our novel neural approaches demonstrate 24.6% error reduction in hashtag segmentation accuracy compared to the current state-of-the-art method. Finally, we demonstrate that a deeper understanding of hashtag semantics obtained through segmentation is useful for downstream applications such as sentiment analysis, for which we achieved a 2.6% increase in average recall on the SemEval 2017 sentiment analysis dataset.",,,,ACL
243,2019,Entity-Centric Contextual Affective Analysis,"Anjalie Field, Yulia Tsvetkov","While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks, their potential usefulness for social-oriented tasks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women.",,,,ACL
244,2019,Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks,"Jing Ma, Wei Gao, Shafiq Joty, Kam-Fai Wong","Claim verification is generally a task of verifying the veracity of a given claim, which is critical to many downstream applications. It is cumbersome and inefficient for human fact-checkers to find consistent pieces of evidence, from which solid verdict could be inferred against the claim. In this paper, we propose a novel end-to-end hierarchical attention network focusing on learning to represent coherent evidence as well as their semantic relatedness with the claim. Our model consists of three main components: 1) A coherence-based attention layer embeds coherent evidence considering the claim and sentences from relevant articles; 2) An entailment-based attention layer attends on sentences that can semantically infer the claim on top of the first attention; and 3) An output layer predicts the verdict based on the embedded evidence. Experimental results on three public benchmark datasets show that our proposed model outperforms a set of state-of-the-art baselines.",,,,ACL
245,2019,Predicting Human Activities from User-Generated Content,"Steven Wilson, Rada Mihalcea","The activities we do are linked to our interests, personality, political preferences, and decisions we make about the future. In this paper, we explore the task of predicting human activities from user-generated content. We collect a dataset containing instances of social media users writing about a range of everyday activities. We then use a state-of-the-art sentence embedding framework tailored to recognize the semantics of human activities and perform an automatic clustering of these activities. We train a neural network model to make predictions about which clusters contain activities that were performed by a given user based on the text of their previous posts and self-description. Additionally, we explore the degree to which incorporating inferred user traits into our model helps with this prediction task.",,,,ACL
246,2019,You Write like You Eat: Stylistic Variation as a Predictor of Social Stratification,"Angelo Basile, Albert Gatt, Malvina Nissim","Inspired by Labov’s seminal work on stylisticvariation as a function of social stratification,we develop and compare neural models thatpredict a person’s presumed socio-economicstatus, obtained through distant supervision,from their writing style on social media. Thefocus of our work is on identifying the mostimportant stylistic parameters to predict socio-economic group. In particular, we show theeffectiveness of morpho-syntactic features aspredictors of style, in contrast to lexical fea-tures, which are good predictors of topic",,,,ACL
247,2019,Encoding Social Information with Graph Convolutional Networks forPolitical Perspective Detection in News Media,"Chang Li, Dan Goldwasser","Identifying the political perspective shaping the way news events are discussed in the media is an important and challenging task. In this paper, we highlight the importance of contextualizing social information, capturing how this information is disseminated in social networks. We use Graph Convolutional Networks, a recently proposed neural architecture for representing relational information, to capture the documents’ social context. We show that social information can be used effectively as a source of distant supervision, and when direct supervision is available, even little social information can significantly improve performance.",,,,ACL
248,2019,Fine-Grained Spoiler Detection from Large-Scale Review Corpora,"Mengting Wan, Rishabh Misra, Ndapa Nakashole, Julian McAuley","This paper presents computational approaches for automatically detecting critical plot twists in reviews of media products. First, we created a large-scale book review dataset that includes fine-grained spoiler annotations at the sentence-level, as well as book and (anonymized) user information. Second, we carefully analyzed this dataset, and found that: spoiler language tends to be book-specific; spoiler distributions vary greatly across books and review authors; and spoiler sentences tend to jointly appear in the latter part of reviews. Third, inspired by these findings, we developed an end-to-end neural network architecture to detect spoiler sentences in review corpora. Quantitative and qualitative results demonstrate that the proposed method substantially outperforms existing baselines.",,,,ACL
249,2019,Celebrity Profiling,"Matti Wiegmann, Benno Stein, Martin Potthast","Celebrities are among the most prolific users of social media, promoting their personas and rallying followers. This activity is closely tied to genuine writing samples, which makes them worthy research subjects in many respects, not least profiling. With this paper we introduce the Webis Celebrity Corpus 2019. For its construction the Twitter feeds of 71,706 verified accounts have been carefully linked with their respective Wikidata items, crawling both. After cleansing, the resulting profiles contain an average of 29,968 words per profile and up to 239 pieces of personal information. A cross-evaluation that checked the correct association of Twitter account and Wikidata item revealed an error rate of only 0.6%, rendering the profiles highly reliable. Our corpus comprises a wide cross-section of local and global celebrities, forming a unique combination of scale, profile comprehensiveness, and label reliability. We further establish the state of the art’s profiling performance by evaluating the winning approaches submitted to the PAN gender prediction tasks in a transfer learning experiment. They are only outperformed by our own deep learning approach, which we also use to exemplify celebrity occupation prediction for the first time.",,,,ACL
250,2019,Dataset Creation for Ranking Constructive News Comments,"Soichiro Fujita, Hayato Kobayashi, Manabu Okumura","Ranking comments on an online news service is a practically important task for the service provider, and thus there have been many studies on this task. However, most of them considered users’ positive feedback, such as “Like”-button clicks, as a quality measure. In this paper, we address directly evaluating the quality of comments on the basis of “constructiveness,” separately from user feedback. To this end, we create a new dataset including 100K+ Japanese comments with constructiveness scores (C-scores). Our experiments clarify that C-scores are not always related to users’ positive feedback, and the performance of pairwise ranking models tends to be enhanced by the variation of comments rather than articles.",,,,ACL
251,2019,Enhancing Air Quality Prediction with Social Media and Natural Language Processing,"Jyun-Yu Jiang, Xue Sun, Wei Wang, Sean Young","Accompanied by modern industrial developments, air pollution has already become a major concern for human health. Hence, air quality measures, such as the concentration of PM2.5, have attracted increasing attention. Even some studies apply historical measurements into air quality forecast, the changes of air quality conditions are still hard to monitor. In this paper, we propose to exploit social media and natural language processing techniques to enhance air quality prediction. Social media users are treated as social sensors with their findings and locations. After filtering noisy tweets using word selection and topic modeling, a deep learning model based on convolutional neural networks and over-tweet-pooling is proposed to enhance air quality prediction. We conduct experiments on 7-month real-world Twitter datasets in the five most heavily polluted states in the USA. The results show that our approach significantly improves air quality prediction over the baseline that does not use social media by 6.9% to 17.7% in macro-F1 scores.",,,,ACL
252,2019,Twitter Homophily: Network Based Prediction of User’s Occupation,"Jiaqi Pan, Rishabh Bhardwaj, Wei Lu, Hai Leong Chieu, Xinghao Pan","In this paper, we investigate the importance of social network information compared to content information in the prediction of a Twitter user’s occupational class. We show that the content information of a user’s tweets, the profile descriptions of a user’s follower/following community, and the user’s social network provide useful information for classifying a user’s occupational group. In our study, we extend an existing data set for this problem, and we achieve significantly better performance by using social network homophily that has not been fully exploited in previous work. In our analysis, we found that by using the graph convolutional network to exploit social homophily, we can achieve competitive performance on this data set with just a small fraction of the training data.",,,,ACL
253,2019,Domain Adaptive Dialog Generation via Meta Learning,"Kun Qian, Zhou Yu","Domain adaptation is an essential task in dialog system building because there are so many new dialog tasks created for different needs every day. Collecting and annotating training data for these new tasks is costly since it involves real user interactions. We propose a domain adaptive dialog generation method based on meta-learning (DAML). DAML is an end-to-end trainable dialog system model that learns from multiple rich-resource tasks and then adapts to new domains with minimal training samples. We train a dialog system model using multiple rich-resource single-domain dialog data by applying the model-agnostic meta-learning algorithm to dialog domain. The model is capable of learning a competitive dialog system on a new domain with only a few training examples in an efficient manner. The two-step gradient updates in DAML enable the model to learn general features across multiple tasks. We evaluate our method on a simulated dialog dataset and achieve state-of-the-art performance, which is generalizable to new tasks.",,,,ACL
254,2019,Strategies for Structuring Story Generation,"Angela Fan, Mike Lewis, Yann Dauphin","Writers often rely on plans or sketches to write long stories, but most current language models generate word by word from left to right. We explore coarse-to-fine models for creating narrative texts of several hundred words, and introduce new models which decompose stories by abstracting over actions and entities. The model first generates the predicate-argument structure of the text, where different mentions of the same entity are marked with placeholder tokens. It then generates a surface realization of the predicate-argument structure, and finally replaces the entity placeholders with context-sensitive names and references. Human judges prefer the stories from our models to a wide range of previous approaches to hierarchical text generation. Extensive analysis shows that our methods can help improve the diversity and coherence of events and entities in generated stories.",,,,ACL
255,2019,"Argument Generation with Retrieval, Planning, and Realization","Xinyu Hua, Zhe Hu, Lu Wang","Automatic argument generation is an appealing but challenging task. In this paper, we study the specific problem of counter-argument generation, and present a novel framework, CANDELA. It consists of a powerful retrieval system and a novel two-step generation model, where a text planning decoder first decides on the main talking points and a proper language style for each sentence, then a content realization decoder reflects the decisions and constructs an informative paragraph-level argument. Furthermore, our generation model is empowered by a retrieval system indexed with 12 million articles collected from Wikipedia and popular English news media, which provides access to high-quality content with diversity. Automatic evaluation on a large-scale dataset collected from Reddit shows that our model yields significantly higher BLEU, ROUGE, and METEOR scores than the state-of-the-art and non-trivial comparisons. Human evaluation further indicates that our system arguments are more appropriate for refutation and richer in content.",,,,ACL
256,2019,A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation,"Feng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, Chin-Yew Lin","Recent neural language generation systems often hallucinate contents (i.e., producing irrelevant or contradicted facts), especially when trained on loosely corresponding pairs of the input structure and text. To mitigate this issue, we propose to integrate a language understanding module for data refinement with self-training iterations to effectively induce strong equivalence between the input data and the paired text. Experiments on the E2E challenge dataset show that our proposed framework can reduce more than 50% relative unaligned noise from the original data-text pairs. A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator.",,,,ACL
257,2019,Cross-Modal Commentator: Automatic Machine Commenting Based on Cross-Modal Information,"Pengcheng Yang, Zhihan Zhang, Fuli Luo, Lei Li, Chengyang Huang","Automatic commenting of online articles can provide additional opinions and facts to the reader, which improves user experience and engagement on social media platforms. Previous work focuses on automatic commenting based solely on textual content. However, in real-scenarios, online articles usually contain multiple modal contents. For instance, graphic news contains plenty of images in addition to text. Contents other than text are also vital because they are not only more attractive to the reader but also may provide critical information. To remedy this, we propose a new task: cross-model automatic commenting (CMAC), which aims to make comments by integrating multiple modal contents. We construct a large-scale dataset for this task and explore several representative methods. Going a step further, an effective co-attention model is presented to capture the dependency between textual and visual information. Evaluation results show that our proposed model can achieve better performance than competitive baselines.",,,,ACL
258,2019,A Working Memory Model for Task-oriented Dialog Response Generation,"Xiuyi Chen, Jiaming Xu, Bo Xu","Recently, to incorporate external Knowledge Base (KB) information, one form of world knowledge, several end-to-end task-oriented dialog systems have been proposed. These models, however, tend to confound the dialog history with KB tuples and simply store them into one memory. Inspired by the psychological studies on working memory, we propose a working memory model (WMM2Seq) for dialog response generation. Our WMM2Seq adopts a working memory to interact with two separated long-term memories, which are the episodic memory for memorizing dialog history and the semantic memory for storing KB tuples. The working memory consists of a central executive to attend to the aforementioned memories, and a short-term storage system to store the “activated” contents from the long-term memories. Furthermore, we introduce a context-sensitive perceptual process for the token representations of dialog history, and then feed them into the episodic memory. Extensive experiments on two task-oriented dialog datasets demonstrate that our WMM2Seq significantly outperforms the state-of-the-art results in several evaluation metrics.",,,,ACL
259,2019,Cognitive Graph for Multi-Hop Reading Comprehension at Scale,"Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, Jie Tang","We propose a new CogQA framework for multi-hop reading comprehension question answering in web-scale documents. Founded on the dual process theory in cognitive science, the framework gradually builds a cognitive graph in an iterative process by coordinating an implicit extraction module (System 1) and an explicit reasoning module (System 2). While giving accurate answers, our framework further provides explainable reasoning paths. Specifically, our implementation based on BERT and graph neural network efficiently handles millions of documents for multi-hop reasoning questions in the HotpotQA fullwiki dataset, achieving a winning joint F1 score of 34.9 on the leaderboard, compared to 23.1 of the best competitor.",,,,ACL
260,2019,Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs,"Ming Tu, Guangtao Wang, Jing Huang, Yun Tang, Xiaodong He","Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.",,,,ACL
261,2019,"Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension","Yichen Jiang, Nitish Joshi, Yen-Chun Chen, Mohit Bansal","Multi-hop reading comprehension requires the model to explore and connect relevant information from multiple sentences/documents in order to answer the question about the context. To achieve this, we propose an interpretable 3-module system called Explore-Propose-Assemble reader (EPAr). First, the Document Explorer iteratively selects relevant documents and represents divergent reasoning chains in a tree structure so as to allow assimilating information from all chains. The Answer Proposer then proposes an answer from every root-to-leaf path in the reasoning tree. Finally, the Evidence Assembler extracts a key sentence containing the proposed answer from every path and combines them to predict the final answer. Intuitively, EPAr approximates the coarse-to-fine-grained comprehension behavior of human readers when facing multiple long documents. We jointly optimize our 3 modules by minimizing the sum of losses from each stage conditioned on the previous stage’s output. On two multi-hop reading comprehension datasets WikiHop and MedHop, our EPAr model achieves significant improvements over the baseline and competitive results compared to the state-of-the-art model. We also present multiple reasoning-chain-recovery tests and ablation studies to demonstrate our system’s ability to perform interpretable and accurate reasoning.",,,,ACL
262,2019,"Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA","Yichen Jiang, Mohit Bansal","Multi-hop question answering requires a model to connect multiple pieces of evidence scattered in a long context to answer the question. In this paper, we show that in the multi-hop HotpotQA (Yang et al., 2018) dataset, the examples often contain reasoning shortcuts through which models can directly locate the answer by word-matching the question with a sentence in the context. We demonstrate this issue by constructing adversarial documents that create contradicting answers to the shortcut but do not affect the validity of the original answer. The performance of strong baseline models drops significantly on our adversarial test, indicating that they are indeed exploiting the shortcuts rather than performing multi-hop reasoning. After adversarial training, the baseline’s performance improves but is still limited on the adversarial test. Hence, we use a control unit that dynamically attends to the question at different reasoning hops to guide the model’s multi-hop reasoning. We show that our 2-hop model trained on the regular data is more robust to the adversaries than the baseline. After adversarial training, it not only achieves significant improvements over its counterpart trained on regular data, but also outperforms the adversarially-trained baseline significantly. Finally, we sanity-check that these improvements are not obtained by exploiting potential new shortcuts in the adversarial data, but indeed due to robust multi-hop reasoning skills of the models.",,,,ACL
263,2019,Exploiting Explicit Paths for Multi-hop Reading Comprehension,"Souvik Kundu, Tushar Khot, Ashish Sabharwal, Peter Clark","We propose a novel, path-based reasoning approach for the multi-hop reading comprehension task where a system needs to combine facts from multiple passages to answer a question. Although inspired by multi-hop reasoning over knowledge graphs, our proposed approach operates directly over unstructured text. It generates potential paths through passages and scores them without any direct path supervision. The proposed model, named PathNet, attempts to extract implicit relations from text through entity pair representations, and compose them to encode each path. To capture additional context, PathNet also composes the passage representations along each path to compute a passage-based representation. Unlike previous approaches, our model is then able to explain its reasoning via these explicit paths through the passages. We show that our approach outperforms prior models on the multi-hop Wikihop dataset, and also can be generalized to apply to the OpenBookQA dataset, matching state-of-the-art performance.",,,,ACL
264,2019,Sentence Mover’s Similarity: Automatic Evaluation for Multi-Sentence Texts,"Elizabeth Clark, Asli Celikyilmaz, Noah A. Smith","For evaluating machine-generated texts, automatic methods hold the promise of avoiding collection of human judgments, which can be expensive and time-consuming. The most common automatic metrics, like BLEU and ROUGE, depend on exact word matching, an inflexible approach for measuring semantic similarity. We introduce methods based on sentence mover’s similarity; our automatic metrics evaluate text in a continuous space using word and sentence embeddings. We find that sentence-based metrics correlate with human judgments significantly better than ROUGE, both on machine-generated summaries (average length of 3.4 sentences) and human-authored essays (average length of 7.5). We also show that sentence mover’s similarity can be used as a reward when learning a generation model via reinforcement learning; we present both automatic and human evaluations of summaries learned in this way, finding that our approach outperforms ROUGE.",,,,ACL
265,2019,Analysis of Automatic Annotation Suggestions for Hard Discourse-Level Tasks in Expert Domains,"Claudia Schulz, Christian M. Meyer, Jan Kiesewetter, Michael Sailer, Elisabeth Bauer","Many complex discourse-level tasks can aid domain experts in their work but require costly expert annotations for data creation. To speed up and ease annotations, we investigate the viability of automatically generated annotation suggestions for such tasks. As an example, we choose a task that is particularly hard for both humans and machines: the segmentation and classification of epistemic activities in diagnostic reasoning texts. We create and publish a new dataset covering two domains and carefully analyse the suggested annotations. We find that suggestions have positive effects on annotation speed and performance, while not introducing noteworthy biases. Envisioning suggestion models that improve with newly annotated texts, we contrast methods for continuous model adjustment and suggest the most effective setup for suggestions in future expert tasks.",,,,ACL
266,2019,Deep Dominance - How to Properly Compare Deep Neural Models,"Rotem Dror, Segev Shlomov, Roi Reichart","Comparing between Deep Neural Network (DNN) models based on their performance on unseen data is crucial for the progress of the NLP field. However, these models have a large number of hyper-parameters and, being non-convex, their convergence point depends on the random values chosen at initialization and during training. Proper DNN comparison hence requires a comparison between their empirical score distributions on unseen data, rather than between single evaluation scores as is standard for more simple, convex models. In this paper, we propose to adapt to this problem a recently proposed test for the Almost Stochastic Dominance relation between two distributions. We define the criteria for a high quality comparison method between DNNs, and show, both theoretically and through analysis of extensive experimental results with leading DNN models for sequence tagging tasks, that the proposed test meets all criteria while previously proposed methods fail to do so. We hope the test we propose here will set a new working practice in the NLP community.",,,,ACL
267,2019,We Need to Talk about Standard Splits,"Kyle Gorman, Steven Bedrick","It is standard practice in speech & language technology to rank systems according to their performance on a test set held out for evaluation. However, few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018, each of which claimed state-of-the-art performance on a widely-used “standard split”. While we replicate results on the standard split, we fail to reliably reproduce some rankings when we repeat this analysis with randomly generated training-testing splits. We argue that randomly generated splits should be used in system evaluation.",,,,ACL
268,2019,Aiming beyond the Obvious: Identifying Non-Obvious Cases in Semantic Similarity Datasets,"Nicole Peinelt, Maria Liakata, Dong Nguyen",Existing datasets for scoring text pairs in terms of semantic similarity contain instances whose resolution differs according to the degree of difficulty. This paper proposes to distinguish obvious from non-obvious text pairs based on superficial lexical overlap and ground-truth labels. We characterise existing datasets in terms of containing difficult cases and find that recently proposed models struggle to capture the non-obvious cases of semantic similarity. We describe metrics that emphasise cases of similarity which require more complex inference and propose that these are used for evaluating systems for semantic similarity.,,,,ACL
269,2019,Putting Evaluation in Context: Contextual Embeddings Improve Machine Translation Evaluation,"Nitika Mathur, Timothy Baldwin, Trevor Cohn","Accurate, automatic evaluation of machine translation is critical for system tuning, and evaluating progress in the field. We proposed a simple unsupervised metric, and additional supervised metrics which rely on contextual word embeddings to encode the translation and reference sentences. We find that these models rival or surpass all existing metrics in the WMT 2017 sentence-level and system-level tracks, and our trained model has a substantially higher correlation with human judgements than all existing metrics on the WMT 2017 to-English sentence level dataset.",,,,ACL
270,2019,Joint Effects of Context and User History for Predicting Online Conversation Re-entries,"Xingshan Zeng, Jing Li, Lu Wang, Kam-Fai Wong","As the online world continues its exponential growth, interpersonal communication has come to play an increasingly central role in opinion formation and change. In order to help users better engage with each other online, we study a challenging problem of re-entry prediction foreseeing whether a user will come back to a conversation they once participated in. We hypothesize that both the context of the ongoing conversations and the users’ previous chatting history will affect their continued interests in future engagement. Specifically, we propose a neural framework with three main layers, each modeling context, user history, and interactions between them, to explore how the conversation context and user chatting history jointly result in their re-entry behavior. We experiment with two large-scale datasets collected from Twitter and Reddit. Results show that our proposed framework with bi-attention achieves an F1 score of 61.1 on Twitter conversations, outperforming the state-of-the-art methods from previous work.",,,,ACL
271,2019,CONAN - COunter NArratives through Nichesourcing: a Multilingual Dataset of Responses to Fight Online Hate Speech,"Yi-Ling Chung, Elizaveta Kuzmenko, Serra Sinem Tekiroglu, Marco Guerini","Although there is an unprecedented effort to provide adequate responses in terms of laws and policies to hate content on social media platforms, dealing with hatred online is still a tough problem. Tackling hate speech in the standard way of content deletion or user suspension may be charged with censorship and overblocking. One alternate strategy, that has received little attention so far by the research community, is to actually oppose hate content with counter-narratives (i.e. informed textual responses). In this paper, we describe the creation of the first large-scale, multilingual, expert-based dataset of hate-speech/counter-narrative pairs. This dataset has been built with the effort of more than 100 operators from three different NGOs that applied their training and expertise to the task. Together with the collected data we also provide additional annotations about expert demographics, hate and response type, and data augmentation through translation and paraphrasing. Finally, we provide initial experiments to assess the quality of our data.",,,,ACL
272,2019,Categorizing and Inferring the Relationship between the Text and Image of Twitter Posts,"Alakananda Vempala, Daniel Preoţiuc-Pietro","Text in social media posts is frequently accompanied by images in order to provide content, supply context, or to express feelings. This paper studies how the meaning of the entire tweet is composed through the relationship between its textual content and its image. We build and release a data set of image tweets annotated with four classes which express whether the text or the image provides additional information to the other modality. We show that by combining the text and image information, we can build a machine learning approach that accurately distinguishes between the relationship types. Further, we derive insights into how these relationships are materialized through text and image content analysis and how they are impacted by user demographic traits. These methods can be used in several downstream applications including pre-training image tagging models, collecting distantly supervised data for image captioning, and can be directly used in end-user applications to optimize screen estate.",,,,ACL
273,2019,Who Sides with Whom? Towards Computational Construction of Discourse Networks for Political Debates,"Sebastian Padó, Andre Blessing, Nico Blokker, Erenay Dayanik, Sebastian Haunss","Understanding the structures of political debates (which actors make what claims) is essential for understanding democratic political decision making. The vision of computational construction of such discourse networks from newspaper reports brings together political science and natural language processing. This paper presents three contributions towards this goal: (a) a requirements analysis, linking the task to knowledge base population; (b) an annotated pilot corpus of migration claims based on German newspaper reports; (c) initial modeling results.",,,,ACL
274,2019,Analyzing Linguistic Differences between Owner and Staff Attributed Tweets,"Daniel Preoţiuc-Pietro, Rita Devlin Marier","Research on social media has to date assumed that all posts from an account are authored by the same person. In this study, we challenge this assumption and study the linguistic differences between posts signed by the account owner or attributed to their staff. We introduce a novel data set of tweets posted by U.S. politicians who self-reported their tweets using a signature. We analyze the linguistic topics and style features that distinguish the two types of tweets. Predictive results show that we are able to predict owner and staff attributed tweets with good accuracy, even when not using any training data from that account.",,,,ACL
275,2019,Exploring Author Context for Detecting Intended vs Perceived Sarcasm,"Silviu Oprea, Walid Magdy","We investigate the impact of using author context on textual sarcasm detection. We define author context as the embedded representation of their historical posts on Twitter and suggest neural models that extract these representations. We experiment with two tweet datasets, one labelled manually for sarcasm, and the other via tag-based distant supervision. We achieve state-of-the-art performance on the second dataset, but not on the one labelled manually, indicating a difference between intended sarcasm, captured by distant supervision, and perceived sarcasm, captured by manual labelling.",,,,ACL
276,2019,Open Domain Event Extraction Using Neural Latent Variable Models,"Xiao Liu, Heyan Huang, Yue Zhang","We consider open domain event extraction, the task of extracting unconstraint types of events from news clusters. A novel latent variable neural model is constructed, which is scalable to very large corpus. A dataset is collected and manually annotated, with task-specific evaluation metrics being designed. Results show that the proposed unsupervised model gives better performance compared to the state-of-the-art method for event schema induction.",,,,ACL
277,2019,Multi-Level Matching and Aggregation Network for Few-Shot Relation Classification,"Zhi-Xiu Ye, Zhen-Hua Ling","This paper presents a multi-level matching and aggregation network (MLMAN) for few-shot relation classification. Previous studies on this topic adopt prototypical networks, which calculate the embedding vector of a query instance and the prototype vector of the support set for each relation candidate independently. On the contrary, our proposed MLMAN model encodes the query instance and each support set in an interactive way by considering their matching information at both local and instance levels. The final class prototype for each support set is obtained by attentive aggregation over the representations of support instances, where the weights are calculated using the query instance. Experimental results demonstrate the effectiveness of our proposed methods, which achieve a new state-of-the-art performance on the FewRel dataset.",,,,ACL
278,2019,Quantifying Similarity between Relations with Fact Distribution,"Weize Chen, Hao Zhu, Xu Han, Zhiyuan Liu, Maosong Sun","We introduce a conceptually simple and effective method to quantify the similarity between relations in knowledge bases. Specifically, our approach is based on the divergence between the conditional probability distributions over entity pairs. In this paper, these distributions are parameterized by a very simple neural network. Although computing the exact similarity is in-tractable, we provide a sampling-based method to get a good approximation. We empirically show the outputs of our approach significantly correlate with human judgments. By applying our method to various tasks, we also find that (1) our approach could effectively detect redundant relations extracted by open information extraction (Open IE) models, that (2) even the most competitive models for relational classification still make mistakes among very similar relations, and that (3) our approach could be incorporated into negative sampling and softmax classification to alleviate these mistakes.",,,,ACL
279,2019,Matching the Blanks: Distributional Similarity for Relation Learning,"Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, Tom Kwiatkowski","General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris’ distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task’s training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED",,,,ACL
280,2019,Fine-Grained Temporal Relation Extraction,"Siddharth Vashishtha, Benjamin Van Durme, Aaron Steven White","We present a novel semantic framework for modeling temporal relations and event durations that maps pairs of events to real-valued scales. We use this framework to construct the largest temporal relations dataset to date, covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to train models for jointly predicting fine-grained temporal relations and event durations. We report strong results on our data and show the efficacy of a transfer-learning approach for predicting categorical relations.",,,,ACL
281,2019,FIESTA: Fast IdEntification of State-of-The-Art models using adaptive bandit algorithms,"Henry Moss, Andrew Moore, David Leslie, Paul Rayson","We present FIESTA, a model selection approach that significantly reduces the computational resources required to reliably identify state-of-the-art performance from large collections of candidate models. Despite being known to produce unreliable comparisons, it is still common practice to compare model evaluations based on single choices of random seeds. We show that reliable model selection also requires evaluations based on multiple train-test splits (contrary to common practice in many shared tasks). Using bandit theory from the statistics literature, we are able to adaptively determine appropriate numbers of data splits and random seeds used to evaluate each model, focusing computational resources on the evaluation of promising models whilst avoiding wasting evaluations on models with lower performance. Furthermore, our user-friendly Python implementation produces confidence guarantees of correctly selecting the optimal model. We evaluate our algorithms by selecting between 8 target-dependent sentiment analysis methods using dramatically fewer model evaluations than current model selection approaches.",,,,ACL
282,2019,Is Attention Interpretable?,"Sofia Serrano, Noah A. Smith","Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components’ representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components’ overall importance to a model, it is by no means a fail-safe indicator.",,,,ACL
283,2019,Correlating Neural and Symbolic Representations of Language,"Grzegorz Chrupała, Afra Alishahi","Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in NLP. Here we present two methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees. We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics, and show that they exhibit the expected pattern of results. We then our methods to correlate neural representations of English sentences with their constituency parse trees.",,,,ACL
284,2019,Interpretable Neural Predictions with Differentiable Binary Variables,"Joost Bastings, Wilker Aziz, Ivan Titov","The success of neural networks comes hand in hand with a desire for more interpretability. We focus on text classifiers and make them more interpretable by having them provide a justification–a rationale–for their predictions. We approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classifier that learns from the words in the rationale alone. Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as L0 regularisation. We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE. In our formulation, we can tractably compute the expected value of penalties such as L0, which allows us to directly optimise the model towards a pre-specified text selection rate. We show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms.",,,,ACL
285,2019,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le","Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",,,,ACL
286,2019,Domain Adaptation of Neural Machine Translation by Lexicon Induction,"Junjie Hu, Mengzhou Xia, Graham Neubig, Jaime Carbonell","It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.",,,,ACL
287,2019,Reference Network for Neural Machine Translation,"Han Fu, Chenghao Liu, Jianling Sun","Neural Machine Translation (NMT) has achieved notable success in recent years. Such a framework usually generates translations in isolation. In contrast, human translators often refer to reference data, either rephrasing the intricate sentence fragments with common terms in source language, or just accessing to the golden translation directly. In this paper, we propose a Reference Network to incorporate referring process into translation decoding of NMT. To construct a reference book, an intuitive way is to store the detailed translation history with extra memory, which is computationally expensive. Instead, we employ Local Coordinates Coding (LCC) to obtain global context vectors containing monolingual and bilingual contextual information for NMT decoding. Experimental results on Chinese-English and English-German tasks demonstrate that our proposed model is effective in improving the translation quality with lightweight computation cost.",,,,ACL
288,2019,Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation,"Chenze Shao, Yang Feng, Jinchao Zhang, Fandong Meng, Xilin Chen","Non-Autoregressive Transformer (NAT) aims to accelerate the Transformer model through discarding the autoregressive mechanism and generating target words independently, which fails to exploit the target sequential information. Over-translation and under-translation errors often occur for the above reason, especially in the long sentence translation scenario. In this paper, we propose two approaches to retrieve the target sequential information for NAT to enhance its translation ability while preserving the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. Experimental results on three translation tasks show that the Reinforce-NAT surpasses the baseline NAT system by a significant margin on BLEU without decelerating the decoding speed and the FS-decoder achieves comparable translation performance to the autoregressive Transformer with considerable speedup.",,,,ACL
289,2019,STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework,"Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu","Simultaneous translation, which translates sentences before they are finished, is use- ful in many scenarios but is notoriously dif- ficult due to word-order differences. While the conventional seq-to-seq framework is only suitable for full-sentence translation, we pro- pose a novel prefix-to-prefix framework for si- multaneous translation that implicitly learns to anticipate in a single translation model. Within this framework, we present a very sim- ple yet surprisingly effective “wait-k” policy trained to generate the target sentence concur- rently with the source sentence, but always k words behind. Experiments show our strat- egy achieves low latency and reasonable qual- ity (compared to full-sentence translation) on 4 directions: zh↔en and de↔en.",,,,ACL
290,2019,Look Harder: A Neural Machine Translation Model with Hard Attention,"Sathish Reddy Indurthi, Insoo Chung, Sangha Kim","Soft-attention based Neural Machine Translation (NMT) models have achieved promising results on several translation tasks. These models attend all the words in the source sequence for each target token, which makes them ineffective for long sequence translation. In this work, we propose a hard-attention based NMT model which selects a subset of source tokens for each target token to effectively handle long sequence translation. Due to the discrete nature of the hard-attention mechanism, we design a reinforcement learning algorithm coupled with reward shaping strategy to efficiently train it. Experimental results show that the proposed model performs better on long sequences and thereby achieves significant BLEU score improvement on English-German (EN-DE) and English-French (ENFR) translation tasks compared to the soft attention based NMT.",,,,ACL
291,2019,Robust Neural Machine Translation with Joint Textual and Phonetic Embedding,"Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, Zhongjun He","Neural machine translation (NMT) is notoriously sensitive to noises, but noises are almost inevitable in practice. One special kind of noise is the homophone noise, where words are replaced by other words with similar pronunciations. We propose to improve the robustness of NMT to homophone noises by 1) jointly embedding both textual and phonetic information of source sentences, and 2) augmenting the training dataset with homophone noises. Interestingly, to achieve better translation quality and more robustness, we found that most (though not all) weights should be put on the phonetic rather than textual information. Experiments show that our method not only significantly improves the robustness of NMT to homophone noises, but also surprisingly improves the translation quality on some clean test sets.",,,,ACL
292,2019,A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning,"Gonçalo M. Correia, André F. T. Martins","Automatic post-editing (APE) seeks to automatically refine the output of a black-box machine translation (MT) system through human post-edits. APE systems are usually trained by complementing human post-edited data with large, artificial data generated through back-translations, a time-consuming process often no easier than training a MT system from scratch. in this paper, we propose an alternative where we fine-tune pre-trained BERT models on both the encoder and decoder of an APE system, exploring several parameter sharing strategies. By only training on a dataset of 23K sentences for 3 hours on a single GPU we obtain results that are competitive with systems that were trained on 5M artificial sentences. When we add this artificial data our method obtains state-of-the-art results.",,,,ACL
293,2019,Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation,"Nima Pourdamghani, Nada Aldarrab, Marjan Ghazvininejad, Kevin Knight, Jonathan May","Given a rough, word-by-word gloss of a source language sentence, target language natives can uncover the latent, fully-fluent rendering of the translation. In this work we explore this intuition by breaking translation into a two step process: generating a rough gloss by means of a dictionary and then ‘translating’ the resulting pseudo-translation, or ‘Translationese’ into a fully fluent translation. We build our Translationese decoder once from a mish-mash of parallel data that has the target language in common and then can build dictionaries on demand using unsupervised techniques, resulting in rapidly generated unsupervised neural MT systems for many source languages. We apply this process to 14 test languages, obtaining better or comparable translation results on high-resource languages than previously published unsupervised MT studies, and obtaining good quality results for low-resource languages that have never been used in an unsupervised MT scenario.",,,,ACL
294,2019,Training Neural Machine Translation to Apply Terminology Constraints,"Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan","This paper proposes a novel method to inject custom terminology into neural machine translation at run time. Previous works have mainly proposed modifications to the decoding algorithm in order to constrain the output to include run-time-provided target terms. While being effective, these constrained decoding methods add, however, significant computational overhead to the inference step, and, as we show in this paper, can be brittle when tested in realistic conditions. In this paper we approach the problem by training a neural MT system to learn how to use custom terminology when provided with the input. Comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding, but is also as fast as constraint-free decoding.",,,,ACL
295,2019,Leveraging Local and Global Patterns for Self-Attention Networks,"Mingzhou Xu, Derek F. Wong, Baosong Yang, Yue Zhang, Lidia S. Chao","Self-attention networks have received increasing research attention. By default, the hidden states of each word are hierarchically calculated by attending to all words in the sentence, which assembles global information. However, several studies pointed out that taking all signals into account may lead to overlooking neighboring information (e.g. phrase pattern). To address this argument, we propose a hybrid attention mechanism to dynamically leverage both of the local and global information. Specifically, our approach uses a gating scalar for integrating both sources of the information, which is also convenient for quantifying their contributions. Experiments on various neural machine translation tasks demonstrate the effectiveness of the proposed method. The extensive analyses verify that the two types of contexts are complementary to each other, and our method gives highly effective improvements in their integration.",,,,ACL
296,2019,Sentence-Level Agreement for Neural Machine Translation,"Mingming Yang, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita","The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references. In NMT, there is a natural correspondence between the source sentence and the target sentence. However, this relationship has only been represented using the entire neural network and the training objective is computed in word-level. In this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance.",,,,ACL
297,2019,Multilingual Unsupervised NMT using Shared Encoder and Language-Specific Decoders,"Sukanta Sen, Kamal Kumar Gupta, Asif Ekbal, Pushpak Bhattacharyya","In this paper, we propose a multilingual unsupervised NMT scheme which jointly trains multiple languages with a shared encoder and multiple decoders. Our approach is based on denoising autoencoding of each language and back-translating between English and multiple non-English languages. This results in a universal encoder which can encode any language participating in training into an inter-lingual representation, and language-specific decoders. Our experiments using only monolingual corpora show that multilingual unsupervised model performs better than the separately trained bilingual models achieving improvement of up to 1.48 BLEU points on WMT test sets. We also observe that even if we do not train the network for all possible translation directions, the network is still able to translate in a many-to-many fashion leveraging encoder’s ability to generate interlingual representation.",,,,ACL
298,2019,Lattice-Based Transformer Encoder for Neural Machine Translation,"Fengshun Xiao, Jiangtong Li, Hai Zhao, Rui Wang, Kehai Chen","Neural machine translation (NMT) takes deterministic sequences for source representations. However, either word-level or subword-level segmentations have multiple choices to split a source sequence with different word segmentors or different subword vocabulary sizes. We hypothesize that the diversity in segmentations may affect the NMT performance. To integrate different segmentations with the state-of-the-art NMT model, Transformer, we propose lattice-based encoders to explore effective word or subword representation in an automatic way during training. We propose two methods: 1) lattice positional encoding and 2) lattice-aware self-attention. These two methods can be used together and show complementary to each other to further improve translation performance. Experiment results show superiorities of lattice-based encoders in word-level and subword-level representations over conventional Transformer encoder.",,,,ACL
299,2019,Multi-Source Cross-Lingual Model Transfer: Learning What to Share,"Xilun Chen, Ahmed Hassan Awadallah, Hany Hassan, Wei Wang, Claire Cardie","Modern NLP applications have enjoyed a great boost utilizing neural networks models. Such deep neural models, however, are not applicable to most human languages due to the lack of annotated training data for various NLP tasks. Cross-lingual transfer learning (CLTL) is a viable method for building NLP models for a low-resource target language by leveraging labeled data from other (source) languages. In this work, we focus on the multilingual transfer setting where training data in multiple source languages is leveraged to further boost target language performance. Unlike most existing methods that rely only on language-invariant features for CLTL, our approach coherently utilizes both language-invariant and language-specific features at instance level. Our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target language and each individual source language. This enables our model to learn effectively what to share between various languages in the multilingual setup. Moreover, when coupled with unsupervised multilingual embeddings, our model can operate in a zero-resource setting where neither target language training data nor cross-lingual resources are available. Our model achieves significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classification and sequence tagging tasks including a large-scale industry dataset.",,,,ACL
300,2019,Unsupervised Multilingual Word Embedding with Limited Resources using Neural Language Models,"Takashi Wada, Tomoharu Iwata, Yuji Matsumoto","Recently, a variety of unsupervised methods have been proposed that map pre-trained word embeddings of different languages into the same space without any parallel data. These methods aim to find a linear transformation based on the assumption that monolingual word embeddings are approximately isomorphic between languages. However, it has been demonstrated that this assumption holds true only on specific conditions, and with limited resources, the performance of these methods decreases drastically. To overcome this problem, we propose a new unsupervised multilingual embedding method that does not rely on such assumption and performs well under resource-poor scenarios, namely when only a small amount of monolingual data (i.e., 50k sentences) are available, or when the domains of monolingual data are different across languages. Our proposed model, which we call ‘Multilingual Neural Language Models’, shares some of the network parameters among multiple languages, and encodes sentences of multiple languages into the same space. The model jointly learns word embeddings of different languages in the same space, and generates multilingual embeddings without any parallel data or pre-training. Our experiments on word alignment tasks have demonstrated that, on the low-resource condition, our model substantially outperforms existing unsupervised and even supervised methods trained with 500 bilingual pairs of words. Our model also outperforms unsupervised methods given different-domain corpora across languages. Our code is publicly available.",,,,ACL
301,2019,Choosing Transfer Languages for Cross-Lingual Learning,"Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang","Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (NLP) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method.",,,,ACL
302,2019,CogNet: A Large-Scale Cognate Database,"Khuyagbaatar Batsuren, Gabor Bella, Fausto Giunchiglia","This paper introduces CogNet, a new, large-scale lexical database that provides cognates -words of common origin and meaning- across languages. The database currently contains 3.1 million cognate pairs across 338 languages using 35 writing systems. The paper also describes the automated method by which cognates were computed from publicly available wordnets, with an accuracy evaluated to 94%. Finally, it presents statistics about the cognate data and some initial insights into it, hinting at a possible future exploitation of the resource by various fields of lingustics.",,,,ACL
303,2019,Neural Decipherment via Minimum-Cost Flow: From Ugaritic to Linear B,"Jiaming Luo, Yuan Cao, Regina Barzilay","In this paper we propose a novel neural approach for automatic decipherment of lost languages. To compensate for the lack of strong supervision signal, our model design is informed by patterns in language change documented in historical linguistics. The model utilizes an expressive sequence-to-sequence model to capture character-level correspondences between cognates. To effectively train the model in unsupervised manner, we innovate the training procedure by formalizing it as a minimum-cost flow problem. When applied to decipherment of Ugaritic, we achieve 5% absolute improvement over state-of-the-art results. We also report first automatic results in deciphering Linear B, a syllabic language related to ancient Greek, where our model correctly translates 67.3% of cognates.",,,,ACL
304,2019,Cross-lingual Knowledge Graph Alignment via Graph Matching Neural Network,"Kun Xu, Liwei Wang, Mo Yu, Yansong Feng, Yan Song","Previous cross-lingual knowledge graph (KG) alignment studies rely on entity embeddings derived only from monolingual KG structural information, which may fail at matching entities that have different facts in two KGs. In this paper, we introduce the topic entity graph, a local sub-graph of an entity, to represent entities with their contextual information in KG. From this view, the KB-alignment task can be formulated as a graph matching problem; and we further propose a graph-attention based solution, which first matches all entities in two topic entity graphs, and then jointly model the local matching information to derive a graph-level matching vector. Experiments show that our model outperforms previous state-of-the-art methods by a large margin.",,,,ACL
305,2019,Zero-Shot Cross-Lingual Abstractive Sentence Summarization through Teaching Generation and Attention,"Xiangyu Duan, Mingming Yin, Min Zhang, Boxing Chen, Weihua Luo","Abstractive Sentence Summarization (ASSUM) targets at grasping the core idea of the source sentence and presenting it as the summary. It is extensively studied using statistical models or neural models based on the large-scale monolingual source-summary parallel corpus. But there is no cross-lingual parallel corpus, whose source sentence language is different to the summary language, to directly train a cross-lingual ASSUM system. We propose to solve this zero-shot problem by using resource-rich monolingual ASSUM system to teach zero-shot cross-lingual ASSUM system on both summary word generation and attention. This teaching process is along with a back-translation process which simulates source-summary pairs. Experiments on cross-lingual ASSUM task show that our proposed method is significantly better than pipeline baselines and previous works, and greatly enhances the cross-lingual performances closer to the monolingual performances.",,,,ACL
306,2019,Improving Low-Resource Cross-lingual Document Retrieval by Reranking with Deep Bilingual Representations,"Rui Zhang, Caitlin Westerfield, Sungrok Shim, Garrett Bingham, Alexander Fabbri","In this paper, we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations. We match queries and documents in both source and target languages with four components, each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input. By including query likelihood scores as extra features, our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs. Due to the shared cross-lingual word embedding space, the model can also be directly applied to another language pair without any training label. Experimental results on the Material dataset show that our model outperforms the competitive translation-based baselines on English-Swahili, English-Tagalog, and English-Somali cross-lingual information retrieval tasks.",,,,ACL
307,2019,Are Girls Neko or Shōjo? Cross-Lingual Alignment of Non-Isomorphic Embeddings with Iterative Normalization,"Mozhi Zhang, Keyulu Xu, Ken-ichi Kawarabayashi, Stefanie Jegelka, Jordan Boyd-Graber","Cross-lingual word embeddings (CLWE) underlie many multilingual natural language processing systems, often through orthogonal transformations of pre-trained monolingual embeddings. However, orthogonal mapping only works on language pairs whose embeddings are naturally isomorphic. For non-isomorphic pairs, our method (Iterative Normalization) transforms monolingual embeddings to make orthogonal alignment easier by simultaneously enforcing that (1) individual word vectors are unit length, and (2) each language’s average vector is zero. Iterative Normalization consistently improves word translation accuracy of three CLWE methods, with the largest improvement observed on English-Japanese (from 2% to 44% test accuracy).",,,,ACL
308,2019,MAAM: A Morphology-Aware Alignment Model for Unsupervised Bilingual Lexicon Induction,"Pengcheng Yang, Fuli Luo, Peng Chen, Tianyu Liu, Xu Sun","The task of unsupervised bilingual lexicon induction (UBLI) aims to induce word translations from monolingual corpora in two languages. Previous work has shown that morphological variation is an intractable challenge for the UBLI task, where the induced translation in failure case is usually morphologically related to the correct translation. To tackle this challenge, we propose a morphology-aware alignment model for the UBLI task. The proposed model aims to alleviate the adverse effect of morphological variation by introducing grammatical information learned by the pre-trained denoising language model. Results show that our approach can substantially outperform several state-of-the-art unsupervised systems, and even achieves competitive performance compared to supervised methods.",,,,ACL
309,2019,Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings,"Mikel Artetxe, Holger Schwenk","Machine translation is highly sensitive to the size and quality of the training data, which has led to an increasing interest in collecting and filtering large parallel corpora. In this paper, we propose a new method for this task based on multilingual sentence embeddings. In contrast to previous approaches, which rely on nearest neighbor retrieval with a hard threshold over cosine similarity, our proposed method accounts for the scale inconsistencies of this measure, considering the margin between a given sentence pair and its closest candidates instead. Our experiments show large improvements over existing methods. We outperform the best published results on the BUCC mining task and the UN reconstruction task by more than 10 F1 and 30 precision points, respectively. Filtering the English-German ParaCrawl corpus with our approach, we obtain 31.2 BLEU points on newstest2014, an improvement of more than one point over the best official filtered version.",,,,ACL
310,2019,JW300: A Wide-Coverage Parallel Corpus for Low-Resource Languages,"Željko Agić, Ivan Vulić","Viable cross-lingual transfer critically depends on the availability of parallel texts. Shortage of such resources imposes a development and evaluation bottleneck in multilingual processing. We introduce JW300, a parallel corpus of over 300 languages with around 100 thousand parallel sentences per language pair on average. In this paper, we present the resource and showcase its utility in experiments with cross-lingual word embedding induction and multi-source part-of-speech projection.",,,,ACL
311,2019,Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of Invertible Projections,"Junxian He, Zhisong Zhang, Taylor Berg-Kirkpatrick, Graham Neubig","Cross-lingual transfer is an effective way to build syntactic analysis tools in low-resource languages. However, transfer is difficult when transferring to typologically distant languages, especially when neither annotated target data nor parallel corpora are available. In this paper, we focus on methods for cross-lingual transfer to distant languages and propose to learn a generative model with a structured prior that utilizes labeled source data and unlabeled target data jointly. The parameters of source model and target model are softly shared through a regularized log likelihood objective. An invertible projection is employed to learn a new interlingual latent embedding space that compensates for imperfect cross-lingual word embedding input. We evaluate our method on two syntactic tasks: part-of-speech (POS) tagging and dependency parsing. On the Universal Dependency Treebanks, we use English as the only source corpus and transfer to a wide range of target languages. On the 10 languages in this dataset that are distant from English, our method yields an average of 5.2% absolute improvement on POS tagging and 8.3% absolute improvement on dependency parsing over a direct transfer method using state-of-the-art discriminative models.",,,,ACL
312,2019,Unsupervised Joint Training of Bilingual Word Embeddings,"Benjamin Marie, Atsushi Fujita","State-of-the-art methods for unsupervised bilingual word embeddings (BWE) train a mapping function that maps pre-trained monolingual word embeddings into a bilingual space. Despite its remarkable results, unsupervised mapping is also well-known to be limited by the original dissimilarity between the word embedding spaces to be mapped. In this work, we propose a new approach that trains unsupervised BWE jointly on synthetic parallel data generated through unsupervised machine translation. We demonstrate that existing algorithms that jointly train BWE are very robust to noisy training data and show that unsupervised BWE jointly trained significantly outperform unsupervised mapped BWE in several cross-lingual NLP tasks.",,,,ACL
313,2019,Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings,"Matthew Le, Stephen Roller, Laetitia Papaxanthos, Douwe Kiela, Maximilian Nickel","We consider the task of inferring “is-a” relationships from large text corpora. For this purpose, we propose a new method combining hyperbolic embeddings and Hearst patterns. This approach allows us to set appropriate constraints for inferring concept hierarchies from distributional contexts while also being able to predict missing “is-a”-relationships and to correct wrong extractions. Moreover – and in contrast with other methods – the hierarchical nature of hyperbolic space allows us to learn highly efficient representations and to improve the taxonomic consistency of the inferred hierarchies. Experimentally, we show that our approach achieves state-of-the-art performance on several commonly-used benchmarks.",,,,ACL
314,2019,Is Word Segmentation Necessary for Deep Learning of Chinese Representations?,"Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan","Segmenting a chunk of text into words is usually the first step of processing Chinese text, but its necessity has rarely been explored. In this paper, we ask the fundamental question of whether Chinese word segmentation (CWS) is necessary for deep learning-based Chinese Natural Language Processing. We benchmark neural word-based models which rely on word segmentation against neural char-based models which do not involve word segmentation in four end-to-end NLP benchmark tasks: language modeling, machine translation, sentence matching/paraphrase and text classification. Through direct comparisons between these two types of models, we find that char-based models consistently outperform word-based models. Based on these observations, we conduct comprehensive experiments to study why word-based models underperform char-based models in these deep learning-based NLP tasks. We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting. We hope this paper could encourage researchers in the community to rethink the necessity of word segmentation in deep learning-based Chinese Natural Language Processing.",,,,ACL
315,2019,Towards Understanding Linear Word Analogies,"Kawin Ethayarajh, David Duvenaud, Graeme Hirst","A surprising property of word vectors is that word analogies can often be solved with vector arithmetic. However, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, justifying its use in capturing word dissimilarity.",,,,ACL
316,2019,On the Compositionality Prediction of Noun Phrases using Poincaré Embeddings,"Abhik Jana, Dima Puzyrev, Alexander Panchenko, Pawan Goyal, Chris Biemann","The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations. Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models. We introduce a novel technique to blend hierarchical information with distributional information for predicting compositionality. In particular, we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced Poincaré embeddings in addition to the distributional information to detect compositionality for noun phrases. Using a weighted average of the distributional similarity and a Poincaré similarity function, we obtain consistent and substantial, statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only. Unlike traditional approaches that solely use an unsupervised setting, we have also framed the problem as a supervised task, obtaining comparable improvements. Further, we publicly release our Poincaré embeddings, which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus.",,,,ACL
317,2019,Robust Representation Learning of Biomedical Names,"Minh C. Phan, Aixin Sun, Yi Tay","Biomedical concepts are often mentioned in medical documents under different name variations (synonyms). This mismatch between surface forms is problematic, resulting in difficulties pertaining to learning effective representations. Consequently, this has tremendous implications such as rendering downstream applications inefficacious and/or potentially unreliable. This paper proposes a new framework for learning robust representations of biomedical names and terms. The idea behind our approach is to consider and encode contextual meaning, conceptual meaning, and the similarity between synonyms during the representation learning process. Via extensive experiments, we show that our proposed method outperforms other baselines on a battery of retrieval, similarity and relatedness benchmarks. Moreover, our proposed method is also able to compute meaningful representations for unseen names, resulting in high practical utility in real-world applications.",,,,ACL
318,2019,Relational Word Embeddings,"Jose Camacho-Collados, Luis Espinosa Anke, Steven Schockaert","While word embeddings have been shown to implicitly encode various forms of attributional knowledge, the extent to which they capture relational information is far more limited. In previous work, this limitation has been addressed by incorporating relational knowledge from external knowledge bases when learning the word embedding. Such strategies may not be optimal, however, as they are limited by the coverage of available resources and conflate similarity with other forms of relatedness. As an alternative, in this paper we propose to encode relational knowledge in a separate word embedding, which is aimed to be complementary to a given standard word embedding. This relational word embedding is still learned from co-occurrence statistics, and can thus be used even when no external knowledge base is available. Our analysis shows that relational word vectors do indeed capture information that is complementary to what is encoded in standard word embeddings.",,,,ACL
319,2019,Unraveling Antonym’s Word Vectors through a Siamese-like Network,"Mathias Etcheverry, Dina Wonsever","Discriminating antonyms and synonyms is an important NLP task that has the difficulty that both, antonyms and synonyms, contains similar distributional information. Consequently, pairs of antonyms and synonyms may have similar word vectors. We present an approach to unravel antonymy and synonymy from word vectors based on a siamese network inspired approach. The model consists of a two-phase training of the same base network: a pre-training phase according to a siamese model supervised by synonyms and a training phase on antonyms through a siamese-like model that supports the antitransitivity present in antonymy. The approach makes use of the claim that the antonyms in common of a word tend to be synonyms. We show that our approach outperforms distributional and pattern-based approaches, relaying on a simple feed forward network as base network of the training phases.",,,,ACL
320,2019,Incorporating Syntactic and Semantic Information in Word Embeddings using Graph Convolutional Networks,"Shikhar Vashishth, Manik Bhandari, Prateek Yadav, Piyush Rai, Chiranjib Bhattacharyya","Word embeddings have been widely adopted across several NLP applications. Most existing word embedding methods utilize sequential context of a word to learn its embedding. While there have been some attempts at utilizing syntactic context of a word, such methods result in an explosion of the vocabulary size. In this paper, we overcome this problem by proposing SynGCN, a flexible Graph Convolution based method for learning word embeddings. SynGCN utilizes the dependency context of a word without increasing the vocabulary size. Word embeddings learned by SynGCN outperform existing methods on various intrinsic and extrinsic tasks and provide an advantage when used with ELMo. We also propose SemGCN, an effective framework for incorporating diverse semantic knowledge for further enhancing learned word representations. We make the source code of both models available to encourage reproducible research.",,,,ACL
321,2019,Word and Document Embedding with vMF-Mixture Priors on Context Word Vectors,"Shoaib Jameel, Steven Schockaert","Word embedding models typically learn two types of vectors: target word vectors and context word vectors. These vectors are normally learned such that they are predictive of some word co-occurrence statistic, but they are otherwise unconstrained. However, the words from a given language can be organized in various natural groupings, such as syntactic word classes (e.g. nouns, adjectives, verbs) and semantic themes (e.g. sports, politics, sentiment). Our hypothesis in this paper is that embedding models can be improved by explicitly imposing a cluster structure on the set of context word vectors. To this end, our model relies on the assumption that context word vectors are drawn from a mixture of von Mises-Fisher (vMF) distributions, where the parameters of this mixture distribution are jointly optimized with the word vectors. We show that this results in word vectors which are qualitatively different from those obtained with existing word embedding models. We furthermore show that our embedding model can also be used to learn high-quality document representations.",,,,ACL
322,2019,Delta Embedding Learning,"Xiao Zhang, Ji Wu, Dejing Dou","Unsupervised word embeddings have become a popular approach of word representation in NLP tasks. However there are limitations to the semantics represented by unsupervised embeddings, and inadequate fine-tuning of embeddings can lead to suboptimal performance. We propose a novel learning technique called Delta Embedding Learning, which can be applied to general NLP tasks to improve performance by optimized tuning of the word embeddings. A structured regularization is applied to the embeddings to ensure they are tuned in an incremental way. As a result, the tuned word embeddings become better word representations by absorbing semantic information from supervision without “forgetting.” We apply the method to various NLP tasks and see a consistent improvement in performance. Evaluation also confirms the tuned word embeddings have better semantic properties.",,,,ACL
323,2019,Annotation and Automatic Classification of Aspectual Categories,"Markus Egg, Helena Prepens, Will Roberts","We present the first annotated resource for the aspectual classification of German verb tokens in their clausal context. We use aspectual features compatible with the plurality of aspectual classifications in previous work and treat aspectual ambiguity systematically. We evaluate our corpus by using it to train supervised classifiers to automatically assign aspectual categories to verbs in context, permitting favourable comparisons to previous work.",,,,ACL
324,2019,Putting Words in Context: LSTM Language Models and Lexical Ambiguity,"Laura Aina, Kristina Gulordava, Gemma Boleda","In neural network models of language, words are commonly represented using context-invariant representations (word embeddings) which are then put in context in the hidden layers. Since words are often ambiguous, representing the contextually relevant information is not trivial. We investigate how an LSTM language model deals with lexical ambiguity in English, designing a method to probe its hidden representations for lexical and contextual information about words. We find that both types of information are represented to a large extent, but also that there is room for improvement for contextual information.",,,,ACL
325,2019,Making Fast Graph-based Algorithms with Graph Metric Embeddings,"Andrey Kutuzov, Mohammad Dorgham, Oleksiy Oliynyk, Chris Biemann, Alexander Panchenko","Graph measures, such as node distances, are inefficient to compute. We explore dense vector representations as an effective way to approximate the same information. We introduce a simple yet efficient and effective approach for learning graph embeddings. Instead of directly operating on the graph structure, our method takes structural measures of pairwise node similarities into account and learns dense node representations reflecting user-defined graph distance measures, such as e.g. the shortest path distance or distance measures that take information beyond the graph structure into account. We demonstrate a speed-up of several orders of magnitude when predicting word similarity by vector operations on our embeddings as opposed to directly computing the respective path-based measures, while outperforming various other graph embeddings on semantic similarity and word sense disambiguation tasks.",,,,ACL
326,2019,Embedding Imputation with Grounded Language Information,"Ziyi Yang, Chenguang Zhu, Vin Sachidananda, Eric Darve","Due to the ubiquitous use of embeddings as input representations for a wide range of natural language tasks, imputation of embeddings for rare and unseen words is a critical problem in language processing. Embedding imputation involves learning representations for rare or unseen words during the training of an embedding model, often in a post-hoc manner. In this paper, we propose an approach for embedding imputation which uses grounded information in the form of a knowledge graph. This is in contrast to existing approaches which typically make use of vector space properties or subword information. We propose an online method to construct a graph from grounded information and design an algorithm to map from the resulting graphical structure to the space of the pre-trained embeddings. Finally, we evaluate our approach on a range of rare and unseen word tasks across various domains and show that our model can learn better representations. For example, on the Card-660 task our method improves Pearson’s and Spearman’s correlation coefficients upon the state-of-the-art by 11% and 17.8% respectively using GloVe embeddings.",,,,ACL
327,2019,The Effectiveness of Simple Hybrid Systems for Hypernym Discovery,"William Held, Nizar Habash","Hypernymy modeling has largely been separated according to two paradigms, pattern-based methods and distributional methods. However, recent works utilizing a mix of these strategies have yielded state-of-the-art results. This paper evaluates the contribution of both paradigms to hybrid success by evaluating the benefits of hybrid treatment of baseline models from each paradigm. Even with a simple methodology for each individual system, utilizing a hybrid approach establishes new state-of-the-art results on two domain-specific English hypernym discovery tasks and outperforms all non-hybrid approaches in a general English hypernym discovery task.",,,,ACL
328,2019,BERT-based Lexical Substitution,"Wangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, Ming Zhou","Previous studies on lexical substitution tend to obtain substitute candidates by finding the target word’s synonyms from lexical resources (e.g., WordNet) and then rank the candidates based on its contexts. These approaches have two limitations: (1) They are likely to overlook good substitute candidates that are not the synonyms of the target words in the lexical resources; (2) They fail to take into account the substitution’s influence on the global context of the sentence. To address these issues, we propose an end-to-end BERT-based lexical substitution approach which can propose and validate substitute candidates without using any annotated data or manually curated resources. Our approach first applies dropout to the target word’s embedding for partially masking the word, allowing BERT to take balanced consideration of the target word’s semantics and contexts for proposing substitute candidates, and then validates the candidates based on their substitution’s influence on the global contextualized representation of the sentence. Experiments show our approach performs well in both proposing and ranking substitute candidates, achieving the state-of-the-art results in both LS07 and LS14 benchmarks.",,,,ACL
329,2019,Exploring Numeracy in Word Embeddings,"Aakanksha Naik, Abhilasha Ravichander, Carolyn Rose, Eduard Hovy","Word embeddings are now pervasive across NLP subfields as the de-facto method of forming text representataions. In this work, we show that existing embedding models are inadequate at constructing representations that capture salient aspects of mathematical meaning for numbers, which is important for language understanding. Numbers are ubiquitous and frequently appear in text. Inspired by cognitive studies on how humans perceive numbers, we develop an analysis framework to test how well word embeddings capture two essential properties of numbers: magnitude (e.g. 3<4) and numeration (e.g. 3=three). Our experiments reveal that most models capture an approximate notion of magnitude, but are inadequate at capturing numeration. We hope that our observations provide a starting point for the development of methods which better capture numeracy in NLP systems.",,,,ACL
330,2019,HighRES: Highlight-based Reference-less Evaluation of Summarization,"Hardy Hardy, Shashi Narayan, Andreas Vlachos","There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",,,,ACL
331,2019,EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing,"Yue Dong, Zichao Li, Mehdi Rezagholizadeh, Jackie Chi Kit Cheung","We present the first sentence simplification model that learns explicit edit operations (ADD, DELETE, and KEEP) via a neural programmer-interpreter approach. Most current neural sentence simplification systems are variants of sequence-to-sequence models adopted from machine translation. These methods learn to simplify sentences as a byproduct of the fact that they are trained on complex-simple sentence pairs. By contrast, our neural programmer-interpreter is directly trained to predict explicit edit operations on targeted parts of the input sentence, resembling the way that humans perform simplification and revision. Our model outperforms previous state-of-the-art neural sentence simplification models (without external knowledge) by large margins on three benchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89 WikiSmall, +1.41 Newsela), and is judged by humans to produce overall better and simpler output sentences.",,,,ACL
332,2019,Decomposable Neural Paraphrase Generation,"Zichao Li, Xin Jiang, Lifeng Shang, Qun Liu","Paraphrasing exists at different granularity levels, such as lexical level, phrasal level and sentential level. This paper presents Decomposable Neural Paraphrase Generator (DNPG), a Transformer-based model that can learn and generate paraphrases of a sentence at different levels of granularity in a disentangled way. Specifically, the model is composed of multiple encoders and decoders with different structures, each of which corresponds to a specific granularity. The empirical study shows that the decomposition mechanism of DNPG makes paraphrase generation more interpretable and controllable. Based on DNPG, we further develop an unsupervised domain adaptation method for paraphrase generation. Experimental results show that the proposed model achieves competitive in-domain performance compared to state-of-the-art neural models, and significantly better performance when adapting to a new domain.",,,,ACL
333,2019,Transforming Complex Sentences into a Semantic Hierarchy,"Christina Niklaus, Matthias Cetto, André Freitas, Siegfried Handschuh","We present an approach for recursively splitting and rephrasing complex English sentences into a novel semantic hierarchy of simplified sentences, with each of them presenting a more regular structure that may facilitate a wide variety of artificial intelligence tasks, such as machine translation (MT) or information extraction (IE). Using a set of hand-crafted transformation rules, input sentences are recursively transformed into a two-layered hierarchical representation in the form of core sentences and accompanying contexts that are linked via rhetorical relations. In this way, the semantic relationship of the decomposed constituents is preserved in the output, maintaining its interpretability for downstream applications. Both a thorough manual analysis and automatic evaluation across three datasets from two different domains demonstrate that the proposed syntactic simplification approach outperforms the state of the art in structural text simplification. Moreover, an extrinsic evaluation shows that when applying our framework as a preprocessing step the performance of state-of-the-art Open IE systems can be improved by up to 346% in precision and 52% in recall. To enable reproducible research, all code is provided online.",,,,ACL
334,2019,Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference,"Tom McCoy, Ellie Pavlick, Tal Linzen","A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",,,,ACL
335,2019,Zero-Shot Entity Linking by Reading Entity Descriptions,"Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin","We present the zero-shot entity linking task, where mentions must be linked to unseen entities without in-domain labeled data. The goal is to enable robust transfer to highly specialized domains, and so no metadata or alias tables are assumed. In this setting, entities are only identified by text descriptions, and models must rely strictly on language understanding to resolve the new entities. First, we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities. Second, we propose a simple and effective adaptive pre-training strategy, which we term domain-adaptive pre-training (DAP), to address the domain shift problem associated with linking unseen entities in a new domain. We present experiments on a new dataset that we construct for this task and show that DAP improves over strong pre-training baselines, including BERT. The data and code are available at https://github.com/lajanugen/zeshel.",,,,ACL
336,2019,Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition,"Joey Tianyi Zhou, Hao Zhang, Di Jin, Hongyuan Zhu, Meng Fang","We propose a new neural transfer method termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are investigated to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. In experiments, we examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data, without augmenting any additional hand-crafted features and pre-trained language model.",,,,ACL
337,2019,Scalable Syntax-Aware Language Models Using Knowledge Distillation,"Adhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen Clark, Phil Blunsom","Prior work has shown that, on small amounts of training data, syntactic neural language models learn structurally sensitive generalisations more successfully than sequential language models. However, their computational complexity renders scaling difficult, and it remains an open question whether structural biases are still necessary when sequential models have access to ever larger amounts of training data. To answer this question, we introduce an efficient knowledge distillation (KD) technique that transfers knowledge from a syntactic language model trained on a small corpus to an LSTM language model, hence enabling the LSTM to develop a more structurally sensitive representation of the larger training data it learns from. On targeted syntactic evaluations, we find that, while sequential LSTMs perform much better than previously reported, our proposed technique substantially improves on this baseline, yielding a new state of the art. Our findings and analysis affirm the importance of structural biases, even in models that learn from large amounts of data.",,,,ACL
338,2019,An Imitation Learning Approach to Unsupervised Parsing,"Bowen Li, Lili Mou, Frank Keller","Recently, there has been an increasing interest in unsupervised parsers that optimize semantically oriented objectives, typically using reinforcement learning. Unfortunately, the learned trees often do not match actual syntax trees well. Shen et al. (2018) propose a structured attention mechanism for language modeling (PRPN), which induces better syntactic structures but relies on ad hoc heuristics. Also, their model lacks interpretability as it is not grounded in parsing actions. In our work, we propose an imitation learning approach to unsupervised parsing, where we transfer the syntactic knowledge induced by PRPN to a Tree-LSTM model with discrete parsing actions. Its policy is then refined by Gumbel-Softmax training towards a semantically oriented objective. We evaluate our approach on the All Natural Language Inference dataset and show that it achieves a new state of the art in terms of parsing F-score, outperforming our base models, including PRPN.",,,,ACL
339,2019,Women’s Syntactic Resilience and Men’s Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing,"Aparna Garimella, Carmen Banea, Dirk Hovy, Rada Mihalcea","Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. To address this, we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles’ authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community.",,,,ACL
340,2019,Multilingual Constituency Parsing with Self-Attention and Pre-Training,"Nikita Kitaev, Steven Cao, Dan Klein","We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2% relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).",,,,ACL
341,2019,A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction,"Mengjie Zhao, Hinrich Schütze","We present a new method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world’s languages. We evaluate our method on Parallel Bible Corpus+ (PBC+), a parallel corpus of 1593 languages. The key idea is to use Byte Pair Encodings (BPEs) as basic units for multilingual embeddings. Through zero-shot transfer from English sentiment, we learn a seed lexicon for each language in the domain of PBC+. Through domain adaptation, we then generalize the domain-specific lexicon to a general one. We show – across typologically diverse languages in PBC+ – good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.",,,,ACL
342,2019,Tree Communication Models for Sentiment Analysis,"Yuan Zhang, Yue Zhang","Tree-LSTMs have been used for tree-based sentiment analysis over Stanford Sentiment Treebank, which allows the sentiment signals over hierarchical phrase structures to be calculated simultaneously. However, traditional tree-LSTMs capture only the bottom-up dependencies between constituents. In this paper, we propose a tree communication model using graph convolutional neural network and graph recurrent neural network, which allows rich information exchange between phrases constituent tree. Experiments show that our model outperforms existing work on bidirectional tree-LSTMs in both accuracy and efficiency, providing more consistent predictions on phrase-level sentiments.",,,,ACL
343,2019,Improved Sentiment Detection via Label Transfer from Monolingual to Synthetic Code-Switched Text,"Bidisha Samanta, Niloy Ganguly, Soumen Chakrabarti","Multilingual writers and speakers often alternate between two languages in a single discourse. This practice is called “code-switching”. Existing sentiment detection methods are usually trained on sentiment-labeled monolingual text. Manually labeled code-switched text, especially involving minority languages, is extremely rare. Consequently, the best monolingual methods perform relatively poorly on code-switched text. We present an effective technique for synthesizing labeled code-switched text from labeled monolingual text, which is relatively readily available. The idea is to replace carefully selected subtrees of constituency parses of sentences in the resource-rich language with suitable token spans selected from automatic translations to the resource-poor language. By augmenting the scarce labeled code-switched text with plentiful synthetic labeled code-switched text, we achieve significant improvements in sentiment labeling accuracy (1.5%, 5.11% 7.20%) for three different language pairs (English-Hindi, English-Spanish and English-Bengali). The improvement is even significant in hatespeech detection whereby we achieve a 4% improvement using only synthetic code-switched data (6% with data augmentation).",,,,ACL
344,2019,Exploring Sequence-to-Sequence Learning in Aspect Term Extraction,"Dehong Ma, Sujian Li, Fangzhao Wu, Xing Xie, Houfeng Wang","Aspect term extraction (ATE) aims at identifying all aspect terms in a sentence and is usually modeled as a sequence labeling problem. However, sequence labeling based methods cannot make full use of the overall meaning of the whole sentence and have the limitation in processing dependencies between labels. To tackle these problems, we first explore to formalize ATE as a sequence-to-sequence (Seq2Seq) learning task where the source sequence and target sequence are composed of words and labels respectively. At the same time, to make Seq2Seq learning suit to ATE where labels correspond to words one by one, we design the gated unit networks to incorporate corresponding word representation into the decoder, and position-aware attention to pay more attention to the adjacent words of a target word. The experimental results on two datasets show that Seq2Seq learning is effective in ATE accompanied with our proposed gated unit networks and position-aware attention mechanism.",,,,ACL
345,2019,Aspect Sentiment Classification Towards Question-Answering with Reinforced Bidirectional Attention Network,"Jingjing Wang, Changlong Sun, Shoushan Li, Xiaozhong Liu, Luo Si","In the literature, existing studies on aspect sentiment classification (ASC) focus on individual non-interactive reviews. This paper extends the research to interactive reviews and proposes a new research task, namely Aspect Sentiment Classification towards Question-Answering (ASC-QA), for real-world applications. This new task aims to predict sentiment polarities for specific aspects from interactive QA style reviews. In particular, a high-quality annotated corpus is constructed for ASC-QA to facilitate corresponding research. On this basis, a Reinforced Bidirectional Attention Network (RBAN) approach is proposed to address two inherent challenges in ASC-QA, i.e., semantic matching between question and answer, and data noise. Experimental results demonstrate the great advantage of the proposed approach to ASC-QA against several state-of-the-art baselines.",,,,ACL
346,2019,ELI5: Long Form Question Answering,"Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston","We introduce the first large-scale corpus for long form question answering, a task requiring elaborate and in-depth answers to open-ended questions. The dataset comprises 270K threads from the Reddit forum “Explain Like I’m Five” (ELI5) where an online community provides answers to questions which are comprehensible by five year olds. Compared to existing datasets, ELI5 comprises diverse questions requiring multi-sentence answers. We provide a large set of web documents to help answer the question. Automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq, language modeling, as well as a strong extractive baseline.However, our best model is still far from human performance since raters prefer gold responses in over 86% of cases, leaving ample opportunity for future improvement.",,,,ACL
347,2019,Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension,"Daesik Kim, Seonhoon Kim, Nojun Kwak","In this work, we introduce a novel algorithm for solving the textbook question answering (TQA) task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with analysis of the TQA dataset. First, solving the TQA problems requires to comprehend multi-modal contexts in complicated input data. To tackle this issue of extracting knowledge features from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f-GCN based on graph convolutional networks (GCN). Second, scientific terms are not spread over the chapters and subjects are split in the TQA dataset. To overcome this so called ‘out-of-domain’ issue, before learning QA problems, we introduce a novel self-supervised open-set learning process without any annotations. The experimental results show that our model significantly outperforms prior state-of-the-art methods. Moreover, ablation studies validate that both methods of incorporating f-GCN for extracting knowledge from multi-modal contexts and our newly proposed self-supervised learning process are effective for TQA problems.",,,,ACL
348,2019,Generating Question Relevant Captions to Aid Visual Question Answering,"Jialin Wu, Zeyuan Hu, Raymond Mooney",Visual question answering (VQA) and image captioning require a shared body of general knowledge connecting language and vision. We present a novel approach to better VQA performance that exploits this connection by jointly generating captions that are targeted to help answer a specific visual question. The model is trained using an existing caption dataset by automatically determining question-relevant captions using an online gradient-based method. Experimental results on the VQA v2 challenge demonstrates that our approach obtains state-of-the-art VQA performance (e.g. 68.4% in the Test-standard set using a single model) by simultaneously generating question-relevant captions.,,,,ACL
349,2019,Multi-grained Attention with Object-level Grounding for Visual Question Answering,"Pingping Huang, Jianhui Huang, Yuqing Guo, Min Qiao, Yong Zhu","Attention mechanisms are widely used in Visual Question Answering (VQA) to search for visual clues related to the question. Most approaches train attention models from a coarse-grained association between sentences and images, which tends to fail on small objects or uncommon concepts. To address this problem, this paper proposes a multi-grained attention method. It learns explicit word-object correspondence by two types of word-level attention complementary to the sentence-image association. Evaluated on the VQA benchmark, the multi-grained attention model achieves competitive performance with state-of-the-art models. And the visualized attention maps demonstrate that addition of object-level groundings leads to a better understanding of the images and locates the attended objects more precisely.",,,,ACL
350,2019,Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering,"Claudio Greco, Barbara Plank, Raquel Fernández, Raffaella Bernardi","We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. Our results show that dramatic forgetting is at play and that task difficulty and order matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.",,,,ACL
351,2019,Improving Visual Question Answering by Referring to Generated Paragraph Captions,"Hyounghun Kim, Mohit Bansal","Paragraph-style image captions describe diverse aspects of an image as opposed to the more common single-sentence captions that only provide an abstract description of the image. These paragraph captions can hence contain substantial information of the image for tasks such as visual question answering. Moreover, this textual information is complementary with visual information present in the image because it can discuss both more abstract concepts and more explicit, intermediate symbolic information about objects, events, and scenes that can directly be matched with the textual question and copied into the textual answer (i.e., via easier modality match). Hence, we propose a combined Visual and Textual Question Answering (VTQA) model which takes as input a paragraph caption as well as the corresponding image, and answers the given question based on both inputs. In our model, the inputs are fused to extract related information by cross-attention (early fusion), then fused again in the form of consensus (late fusion), and finally expected answers are given an extra score to enhance the chance of selection (later fusion). Empirical results show that paragraph captions, even when automatically generated (via an RL-based encoder-decoder model), help correctly answer more visual questions. Overall, our joint model, when trained on the Visual Genome dataset, significantly improves the VQA performance over a strong baseline model.",,,,ACL
352,2019,Shared-Private Bilingual Word Embeddings for Neural Machine Translation,"Xuebo Liu, Derek F. Wong, Yang Liu, Lidia S. Chao, Tong Xiao","Word embedding is central to neural machine translation (NMT), which has attracted intensive research interest in recent years. In NMT, the source embedding plays the role of the entrance while the target embedding acts as the terminal. These layers occupy most of the model parameters for representation learning. Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated. In this paper, we propose shared-private bilingual word embeddings, which give a closer relationship between the source and target embeddings, and which also reduce the number of model parameters. For similar source and target words, their embeddings tend to share a part of the features and they cooperatively learn these common representation units. Experiments on 5 language pairs belonging to 6 different language families and written in 5 different alphabets demonstrate that the proposed model provides a significant performance boost over the strong baselines with dramatically fewer model parameters.",,,,ACL
353,2019,Literary Event Detection,"Matthew Sims, Jong Ho Park, David Bamman","In this work we present a new dataset of literary events—events that are depicted as taking place within the imagined space of a novel. While previous work has focused on event detection in the domain of contemporary news, literature poses a number of complications for existing systems, including complex narration, the depiction of a broad array of mental states, and a strong emphasis on figurative language. We outline the annotation decisions of this new dataset and compare several models for predicting events; the best performing model, a bidirectional LSTM with BERT token representations, achieves an F1 score of 73.9. We then apply this model to a corpus of novels split across two dimensions—prestige and popularity—and demonstrate that there are statistically significant differences in the distribution of events for prestige.",,,,ACL
354,2019,Assessing the Ability of Self-Attention Networks to Learn Word Order,"Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, Zhaopeng Tu","Self-attention networks (SAN) have attracted a lot of interests due to their high parallelization and strong performance on a variety of NLP tasks, e.g. machine translation. Due to the lack of recurrence structure such as recurrent neural networks (RNN), SAN is ascribed to be weak at learning positional information of words for sequence modeling. However, neither this speculation has been empirically confirmed, nor explanations for their strong performances on machine translation tasks when “lacking positional information” have been explored. To this end, we propose a novel word reordering detection task to quantify how well the word order information learned by SAN and RNN. Specifically, we randomly move one word to another position, and examine whether a trained model can detect both the original and inserted positions. Experimental results reveal that: 1) SAN trained on word reordering detection indeed has difficulty learning the positional information even with the position embedding; and 2) SAN trained on machine translation learns better positional information than its RNN counterpart, in which position embedding plays a critical role. Although recurrence structure make the model more universally-effective on learning word order, learning objectives matter more in the downstream tasks such as machine translation.",,,,ACL
355,2019,Energy and Policy Considerations for Deep Learning in NLP,"Emma Strubell, Ananya Ganesh, Andrew McCallum","Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",,,,ACL
356,2019,What Does BERT Learn about the Structure of Language?,"Ganesh Jawahar, Benoît Sagot, Djamé Seddah","BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT’s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.",,,,ACL
357,2019,A Just and Comprehensive Strategy for Using NLP to Address Online Abuse,"David Jurgens, Libby Hemphill, Eshwar Chandrasekharan","Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities.",,,,ACL
358,2019,"Learning from Dialogue after Deployment: Feed Yourself, Chatbot!","Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, Jason Weston","The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving a vast store of potential training signal untapped. In this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in. As our agent engages in conversation, it also estimates user satisfaction in its responses. When the conversation appears to be going well, the user’s responses become new training examples to imitate. When the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot’s dialogue abilities further. On the PersonaChat chit-chat dataset with over 131k training examples, we find that learning from dialogue with a self-feeding chatbot significantly improves performance, regardless of the amount of traditional supervision.",,,,ACL
359,2019,Generating Responses with a Specific Emotion in Dialog,"Zhenqiao Song, Xiaoqing Zheng, Lu Liu, Mu Xu, Xuanjing Huang","It is desirable for dialog systems to have capability to express specific emotions during a conversation, which has a direct, quantifiable impact on improvement of their usability and user satisfaction. After a careful investigation of real-life conversation data, we found that there are at least two ways to express emotions with language. One is to describe emotional states by explicitly using strong emotional words; another is to increase the intensity of the emotional experiences by implicitly combining neutral words in distinct ways. We propose an emotional dialogue system (EmoDS) that can generate the meaningful responses with a coherent structure for a post, and meanwhile express the desired emotion explicitly or implicitly within a unified framework. Experimental results showed EmoDS performed better than the baselines in BLEU, diversity and the quality of emotional expression.",,,,ACL
360,2019,Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention,"Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan, William Yang Wang","Semantically controlled neural response generation on limited-domain has achieved great performance. However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics.",,,,ACL
361,2019,Incremental Learning from Scratch for Task-Oriented Dialogue Systems,"Weikang Wang, Jiajun Zhang, Qian Li, Mei-Yuh Hwang, Chengqing Zong","Clarifying user needs is essential for existing task-oriented dialogue systems. However, in real-world applications, developers can never guarantee that all possible user demands are taken into account in the design phase. Consequently, existing systems will break down when encountering unconsidered user needs. To address this problem, we propose a novel incremental learning framework to design task-oriented dialogue systems, or for short Incremental Dialogue System (IDS), without pre-defining the exhaustive list of user needs. Specifically, we introduce an uncertainty estimation module to evaluate the confidence of giving correct responses. If there is high confidence, IDS will provide responses to users. Otherwise, humans will be involved in the dialogue process, and IDS can learn from human intervention through an online learning module. To evaluate our method, we propose a new dataset which simulates unanticipated user needs in the deployment stage. Experiments show that IDS is robust to unconsidered user actions, and can update itself online by smartly selecting only the most effective training data, and hence attains better performance with less annotation cost.",,,,ACL
362,2019,ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation,"Hainan Zhang, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng","In multi-turn dialogue generation, response is usually related with only a few contexts. Therefore, an ideal model should be able to detect these relevant contexts and produce a suitable response accordingly. However, the widely used hierarchical recurrent encoder-decoder models just treat all the contexts indiscriminately, which may hurt the following response generation process. Some researchers try to use the cosine similarity or the traditional attention mechanism to find the relevant contexts, but they suffer from either insufficient relevance assumption or position bias problem. In this paper, we propose a new model, named ReCoSa, to tackle this problem. Firstly, a word level LSTM encoder is conducted to obtain the initial representation of each context. Then, the self-attention mechanism is utilized to update both the context and masked response representation. Finally, the attention weights between each context and response representations are computed and used in the further decoding process. Experimental results on both Chinese customer services dataset and English Ubuntu dialogue dataset show that ReCoSa significantly outperforms baseline models, in terms of both metric-based and human evaluations. Further analysis on attention shows that the detected relevant contexts by ReCoSa are highly coherent with human’s understanding, validating the correctness and interpretability of ReCoSa.",,,,ACL
363,2019,Dialogue Natural Language Inference,"Sean Welleck, Jason Weston, Arthur Szlam, Kyunghyun Cho","Consistency is a long standing issue faced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model’s consistency.",,,,ACL
364,2019,Budgeted Policy Learning for Task-Oriented Dialogue Systems,"Zhirui Zhang, Xiujun Li, Jianfeng Gao, Enhong Chen","This paper presents a new approach that extends Deep Dyna-Q (DDQ) by incorporating a Budget-Conscious Scheduling (BCS) to best utilize a fixed, small amount of user interactions (budget) for learning task-oriented dialogue agents. BCS consists of (1) a Poisson-based global scheduler to allocate budget over different stages of training; (2) a controller to decide at each training step whether the agent is trained using real or simulated experiences; (3) a user goal sampling module to generate the experiences that are most effective for policy learning. Experiments on a movie-ticket booking task with simulated and real users show that our approach leads to significant improvements in success rate over the state-of-the-art baselines given the fixed budget.",,,,ACL
365,2019,Comparison of Diverse Decoding Methods from Conditional Language Models,"Daphne Ippolito, Reno Kriz, Joao Sedoc, Maria Kustikova, Chris Callison-Burch","While conditional language models have greatly improved in their ability to output high quality natural language, many NLP applications benefit from being able to generate a diverse set of candidate sequences. Diverse decoding strategies aim to, within a given-sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that rerank and combine candidate outputs. Standard decoding methods, such as beam search, optimize for generating high likelihood sequences rather than diverse ones, though recent work has focused on increasing diversity in these methods. In this work, we perform an extensive survey of decoding-time strategies for generating diverse outputs from a conditional language model. In addition, we present a novel method where we over-sample candidates, then use clustering to remove similar sequences, thus achieving high diversity without sacrificing quality.",,,,ACL
366,2019,Retrieval-Enhanced Adversarial Training for Neural Response Generation,"Qingfu Zhu, Lei Cui, Wei-Nan Zhang, Furu Wei, Ting Liu","Dialogue systems are usually built on either generation-based or retrieval-based approaches, yet they do not benefit from the advantages of different models. In this paper, we propose a Retrieval-Enhanced Adversarial Training (REAT) method for neural response generation. Distinct from existing approaches, the REAT method leverages an encoder-decoder framework in terms of an adversarial training paradigm, while taking advantage of N-best response candidates from a retrieval-based system to construct the discriminator. An empirical study on a large scale public available benchmark dataset shows that the REAT method significantly outperforms the vanilla Seq2Seq model as well as the conventional adversarial training approach.",,,,ACL
367,2019,Vocabulary Pyramid Network: Multi-Pass Encoding and Decoding with Multi-Level Vocabularies for Response Generation,"Cao Liu, Shizhu He, Kang Liu, Jun Zhao","We study the task of response generation. Conventional methods employ a fixed vocabulary and one-pass decoding, which not only make them prone to safe and general responses but also lack further refining to the first generated raw sequence. To tackle the above two problems, we present a Vocabulary Pyramid Network (VPN) which is able to incorporate multi-pass encoding and decoding with multi-level vocabularies into response generation. Specifically, the dialogue input and output are represented by multi-level vocabularies which are obtained from hierarchical clustering of raw words. Then, multi-pass encoding and decoding are conducted on the multi-level vocabularies. Since VPN is able to leverage rich encoding and decoding information with multi-level vocabularies, it has the potential to generate better responses. Experiments on English Twitter and Chinese Weibo datasets demonstrate that VPN remarkably outperforms strong baselines.",,,,ACL
368,2019,On-device Structured and Context Partitioned Projection Networks,"Sujith Ravi, Zornitsa Kozareva","A challenging problem in on-device text classification is to build highly accurate neural models that can fit in small memory footprint and have low latency. To address this challenge, we propose an on-device neural network SGNN++ which dynamically learns compact projection vectors from raw text using structured and context-dependent partition projections. We show that this results in accelerated inference and performance improvements. We conduct extensive evaluation on multiple conversational tasks and languages such as English, Japanese, Spanish and French. Our SGNN++ model significantly outperforms all baselines, improves upon existing on-device neural models and even surpasses RNN, CNN and BiLSTM models on dialog act and intent prediction. Through a series of ablation studies we show the impact of the partitioned projections and structured information leading to 10% improvement. We study the impact of the model size on accuracy and introduce quatization-aware training for SGNN++ to further reduce the model size while preserving the same quality. Finally, we show fast inference on mobile phones.",,,,ACL
369,2019,Proactive Human-Machine Conversation with Explicit Conversation Goal,"Wenquan Wu, Zhen Guo, Xiangyang Zhou, Hua Wu, Xiyuan Zhang","Though great progress has been made for human-machine conversation, current dialogue system is still in its infancy: it usually converses passively and utters words more as a matter of response, rather than on its own initiatives. In this paper, we take a radical step towards building a human-like conversational agent: endowing it with the ability of proactively leading the conversation (introducing a new topic or maintaining the current topic). To facilitate the development of such conversation systems, we create a new dataset named Konv where one acts as a conversation leader and the other acts as the follower. The leader is provided with a knowledge graph and asked to sequentially change the discussion topics, following the given conversation goal, and meanwhile keep the dialogue as natural and engaging as possible. Konv enables a very challenging task as the model needs to both understand dialogue and plan over the given knowledge graph. We establish baseline results on this dataset (about 270K utterances and 30k dialogues) using several state-of-the-art models. Experimental results show that dialogue models that plan over the knowledge graph can make full use of related knowledge to generate more diverse multi-turn conversations. The baseline systems along with the dataset are publicly available.",,,,ACL
370,2019,Learning a Matching Model with Co-teaching for Multi-turn Response Selection in Retrieval-based Dialogue Systems,"Jiazhan Feng, Chongyang Tao, Wei Wu, Yansong Feng, Dongyan Zhao","We study learning of a matching model for response selection in retrieval-based dialogue systems. The problem is equally important with designing the architecture of a model, but is less explored in existing literature. To learn a robust matching model from noisy training data, we propose a general co-teaching framework with three specific teaching strategies that cover both teaching with loss functions and teaching with data curriculum. Under the framework, we simultaneously learn two matching models with independent training sets. In each iteration, one model transfers the knowledge learned from its training set to the other model, and at the same time receives the guide from the other model on how to overcome noise in training. Through being both a teacher and a student, the two models learn from each other and get improved together. Evaluation results on two public data sets indicate that the proposed learning approach can generally and significantly improve the performance of existing matching models.",,,,ACL
371,2019,Learning to Abstract for Memory-augmented Conversational Response Generation,"Zhiliang Tian, Wei Bi, Xiaopeng Li, Nevin L. Zhang","Neural generative models for open-domain chit-chat conversations have become an active area of research in recent years. A critical issue with most existing generative models is that the generated responses lack informativeness and diversity. A few researchers attempt to leverage the results of retrieval models to strengthen the generative models, but these models are limited by the quality of the retrieval results. In this work, we propose a memory-augmented generative model, which learns to abstract from the training corpus and saves the useful information to the memory to assist the response generation. Our model clusters query-response samples, extracts characteristics of each cluster, and learns to utilize these characteristics for response generation. Experimental results show that our model outperforms other competitive baselines.",,,,ACL
372,2019,Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References,"Lisong Qiu, Juntao Li, Wei Bi, Dongyan Zhao, Rui Yan","Due to its potential applications, open-domain dialogue generation has become popular and achieved remarkable progress in recent years, but sometimes suffers from generic responses. Previous models are generally trained based on 1-to-1 mapping from an input query to its response, which actually ignores the nature of 1-to-n mapping in dialogue that there may exist multiple valid responses corresponding to the same query. In this paper, we propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1-to-n mapping with a novel two-step generation architecture. The first generation phase extracts the common features of different responses which, combined with distinctive features obtained in the second phase, can generate multiple diverse and appropriate responses. Experimental results show that our proposed model can effectively improve the quality of response and outperform existing neural dialogue models on both automatic and human evaluations.",,,,ACL
373,2019,Pretraining Methods for Dialog Context Representation Learning,"Shikib Mehri, Evgeniia Razumovskaia, Tiancheng Zhao, Maxine Eskenazi","This paper examines various unsupervised pretraining objectives for learning dialog context representations. Two novel methods of pretraining dialog context encoders are proposed, and a total of four methods are examined. Each pretraining objective is fine-tuned and evaluated on a set of downstream dialog tasks using the MultiWoz dataset and strong performance improvement is observed. Further evaluation shows that our pretraining objectives result in not only better performance, but also better convergence, models that are less data hungry and have better domain generalizability.",,,,ACL
374,2019,A Large-Scale Corpus for Conversation Disentanglement,"Jonathan K. Kummerfeld, Sai R. Gouravajhala, Joseph J. Peper, Vignesh Athreya, Chulaka Gunasekara","Disentangling conversations mixed together in a single stream of messages is a difficult task, made harder by the lack of large manually annotated datasets. We created a new dataset of 77,563 messages manually annotated with reply-structure graphs that both disentangle conversations and define internal conversation structure. Our data is 16 times larger than all previously released datasets combined, the first to include adjudication of annotation disagreements, and the first to include context. We use our data to re-examine prior work, in particular, finding that 89% of conversations in a widely used dialogue corpus are either missing messages or contain extra messages. Our manually-annotated data presents an opportunity to develop robust data-driven methods for conversation disentanglement, which will help advance dialogue research.",,,,ACL
375,2019,Self-Supervised Dialogue Learning,"Jiawei Wu, Xin Wang, William Yang Wang","The sequential order of utterances is often meaningful in coherent dialogues, and the order changes of utterances could lead to low-quality and incoherent conversations. We consider the order information as a crucial supervised signal for dialogue learning, which, however, has been neglected by many previous dialogue systems. Therefore, in this paper, we introduce a self-supervised learning task, inconsistent order detection, to explicitly capture the flow of conversation in dialogues. Given a sampled utterance pair triple, the task is to predict whether it is ordered or misordered. Then we propose a sampling-based self-supervised network SSN to perform the prediction with sampled triple references from previous dialogue history. Furthermore, we design a joint learning framework where SSN can guide the dialogue systems towards more coherent and relevant dialogue learning through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and task-oriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets.",,,,ACL
376,2019,Are we there yet? Encoder-decoder neural networks as cognitive models of English past tense inflection,"Maria Corkery, Yevgen Matusevych, Sharon Goldwater","The cognitive mechanisms needed to account for the English past tense have long been a subject of debate in linguistics and cognitive science. Neural network models were proposed early on, but were shown to have clear flaws. Recently, however, Kirov and Cotterell (2018) showed that modern encoder-decoder (ED) models overcome many of these flaws. They also presented evidence that ED models demonstrate humanlike performance in a nonce-word task. Here, we look more closely at the behaviour of their model in this task. We find that (1) the model exhibits instability across multiple simulations in terms of its correlation with human data, and (2) even when results are aggregated across simulations (treating each simulation as an individual human participant), the fit to the human data is not strong—worse than an older rule-based model. These findings hold up through several alternative training regimes and evaluation measures. Although other neural architectures might do better, we conclude that there is still insufficient evidence to claim that neural nets are a good cognitive model for this task.",,,,ACL
377,2019,A Spreading Activation Framework for Tracking Conceptual Complexity of Texts,"Ioana Hulpuș, Sanja Štajner, Heiner Stuckenschmidt","We propose an unsupervised approach for assessing conceptual complexity of texts, based on spreading activation. Using DBpedia knowledge graph as a proxy to long-term memory, mentioned concepts become activated and trigger further activation as the text is sequentially traversed. Drawing inspiration from psycholinguistic theories of reading comprehension, we model memory processes such as semantic priming, sentence wrap-up, and forgetting. We show that our models capture various aspects of conceptual text complexity and significantly outperform current state of the art.",,,,ACL
378,2019,End-to-End Sequential Metaphor Identification Inspired by Linguistic Theories,"Rui Mao, Chenghua Lin, Frank Guerin","End-to-end training with Deep Neural Networks (DNN) is a currently popular method for metaphor identification. However, standard sequence tagging models do not explicitly take advantage of linguistic theories of metaphor identification. We experiment with two DNN models which are inspired by two human metaphor identification procedures. By testing on three public datasets, we find that our models achieve state-of-the-art performance in end-to-end metaphor identification.",,,,ACL
379,2019,Diachronic Sense Modeling with Deep Contextualized Word Embeddings: An Ecological View,"Renfen Hu, Shen Li, Shichen Liang","Diachronic word embeddings have been widely used in detecting temporal changes. However, existing methods face the meaning conflation deficiency by representing a word as a single vector at each time period. To address this issue, this paper proposes a sense representation and tracking framework based on deep contextualized embeddings, aiming at answering not only what and when, but also how the word meaning changes. The experiments show that our framework is effective in representing fine-grained word senses, and it brings a significant improvement in word change detection task. Furthermore, we model the word change from an ecological viewpoint, and sketch two interesting sense behaviors in the process of language evolution, i.e. sense competition and sense cooperation.",,,,ACL
380,2019,Miss Tools and Mr Fruit: Emergent Communication in Agents Learning about Object Affordances,"Diane Bouchacourt, Marco Baroni","Recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. We propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential communication. However, the agents develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge.",,,,ACL
381,2019,CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks,"Roberto Dessì, Marco Baroni","Lake and Baroni (2018) introduced the SCAN dataset probing the ability of seq2seq models to capture compositional generalizations, such as inferring the meaning of “jump around” 0-shot from the component words. Recurrent networks (RNNs) were found to completely fail the most challenging generalization cases. We test here a convolutional network (CNN) on these tasks, reporting hugely improved performance with respect to RNNs. Despite the big improvement, the CNN has however not induced systematic rules, suggesting that the difference between compositional and non-compositional behaviour is not clear-cut.",,,,ACL
382,2019,Uncovering Probabilistic Implications in Typological Knowledge Bases,"Johannes Bjerva, Yova Kementchedjhieva, Ryan Cotterell, Isabelle Augenstein","The study of linguistic typology is rooted in the implications we find between linguistic features, such as the fact that languages with object-verb word ordering tend to have postpositions. Uncovering such implications typically amounts to time-consuming manual processing by trained and experienced linguists, which potentially leaves key linguistic universals unexplored. In this paper, we present a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further linguistic investigation. Our approach outperforms baselines previously used for this problem, as well as a strong baseline from knowledge base population.",,,,ACL
383,2019,Is Word Segmentation Child’s Play in All Languages?,"Georgia R. Loukatou, Steven Moran, Damian Blasi, Sabine Stoll, Alejandrina Cristia","When learning language, infants need to break down the flow of input speech into minimal word-like units, a process best described as unsupervised bottom-up segmentation. Proposed strategies include several segmentation algorithms, but only cross-linguistically robust algorithms could be plausible candidates for human word learning, since infants have no initial knowledge of the ambient language. We report on the stability in performance of 11 conceptually diverse algorithms on a selection of 8 typologically distinct languages. The results consist evidence that some segmentation algorithms are cross-linguistically valid, thus could be considered as potential strategies employed by all infants.",,,,ACL
384,2019,On the Distribution of Deep Clausal Embeddings: A Large Cross-linguistic Study,"Damian Blasi, Ryan Cotterell, Lawrence Wolf-Sonkin, Sabine Stoll, Balthasar Bickel","Embedding a clause inside another (“the girl [who likes cars [that run fast]] has arrived”) is a fundamental resource that has been argued to be a key driver of linguistic expressiveness. As such, it plays a central role in fundamental debates on what makes human language unique, and how they might have evolved. Empirical evidence on the prevalence and the limits of embeddings has however been based on either laboratory setups or corpus data of relatively limited size. We introduce here a collection of large, dependency-parsed written corpora in 17 languages, that allow us, for the first time, to capture clausal embedding through dependency graphs and assess their distribution. Our results indicate that there is no evidence for hard constraints on embedding depth: the tail of depth distributions is heavy. Moreover, although deeply embedded clauses tend to be shorter, suggesting processing load issues, complex sentences with many embeddings do not display a bias towards less deep embeddings. Taken together, the results suggest that deep embeddings are not disfavoured in written language. More generally, our study illustrates how resources and methods from latest-generation big-data NLP can provide new perspectives on fundamental questions in theoretical linguistics.",,,,ACL
385,2019,Attention-based Conditioning Methods for External Knowledge Integration,"Katerina Margatina, Christos Baziotis, Alexandros Potamianos","In this paper, we present a novel approach for incorporating external knowledge in Recurrent Neural Networks (RNNs). We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures. This form of conditioning on the attention distribution, enforces the contribution of the most salient words for the task at hand. We introduce three methods, namely attentional concatenation, feature-based gating and affine transformation. Experiments on six benchmark datasets show the effectiveness of our methods. Attentional feature-based gating yields consistent performance improvement across tasks. Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture.",,,,ACL
386,2019,The KnowRef Coreference Corpus: Removing Gender and Number Cues for Difficult Pronominal Anaphora Resolution,"Ali Emami, Paul Trichelair, Adam Trischler, Kaheer Suleman, Hannes Schulz","We introduce a new benchmark for coreference resolution and NLI, KnowRef, that targets common-sense understanding and world knowledge. Previous coreference resolution tasks can largely be solved by exploiting the number and gender of the antecedents, or have been handcrafted and do not reflect the diversity of naturally occurring text. We present a corpus of over 8,000 annotated text passages with ambiguous pronominal anaphora. These instances are both challenging and realistic. We show that various coreference systems, whether rule-based, feature-rich, or neural, perform significantly worse on the task than humans, who display high inter-annotator agreement. To explain this performance gap, we show empirically that state-of-the art models often fail to capture context, instead relying on the gender or number of candidate antecedents to make a decision. We then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in models. Finally, we show that antecedent switching yields promising results on other tasks as well: we use it to achieve state-of-the-art results on the GAP coreference task.",,,,ACL
387,2019,StRE: Self Attentive Edit Quality Prediction in Wikipedia,"Soumya Sarkar, Bhanu Prakash Reddy, Sandipan Sikdar, Animesh Mukherjee","Wikipedia can easily be justified as a behemoth, considering the sheer volume of content that is added or removed every minute to its several projects. This creates an immense scope, in the field of natural language processing toward developing automated tools for content moderation and review. In this paper we propose Self Attentive Revision Encoder (StRE) which leverages orthographic similarity of lexical units toward predicting the quality of new edits. In contrast to existing propositions which primarily employ features like page reputation, editor activity or rule based heuristics, we utilize the textual content of the edits which, we believe contains superior signatures of their quality. More specifically, we deploy deep encoders to generate representations of the edits from its text content, which we then leverage to infer quality. We further contribute a novel dataset containing ∼ 21M revisions across 32K Wikipedia pages and demonstrate that StRE outperforms existing methods by a significant margin – at least 17% and at most 103%. Our pre-trained model achieves such result after retraining on a set as small as 20% of the edits in a wikipage. This, to the best of our knowledge, is also the first attempt towards employing deep language models to the enormous domain of automated content moderation and review in Wikipedia.",,,,ACL
388,2019,How Large Are Lions? Inducing Distributions over Quantitative Attributes,"Yanai Elazar, Abhijit Mahabal, Deepak Ramachandran, Tania Bedrax-Weiss, Dan Roth","Most current NLP systems have little knowledge about quantitative attributes of objects and events. We propose an unsupervised method for collecting quantitative information from large amounts of web data, and use it to create a new, very large resource consisting of distributions over physical quantities associated with objects, adjectives, and verbs which we call Distributions over Quantitative (DoQ). This contrasts with recent work in this area which has focused on making only relative comparisons such as “Is a lion bigger than a wolf?”. Our evaluation shows that DoQ compares favorably with state of the art results on existing datasets for relative comparisons of nouns and adjectives, and on a new dataset we introduce.",,,,ACL
389,2019,Fine-Grained Sentence Functions for Short-Text Conversation,"Wei Bi, Jun Gao, Xiaojiang Liu, Shuming Shi","Sentence function is an important linguistic feature referring to a user’s purpose in uttering a specific sentence. The use of sentence function has shown promising results to improve the performance of conversation models. However, there is no large conversation dataset annotated with sentence functions. In this work, we collect a new Short-Text Conversation dataset with manually annotated SEntence FUNctions (STC-Sefun). Classification models are trained on this dataset to (i) recognize the sentence function of new data in a large corpus of short-text conversations; (ii) estimate a proper sentence function of the response given a test query. We later train conversation models conditioned on the sentence functions, including information retrieval-based and neural generative models. Experimental results demonstrate that the use of sentence functions can help improve the quality of the returned responses.",,,,ACL
390,2019,Give Me More Feedback II: Annotating Thesis Strength and Related Attributes in Student Essays,"Zixuan Ke, Hrishikesh Inamdar, Hui Lin, Vincent Ng","While the vast majority of existing work on automated essay scoring has focused on holistic scoring, researchers have recently begun work on scoring specific dimensions of essay quality. Nevertheless, progress on dimension-specific essay scoring is limited in part by the lack of annotated corpora. To facilitate advances in this area, we design a scoring rubric for scoring a core, yet unexplored dimension of persuasive essay quality, thesis strength, and annotate a corpus of essays with thesis strength scores. We additionally identify the attributes that could impact thesis strength and annotate the essays with the values of these attributes, which, when predicted by computational models, could provide further feedback to students on why her essay receives a particular thesis strength score.",,,,ACL
391,2019,Crowdsourcing and Validating Event-focused Emotion Corpora for German and English,"Enrica Troiano, Sebastian Padó, Roman Klinger","Sentiment analysis has a range of corpora available across multiple languages. For emotion analysis, the situation is more limited, which hinders potential research on crosslingual modeling and the development of predictive models for other languages. In this paper, we fill this gap for German by constructing deISEAR, a corpus designed in analogy to the well-established English ISEAR emotion dataset. Motivated by Scherer’s appraisal theory, we implement a crowdsourcing experiment which consists of two steps. In step 1, participants create descriptions of emotional events for a given emotion. In step 2, five annotators assess the emotion expressed by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not, on average, cause a performance drop.",,,,ACL
392,2019,Pay Attention when you Pay the Bills. A Multilingual Corpus with Dependency-based and Semantic Annotation of Collocations.,"Marcos Garcia, Marcos García Salido, Susana Sotelo, Estela Mosqueira, Margarita Alonso-Ramos","This paper presents a new multilingual corpus with semantic annotation of collocations in English, Portuguese, and Spanish. The whole resource contains 155k tokens and 1,526 collocations labeled in context. The annotated examples belong to three syntactic relations (adjective-noun, verb-object, and nominal compounds), and represent 58 lexical functions in the Meaning-Text Theory (e.g., Oper, Magn, Bon, etc.). Each collocation was annotated by three linguists and the final resource was revised by a team of experts. The resulting corpus can serve as a basis to evaluate different approaches for collocation identification, which in turn can be useful for different NLP tasks such as natural language understanding or natural language generation.",,,,ACL
393,2019,Does it Make Sense? And Why? A Pilot Study for Sense Making and Explanation,"Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, Tian Gao","Introducing common sense to natural language understanding systems has received increasing research attention. It remains a fundamental question on how to evaluate whether a system has the sense-making capability. Existing benchmarks measure common sense knowledge indirectly or without reasoning. In this paper, we release a benchmark to directly test whether a system can differentiate natural language statements that make sense from those that do not make sense. In addition, a system is asked to identify the most crucial reason why a statement does not make sense. We evaluate models trained over large-scale language modeling tasks as well as human performance, showing that there are different challenges for system sense-making.",,,,ACL
394,2019,Large Dataset and Language Model Fun-Tuning for Humor Recognition,"Vladislav Blinov, Valeria Bolotova-Baranova, Pavel Braslavski","The task of humor recognition has attracted a lot of attention recently due to the urge to process large amounts of user-generated texts and rise of conversational agents. We collected a dataset of jokes and funny dialogues in Russian from various online resources and complemented them carefully with unfunny texts with similar lexical properties. The dataset comprises of more than 300,000 short texts, which is significantly larger than any previous humor-related corpus. Manual annotation of 2,000 items proved the reliability of the corpus construction approach. Further, we applied language model fine-tuning for text classification and obtained an F1 score of 0.91 on a test set, which constitutes a considerable gain over baseline methods. The dataset is freely available for research community.",,,,ACL
395,2019,Towards Language Agnostic Universal Representations,"Armen Aghajanyan, Xia Song, Saurabh Tiwary","When a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in, even if the math lessons were only taught in one language. However, current representations in machine learning are language dependent. In this work, we present a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion. We learn these representations by taking inspiration from linguistics, specifically the Universal Grammar hypothesis and learn universal latent representations that are language agnostic. We demonstrate the capabilities of these representations by showing that models trained on a single language using language agnostic representations achieve very similar accuracies in other languages.",,,,ACL
396,2019,Leveraging Meta Information in Short Text Aggregation,"He Zhao, Lan Du, Guanfeng Liu, Wray Buntine","Short texts such as tweets often contain insufficient word co-occurrence information for training conventional topic models. To deal with the insufficiency, we propose a generative model that aggregates short texts into clusters by leveraging the associated meta information. Our model can generate more interpretable topics as well as document clusters. We develop an effective Gibbs sampling algorithm favoured by the fully local conjugacy in the model. Extensive experiments demonstrate that our model achieves better performance in terms of document clustering and topic coherence.",,,,ACL
397,2019,Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning,"Shuai Tang, Virginia R. de Sa","Encoder-decoder models for unsupervised sentence representation learning using the distributional hypothesis effectively constrain the learnt representation of a sentence to only that needed to reproduce the next sentence. While the decoder is important to constrain the representation, these models tend to discard the decoder after training since only the encoder is needed to map the input sentence into a vector representation. However, parameters learnt in the decoder also contain useful information about the language. In order to utilise the decoder after learning, we present two types of decoding functions whose inverse can be easily derived without expensive inverse calculation. Therefore, the inverse of the decoding function serves as another encoder that produces sentence representations. We show that, with careful design of the decoding functions, the model learns good sentence representations, and the ensemble of the representations produced from the encoder and the inverse of the decoder demonstrate even better generalisation ability and solid transferability.",,,,ACL
398,2019,"Self-Attentive, Multi-Context One-Class Classification for Unsupervised Anomaly Detection on Text","Lukas Ruff, Yury Zemlyanskiy, Robert Vandermeulen, Thomas Schnake, Marius Kloft","There exist few text-specific methods for unsupervised anomaly detection, and for those that do exist, none utilize pre-trained models for distributed vector representations of words. In this paper we introduce a new anomaly detection method—Context Vector Data Description (CVDD)—which builds upon word embedding models to learn multiple sentence representations that capture multiple semantic contexts via the self-attention mechanism. Modeling multiple contexts enables us to perform contextual anomaly detection of sentences and phrases with respect to the multiple themes and concepts present in an unlabeled text corpus. These contexts in combination with the self-attention weights make our method highly interpretable. We demonstrate the effectiveness of CVDD quantitatively as well as qualitatively on the well-known Reuters, 20 Newsgroups, and IMDB Movie Reviews datasets.",,,,ACL
399,2019,Hubless Nearest Neighbor Search for Bilingual Lexicon Induction,"Jiaji Huang, Qiang Qiu, Kenneth Church","Bilingual Lexicon Induction (BLI) is the task of translating words from corpora in two languages. Recent advances in BLI work by aligning the two word embedding spaces. Following that, a key step is to retrieve the nearest neighbor (NN) in the target space given the source word. However, a phenomenon called hubness often degrades the accuracy of NN. Hubness appears as some data points, called hubs, being extra-ordinarily close to many of the other data points. Reducing hubness is necessary for retrieval tasks. One successful example is Inverted SoFtmax (ISF), recently proposed to improve NN. This work proposes a new method, Hubless Nearest Neighbor (HNN), to mitigate hubness. HNN differs from NN by imposing an additional equal preference assumption. Moreover, the HNN formulation explains why ISF works as well as it does. Empirical results demonstrate that HNN outperforms NN, ISF and other state-of-the-art. For reproducibility and follow-ups, we have published all code.",,,,ACL
400,2019,Distant Learning for Entity Linking with Automatic Noise Detection,"Phong Le, Ivan Titov","Accurate entity linkers have been produced for domains and languages where annotated data (i.e., texts linked to a knowledge base) is available. However, little progress has been made for the settings where no or very limited amounts of labeled data are present (e.g., legal or most scientific domains). In this work, we show how we can learn to link mentions without having any labeled examples, only a knowledge base and a collection of unannotated texts from the corresponding domain. In order to achieve this, we frame the task as a multi-instance learning problem and rely on surface matching to create initial noisy labels. As the learning signal is weak and our surrogate labels are noisy, we introduce a noise detection component in our model: it lets the model detect and disregard examples which are likely to be noisy. Our method, jointly learning to detect noise and link entities, greatly outperforms the surface matching baseline. For a subset of entity categories, it even approaches the performance of supervised learning.",,,,ACL
401,2019,Learning How to Active Learn by Dreaming,"Thuy-Trang Vu, Ming Liu, Dinh Phung, Gholamreza Haffari",Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary. Recent data-driven AL policy learning methods are also restricted to learn from closely related domains. We introduce a new sample-efficient method that learns the AL policy directly on the target domain of interest by using wake and dream cycles. Our approach interleaves between querying the annotation of the selected datapoints to update the underlying student learner and improving AL policy using simulation where the current student learner acts as an imperfect annotator. We evaluate our method on cross-domain and cross-lingual text classification and named entity recognition tasks. Experimental results show that our dream-based AL policy training strategy is more effective than applying the pretrained policy without further fine-tuning and better than the existing strong baseline methods that use heuristics or reinforcement learning.,,,,ACL
402,2019,Few-Shot Representation Learning for Out-Of-Vocabulary Words,"Ziniu Hu, Ting Chen, Kai-Wei Chang, Yizhou Sun","Existing approaches for learning word embedding often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts. However, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do not appear in training corpus emerge frequently. How to learn accurate representations of these words to augment a pre-trained embedding by only a few observations is a challenging research problem. In this paper, we formulate the learning of OOV embedding as a few-shot regression problem by fitting a representation function to predict an oracle embedding vector (defined as embedding trained with abundant observations) based on limited contexts. Specifically, we propose a novel hierarchical attention network-based embedding framework to serve as the neural regression function, in which the context information of a word is encoded and aggregated from K observations. Furthermore, we propose to use Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new corpus fast and robustly. Experiments show that the proposed approach significantly outperforms existing methods in constructing an accurate embedding for OOV words and improves downstream tasks when the embedding is utilized.",,,,ACL
403,2019,Neural Temporality Adaptation for Document Classification: Diachronic Word Embeddings and Domain Adaptation Models,"Xiaolei Huang, Michael J. Paul","Language usage can change across periods of time, but document classifiers models are usually trained and tested on corpora spanning multiple years without considering temporal variations. This paper describes two complementary ways to adapt classifiers to shifts across time. First, we show that diachronic word embeddings, which were originally developed to study language change, can also improve document classification, and we show a simple method for constructing this type of embedding. Second, we propose a time-driven neural classification model inspired by methods for domain adaptation. Experiments on six corpora show how these methods can make classifiers more robust over time.",,,,ACL
404,2019,Learning Transferable Feature Representations Using Neural Networks,"Himanshu Sharad Bhatt, Shourya Roy, Arun Rajkumar, Sriranjani Ramakrishnan","Learning representations such that the source and target distributions appear as similar as possible has benefited transfer learning tasks across several applications. Generally it requires labeled data from the source and only unlabeled data from the target to learn such representations. While these representations act like a bridge to transfer knowledge learned in the source to the target; they may lead to negative transfer when the source specific characteristics detract their ability to represent the target data. We present a novel neural network architecture to simultaneously learn a two-part representation which is based on the principle of segregating source specific representation from the common representation. The first part captures the source specific characteristics while the second part captures the truly common representation. Our architecture optimizes an objective function which acts adversarial for the source specific part if it contributes towards the cross-domain learning. We empirically show that two parts of the representation, in different arrangements, outperforms existing learning algorithms on the source learning as well as cross-domain tasks on multiple datasets.",,,,ACL
405,2019,"Bayes Test of Precision, Recall, and F1 Measure for Comparison of Two Natural Language Processing Models","Ruibo Wang, Jihong Li","Direct comparison on point estimation of the precision (P), recall (R), and F1 measure of two natural language processing (NLP) models on a common test corpus is unreasonable and results in less replicable conclusions due to a lack of a statistical test. However, the existing t-tests in cross-validation (CV) for model comparison are inappropriate because the distributions of P, R, F1 are skewed and an interval estimation of P, R, and F1 based on a t-test may exceed [0,1]. In this study, we propose to use a block-regularized 3×2 CV (3×2 BCV) in model comparison because it could regularize the difference in certain frequency distributions over linguistic units between training and validation sets and yield stable estimators of P, R, and F1. On the basis of the 3×2 BCV, we calibrate the posterior distributions of P, R, and F1 and derive an accurate interval estimation of P, R, and F1. Furthermore, we formulate the comparison into a hypothesis testing problem and propose a novel Bayes test. The test could directly compute the probabilities of the hypotheses on the basis of the posterior distributions and provide more informative decisions than the existing significance t-tests. Three experiments with regard to NLP chunking tasks are conducted, and the results illustrate the validity of the Bayes test.",,,,ACL
406,2019,TIGS: An Inference Algorithm for Text Infilling with Gradient Search,"Dayiheng Liu, Jie Fu, Pengfei Liu, Jiancheng Lv","Text infilling aims at filling in the missing part of a sentence or paragraph, which has been applied to a variety of real-world natural language generation scenarios. Given a well-trained sequential generative model, it is challenging for its unidirectional decoder to generate missing symbols conditioned on the past and future information around the missing part. In this paper, we propose an iterative inference algorithm based on gradient search, which could be the first inference algorithm that can be broadly applied to any neural sequence generative models for text infilling tasks. Extensive experimental comparisons show the effectiveness and efficiency of the proposed method on three different text infilling tasks with various mask ratios and different mask strategies, comparing with five state-of-the-art methods.",,,,ACL
407,2019,Keeping Notes: Conditional Natural Language Generation with a Scratchpad Encoder,"Ryan Benmalek, Madian Khabsa, Suma Desu, Claire Cardie, Michele Banko","We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.",,,,ACL
408,2019,Using Automatically Extracted Minimum Spans to Disentangle Coreference Evaluation from Boundary Detection,"Nafise Sadat Moosavi, Leo Born, Massimo Poesio, Michael Strube","The common practice in coreference resolution is to identify and evaluate the maximum span of mentions. The use of maximum spans tangles coreference evaluation with the challenges of mention boundary detection like prepositional phrase attachment. To address this problem, minimum spans are manually annotated in smaller corpora. However, this additional annotation is costly and therefore, this solution does not scale to large corpora. In this paper, we propose the MINA algorithm for automatically extracting minimum spans to benefit from minimum span evaluation in all corpora. We show that the extracted minimum spans by MINA are consistent with those that are manually annotated by experts. Our experiments show that using minimum spans is in particular important in cross-dataset coreference evaluation, in which detected mention boundaries are noisier due to domain shift. We have integrated MINA into https://github.com/ns-moosavi/coval for reporting standard coreference scores based on both maximum and automatically detected minimum spans.",,,,ACL
409,2019,Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution,"Shany Barhom, Vered Shwartz, Alon Eirew, Michael Bugert, Nils Reimers","Recognizing coreferring events and entities across multiple texts is crucial for many NLP applications. Despite the task’s importance, research focus was given mostly to within-document entity coreference, with rather little attention to the other variants. We propose a neural architecture for cross-document coreference resolution. Inspired by Lee et al. (2012), we jointly model entity and event coreference. We represent an event (entity) mention using its lexical span, surrounding context, and relation to entity (event) mentions via predicate-arguments structures. Our model outperforms the previous state-of-the-art event coreference model on ECB+, while providing the first entity coreference results on this corpus. Our analysis confirms that all our representation elements, including the mention span itself, its context, and the relation to other mentions contribute to the model’s success.",,,,ACL
410,2019,A Unified Linear-Time Framework for Sentence-Level Discourse Parsing,"Xiang Lin, Shafiq Joty, Prathyusha Jwalapuram, M Saiful Bari","We propose an efficient neural framework for sentence-level discourse analysis in accordance with Rhetorical Structure Theory (RST). Our framework comprises a discourse segmenter to identify the elementary discourse units (EDU) in a text, and a discourse parser that constructs a discourse tree in a top-down fashion. Both the segmenter and the parser are based on Pointer Networks and operate in linear time. Our segmenter yields an F1 score of 95.4%, and our parser achieves an F1 score of 81.7% on the aggregated labeled (relation) metric, surpassing previous approaches by a good margin and approaching human agreement on both tasks (98.3 and 83.0 F1).",,,,ACL
411,2019,Employing the Correspondence of Relations and Connectives to Identify Implicit Discourse Relations via Label Embeddings,"Linh The Nguyen, Linh Van Ngo, Khoat Than, Thien Huu Nguyen","It has been shown that implicit connectives can be exploited to improve the performance of the models for implicit discourse relation recognition (IDRR). An important property of the implicit connectives is that they can be accurately mapped into the discourse relations conveying their functions. In this work, we explore this property in a multi-task learning framework for IDRR in which the relations and the connectives are simultaneously predicted, and the mapping is leveraged to transfer knowledge between the two prediction tasks via the embeddings of relations and connectives. We propose several techniques to enable such knowledge transfer that yield the state-of-the-art performance for IDRR on several settings of the benchmark dataset (i.e., the Penn Discourse Treebank dataset).",,,,ACL
412,2019,Do You Know That Florence Is Packed with Visitors? Evaluating State-of-the-art Models of Speaker Commitment,"Nanjiang Jiang, Marie-Catherine de Marneffe","When a speaker, Mary, asks “Do you know that Florence is packed with visitors?”, we take her to believe that Florence is packed with visitors, but not if she asks “Do you think that Florence is packed with visitors?”. Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. Here, we explore the hypothesis that linguistic deficits drive the error patterns of existing speaker commitment models by analyzing the linguistic correlates of model error on a challenging naturalistic dataset. We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses. The CommitmentBank is annotated with speaker commitment towards the content of the complement (“Florence is packed with visitors” in our example) of clause-embedding verbs (“know”, “think”) under four entailment-canceling environments (negation, modal, question, conditional). A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement.",,,,ACL
413,2019,Multi-Relational Script Learning for Discourse Relations,"I-Ta Lee, Dan Goldwasser","Modeling script knowledge can be useful for a wide range of NLP tasks. Current statistical script learning approaches embed the events, such that their relationships are indicated by their similarity in the embedding. While intuitive, these approaches fall short of representing nuanced relations, needed for downstream tasks. In this paper, we suggest to view learning event embedding as a multi-relational problem, which allows us to capture different aspects of event pairs. We model a rich set of event relations, such as Cause and Contrast, derived from the Penn Discourse Tree Bank. We evaluate our model on three types of tasks, the popular Mutli-Choice Narrative Cloze and its variants, several multi-relational prediction tasks, and a related downstream task—implicit discourse sense classification.",,,,ACL
414,2019,Open-Domain Why-Question Answering with Adversarial Learning to Encode Answer Texts,"Jong-Hoon Oh, Kazuma Kadowaki, Julien Kloetzer, Ryu Iida, Kentaro Torisawa","In this paper, we propose a method for why-question answering (why-QA) that uses an adversarial learning framework. Existing why-QA methods retrieve “answer passages” that usually consist of several sentences. These multi-sentence passages contain not only the reason sought by a why-question and its connection to the why-question, but also redundant and/or unrelated parts. We use our proposed “Adversarial networks for Generating compact-answer Representation” (AGR) to generate from a passage a vector representation of the non-redundant reason sought by a why-question and exploit the representation for judging whether the passage actually answers the why-question. Through a series of experiments using Japanese why-QA datasets, we show that these representations improve the performance of our why-QA neural model as well as that of a BERT-based why-QA model. We show that they also improve a state-of-the-art distantly supervised open-domain QA (DS-QA) method on publicly available English datasets, even though the target task is not a why-QA.",,,,ACL
415,2019,Learning to Ask Unanswerable Questions for Machine Reading Comprehension,"Haichao Zhu, Li Dong, Furu Wei, Wenhui Wang, Bing Qin","Machine reading comprehension with unanswerable questions is a challenging task. In this work, we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable question paired with its corresponding paragraph that contains the answer. We introduce a pair-to-sequence model for unanswerable question generation, which effectively captures the interactions between the question and the paragraph. We also present a way to construct training data for our question generation models by leveraging the existing reading comprehension dataset. Experimental results show that the pair-to-sequence model performs consistently better compared with the sequence-to-sequence baseline. We further use the automatically generated unanswerable questions as a means of data augmentation on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model.",,,,ACL
416,2019,Compositional Questions Do Not Necessitate Multi-hop Reasoning,"Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi","Multi-hop reading comprehension (RC) questions are challenging because they require reading and reasoning over multiple paragraphs. We argue that it can be difficult to construct large multi-hop RC datasets. For example, even highly compositional questions can be answered with a single hop if they target specific entity types, or the facts needed to answer them are redundant. Our analysis is centered on HotpotQA, where we show that single-hop reasoning can solve much more of the dataset than previously thought. We introduce a single-hop BERT-based RC model that achieves 67 F1—comparable to state-of-the-art multi-hop models. We also design an evaluation setting where humans are not shown all of the necessary paragraphs for the intended multi-hop reasoning but can still answer over 80% of questions. Together with detailed error analysis, these results suggest there should be an increasing focus on the role of evidence in multi-hop reasoning and possibly even a shift towards information retrieval style evaluations with large and diverse evidence collections.",,,,ACL
417,2019,Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader,"Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, William Yang Wang","We propose a new end-to-end question answering model, which learns to aggregate answer evidence from an incomplete knowledge base (KB) and a set of retrieved text snippets.Under the assumptions that structured data is easier to query and the acquired knowledge can help the understanding of unstructured text, our model first accumulates knowledge ofKB entities from a question-related KB sub-graph; then reformulates the question in the latent space and reads the text with the accumulated entity knowledge at hand. The evidence from KB and text are finally aggregated to predict answers. On the widely-used KBQA benchmark WebQSP, our model achieves consistent improvements across settings with different extents of KB incompleteness.",,,,ACL
418,2019,AdaNSP: Uncertainty-driven Adaptive Decoding in Neural Semantic Parsing,"Xiang Zhang, Shizhu He, Kang Liu, Jun Zhao","Neural semantic parsers utilize the encoder-decoder framework to learn an end-to-end model for semantic parsing that transduces a natural language sentence to the formal semantic representation. To keep the model aware of the underlying grammar in target sequences, many constrained decoders were devised in a multi-stage paradigm, which decode to the sketches or abstract syntax trees first, and then decode to target semantic tokens. We instead to propose an adaptive decoding method to avoid such intermediate representations. The decoder is guided by model uncertainty and automatically uses deeper computations when necessary. Thus it can predict tokens adaptively. Our model outperforms the state-of-the-art neural models and does not need any expertise like predefined grammar or sketches in the meantime.",,,,ACL
419,2019,The Language of Legal and Illegal Activity on the Darknet,"Leshem Choshen, Dan Eldad, Daniel Hershcovich, Elior Sulem, Omri Abend","The non-indexed parts of the Internet (the Darknet) have become a haven for both legal and illegal anonymous activity. Given the magnitude of these networks, scalably monitoring their activity necessarily relies on automated tools, and notably on NLP tools. However, little is known about what characteristics texts communicated through the Darknet have, and how well do off-the-shelf NLP tools do on this domain. This paper tackles this gap and performs an in-depth investigation of the characteristics of legal and illegal text in the Darknet, comparing it to a clear net website with similar content as a control condition. Taking drugs-related websites as a test case, we find that texts for selling legal and illegal drugs have several linguistic characteristics that distinguish them from one another, as well as from the control condition, among them the distribution of POS tags, and the coverage of their named entities in Wikipedia.",,,,ACL
420,2019,Eliciting Knowledge from Experts: Automatic Transcript Parsing for Cognitive Task Analysis,"Junyi Du, He Jiang, Jiaming Shen, Xiang Ren","Cognitive task analysis (CTA) is a type of analysis in applied psychology aimed at eliciting and representing the knowledge and thought processes of domain experts. In CTA, often heavy human labor is involved to parse the interview transcript into structured knowledge (e.g., flowchart for different actions). To reduce human efforts and scale the process, automated CTA transcript parsing is desirable. However, this task has unique challenges as (1) it requires the understanding of long-range context information in conversational text; and (2) the amount of labeled data is limited and indirect—i.e., context-aware, noisy, and low-resource. In this paper, we propose a weakly-supervised information extraction framework for automated CTA transcript parsing. We partition the parsing process into a sequence labeling task and a text span-pair relation extraction task, with distant supervision from human-curated protocol files. To model long-range context information for extracting sentence relations, neighbor sentences are involved as a part of input. Different types of models for capturing context dependency are then applied. We manually annotate real-world CTA transcripts to facilitate the evaluation of the parsing tasks.",,,,ACL
421,2019,Course Concept Expansion in MOOCs with External Knowledge and Interactive Game,"Jifan Yu, Chenyu Wang, Gan Luo, Lei Hou, Juanzi Li","As Massive Open Online Courses (MOOCs) become increasingly popular, it is promising to automatically provide extracurricular knowledge for MOOC users. Suffering from semantic drifts and lack of knowledge guidance, existing methods can not effectively expand course concepts in complex MOOC environments. In this paper, we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results. In addition, to involve human efforts in our model, we design an interactive optimization mechanism based on a game. Our experiments on the four datasets from Coursera and XuetangX show that the proposed method achieves significant improvements(+0.19 by MAP) over existing methods.",,,,ACL
422,2019,Towards Near-imperceptible Steganographic Text,"Falcon Dai, Zheng Cai","We show that the imperceptibility of several existing linguistic steganographic systems (Fang et al., 2017; Yang et al., 2018) relies on implicit assumptions on statistical behaviors of fluent text. We formally analyze them and empirically evaluate these assumptions. Furthermore, based on these observations, we propose an encoding algorithm called patient-Huffman with improved near-imperceptible guarantees.",,,,ACL
423,2019,Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network,"Sunil Kumar Sahu, Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou","Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.",,,,ACL
424,2019,Neural Legal Judgment Prediction in English,"Ilias Chalkidis, Ion Androutsopoulos, Nikolaos Aletras","Legal judgment prediction is the task of automatically predicting the outcome of a court case, given a text describing the case’s facts. Previous work on using neural models for this task has focused on Chinese; only feature-based models (e.g., using bags of words and topics) have been considered in English. We release a new English legal judgment prediction dataset, containing cases from the European Court of Human Rights. We evaluate a broad variety of neural models on the new dataset, establishing strong baselines that surpass previous feature-based models in three tasks: (1) binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation.",,,,ACL
425,2019,Robust Neural Machine Translation with Doubly Adversarial Inputs,"Yong Cheng, Lu Jiang, Wolfgang Macherey","Neural machine translation (NMT) often suffers from the vulnerability to noisy perturbations in the input. We propose an approach to improving the robustness of NMT models, which consists of two parts: (1) attack the translation model with adversarial source examples; (2) defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs. For the generation of adversarial inputs, we propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs. Experimental results on Chinese-English and English-German translation tasks demonstrate that our approach achieves significant improvements (2.8 and 1.6 BLEU points) over Transformer on standard clean benchmarks as well as exhibiting higher robustness on noisy data.",,,,ACL
426,2019,Bridging the Gap between Training and Inference for Neural Machine Translation,"Wen Zhang, Yang Feng, Fandong Meng, Di You, Qun Liu","Neural Machine Translation (NMT) generates target words sequentially in the way of predicting the next word conditioned on the context words. At training time, it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch. This discrepancy of the fed context leads to error accumulation among the way. Furthermore, word-level training requires strict matching between the generated sequence and the ground truth sequence which leads to overcorrection over different but reasonable translations. In this paper, we address these issues by sampling context words not only from the ground truth sequence but also from the predicted sequence by the model during training, where the predicted sequence is selected with a sentence-level optimum. Experiment results on Chinese->English and WMT’14 English->German translation tasks demonstrate that our approach can achieve significant improvements on multiple datasets.",,,,ACL
427,2019,Beyond BLEU:Training Neural Machine Translation with Semantic Similarity,"John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, Graham Neubig","While most neural machine translation (NMT)systems are still trained using maximum likelihood estimation, recent work has demonstrated that optimizing systems to directly improve evaluation metrics such as BLEU can significantly improve final translation accuracy. However, training with BLEU has some limitations: it doesn’t assign partial credit, it has a limited range of output values, and it can penalize semantically correct hypotheses if they differ lexically from the reference. In this paper, we introduce an alternative reward function for optimizing NMT systems that is based on recent work in semantic similarity. We evaluate on four disparate languages trans-lated to English, and find that training with our proposed metric results in better translations as evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU",,,,ACL
428,2019,AutoML Strategy Based on Grammatical Evolution: A Case Study about Knowledge Discovery from Text,"Suilan Estevez-Velarde, Yoan Gutiérrez, Andrés Montoyo, Yudivián Almeida-Cruz","The process of extracting knowledge from natural language text poses a complex problem that requires both a combination of machine learning techniques and proper feature selection. Recent advances in Automatic Machine Learning (AutoML) provide effective tools to explore large sets of algorithms, hyper-parameters and features to find out the most suitable combination of them. This paper proposes a novel AutoML strategy based on probabilistic grammatical evolution, which is evaluated on the health domain by facing the knowledge discovery challenge in Spanish text documents. Our approach achieves state-of-the-art results and provides interesting insights into the best combination of parameters and algorithms to use when dealing with this challenge. Source code is provided for the research community.",,,,ACL
429,2019,Distilling Discrimination and Generalization Knowledge for Event Detection via Delta-Representation Learning,"Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun","Event detection systems rely on discrimination knowledge to distinguish ambiguous trigger words and generalization knowledge to detect unseen/sparse trigger words. Current neural event detection approaches focus on trigger-centric representations, which work well on distilling discrimination knowledge, but poorly on learning generalization knowledge. To address this problem, this paper proposes a Delta-learning approach to distill discrimination and generalization knowledge by effectively decoupling, incrementally learning and adaptively fusing event representation. Experiments show that our method significantly outperforms previous approaches on unseen/sparse trigger words, and achieves state-of-the-art performance on both ACE2005 and KBP2017 datasets.",,,,ACL
430,2019,Chinese Relation Extraction with Multi-Grained Information and External Linguistic Knowledge,"Ziran Li, Ning Ding, Zhiyuan Liu, Haitao Zheng, Ying Shen","Chinese relation extraction is conducted using neural networks with either character-based or word-based inputs, and most existing methods typically suffer from segmentation errors and ambiguity of polysemy. To address the issues, we propose a multi-grained lattice framework (MG lattice) for Chinese relation extraction to take advantage of multi-grained language information and external linguistic knowledge. In this framework, (1) we incorporate word-level information into character sequence inputs so that segmentation errors can be avoided. (2) We also model multiple senses of polysemous words with the help of external linguistic knowledge, so as to alleviate polysemy ambiguity. Experiments on three real-world datasets in distinct domains show consistent and significant superiority and robustness of our model, as compared with other baselines. We will release the source code of this paper in the future.",,,,ACL
431,2019,A2N: Attending to Neighbors for Knowledge Graph Inference,"Trapit Bansal, Da-Cheng Juan, Sujith Ravi, Andrew McCallum","State-of-the-art models for knowledge graph completion aim at learning a fixed embedding representation of entities in a multi-relational graph which can generalize to infer unseen entity relationships at test time. This can be sub-optimal as it requires memorizing and generalizing to all possible entity relationships using these fixed representations. We thus propose a novel attention-based method to learn query-dependent representation of entities which adaptively combines the relevant graph neighborhood of an entity leading to more accurate KG completion. The proposed method is evaluated on two benchmark datasets for knowledge graph completion, and experimental results show that the proposed model performs competitively or better than existing state-of-the-art, including recent methods for explicit multi-hop reasoning. Qualitative probing offers insight into how the model can reason about facts involving multiple hops in the knowledge graph, through the use of neighborhood attention.",,,,ACL
432,2019,Graph based Neural Networks for Event Factuality Prediction using Syntactic and Semantic Structures,"Amir Pouran Ben Veyseh, Thien Huu Nguyen, Dejing Dou","Event factuality prediction (EFP) is the task of assessing the degree to which an event mentioned in a sentence has happened. For this task, both syntactic and semantic information are crucial to identify the important context words. The previous work for EFP has only combined these information in a simple way that cannot fully exploit their coordination. In this work, we introduce a novel graph-based neural network for EFP that can integrate the semantic and syntactic information more effectively. Our experiments demonstrate the advantage of the proposed model for EFP.",,,,ACL
433,2019,Embedding Time Expressions for Deep Temporal Ordering Models,"Tanya Goyal, Greg Durrett","Data-driven models have demonstrated state-of-the-art performance in inferring the temporal ordering of events in text. However, these models often overlook explicit temporal signals, such as dates and time windows. Rule-based methods can be used to identify the temporal links between these time expressions (timexes), but they fail to capture timexes’ interactions with events and are hard to integrate with the distributed representations of neural net models. In this paper, we introduce a framework to infuse temporal awareness into such models by learning a pre-trained model to embed timexes. We generate synthetic data consisting of pairs of timexes, then train a character LSTM to learn embeddings and classify the timexes’ temporal relation. We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions.",,,,ACL
434,2019,Episodic Memory Reader: Learning What to Remember for Question Answering from Streaming Data,"Moonsu Han, Minki Kang, Hyunwoo Jung, Sung Ju Hwang","We consider a novel question answering (QA) task where the machine needs to read from large streaming data (long documents or videos) without knowing when the questions will be given, which is difficult to solve with existing QA methods due to their lack of scalability. To tackle this problem, we propose a novel end-to-end deep network model for reading comprehension, which we refer to as Episodic Memory Reader (EMR) that sequentially reads the input contexts into an external memory, while replacing memories that are less important for answering unseen questions. Specifically, we train an RL agent to replace a memory entry when the memory is full, in order to maximize its QA accuracy at a future timepoint, while encoding the external memory using either the GRU or the Transformer architecture to learn representations that considers relative importance between the memory entries. We validate our model on a synthetic dataset (bAbI) as well as real-world large-scale textual QA (TriviaQA) and video QA (TVQA) datasets, on which it achieves significant improvements over rule based memory scheduling policies or an RL based baseline that independently learns the query-specific importance of each memory.",,,,ACL
435,2019,Selection Bias Explorations and Debias Methods for Natural Language Sentence Matching Datasets,"Guanhua Zhang, Bing Bai, Jian Liang, Kun Bai, Shiyu Chang","Natural Language Sentence Matching (NLSM) has gained substantial attention from both academics and the industry, and rich public datasets contribute a lot to this process. However, biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results. For many NLSM datasets, the providers select some pairs of sentences into the datasets, and this sampling procedure can easily bring unintended pattern, i.e., selection bias. One example is the QuoraQP dataset, where some content-independent naive features are unreasonably predictive. Such features are the reflection of the selection bias and termed as the “leakage features.” In this paper, we investigate the problem of selection bias on six NLSM datasets and find that four out of them are significantly biased. We further propose a training and evaluation framework to alleviate the bias. Experimental results on QuoraQP suggest that the proposed framework can improve the generalization ability of trained models, and give more trustworthy evaluation results for real-world adoptions.",,,,ACL
436,2019,Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index,"Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi","Existing open-domain question answering (QA) models are not suitable for real-time usage because they need to process several long documents on-demand for every input query, which is computationally prohibitive. In this paper, we introduce query-agnostic indexable representations of document phrases that can drastically speed up open-domain QA. In particular, our dense-sparse phrase encoding effectively captures syntactic, semantic, and lexical information of the phrases and eliminates the pipeline filtering of context documents. Leveraging strategies for optimizing training and inference time, our model can be trained and deployed even in a single 4-GPU server. Moreover, by representing phrases as pointers to their start and end tokens, our model indexes phrases in the entire English Wikipedia (up to 60 billion phrases) using under 2TB. Our experiments on SQuAD-Open show that our model is on par with or more accurate than previous models with 6000x reduced computational cost, which translates into at least 68x faster end-to-end inference benchmark on CPUs. Code and demo are available at nlp.cs.washington.edu/denspi",,,,ACL
437,2019,Language Modeling with Shared Grammar,"Yuyu Zhang, Le Song","Sequential recurrent neural networks have achieved superior performance on language modeling, but overlook the structure information in natural language. Recent works on structure-aware models have shown promising results on language modeling. However, how to incorporate structure knowledge on corpus without syntactic annotations remains an open problem. In this work, we propose neural variational language model (NVLM), which enables the sharing of grammar knowledge among different corpora. Experimental results demonstrate the effectiveness of our framework on two popular benchmark datasets. With the help of shared grammar, our language model converges significantly faster to a lower perplexity on new training corpus.",,,,ACL
438,2019,Zero-Shot Semantic Parsing for Instructions,"Ofer Givoli, Roi Reichart","We consider a zero-shot semantic parsing task: parsing instructions into compositional logical forms, in domains that were not seen during training. We present a new dataset with 1,390 examples from 7 application domains (e.g. a calendar or a file manager), each example consisting of a triplet: (a) the application’s initial state, (b) an instruction, to be carried out in the context of that state, and (c) the state of the application after carrying out the instruction. We introduce a new training algorithm that aims to train a semantic parser on examples from a set of source domains, so that it can effectively parse instructions from an unknown target domain. We integrate our algorithm into the floating parser of Pasupat and Liang (2015), and further augment the parser with features and a logical form candidate filtering logic, to support zero-shot adaptation. Our experiments with various zero-shot adaptation setups demonstrate substantial performance gains over a non-adapted parser.",,,,ACL
439,2019,Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling,"Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy","Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning BERT on an intermediate task often negatively impacts downstream transfer. In a more positive trend, we see modest gains from multitask training, suggesting the development of more sophisticated multitask and transfer learning techniques as an avenue for further research.",,,,ACL
440,2019,Complex Question Decomposition for Semantic Parsing,"Haoyu Zhang, Jingjing Cai, Jianjun Xu, Ji Wang","In this work, we focus on complex question semantic parsing and propose a novel Hierarchical Semantic Parsing (HSP) method, which utilizes the decompositionality of complex questions for semantic parsing. Our model is designed within a three-stage parsing architecture based on the idea of decomposition-integration. In the first stage, we propose a question decomposer which decomposes a complex question into a sequence of sub-questions. In the second stage, we design an information extractor to derive the type and predicate information of these questions. In the last stage, we integrate the generated information from previous stages and generate a logical form for the complex question. We conduct experiments on COMPLEXWEBQUESTIONS which is a large scale complex question semantic parsing dataset, results show that our model achieves significant improvement compared to state-of-the-art methods.",,,,ACL
441,2019,Multi-Task Deep Neural Networks for Natural Language Understanding,"Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao","In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.",,,,ACL
442,2019,DisSent: Learning Sentence Representations from Explicit Discourse Relations,"Allen Nie, Erin Bennett, Noah Goodman","Learning effective representations of sentences is one of the core missions of natural language understanding. Existing models either train on a vast amount of text, or require costly, manually curated sentence relation datasets. We show that with dependency parsing and rule-based rubrics, we can curate a high quality sentence relation task by leveraging explicit discourse relations. We show that our curated dataset provides an excellent signal for learning vector representations of sentence meaning, representing relations that can only be determined when the meanings of two sentences are combined. We demonstrate that the automatically curated corpus allows a bidirectional LSTM sentence encoder to yield high quality sentence embeddings and can serve as a supervised fine-tuning dataset for larger models such as BERT. Our fixed sentence embeddings achieve high performance on a variety of transfer tasks, including SentEval, and we achieve state-of-the-art results on Penn Discourse Treebank’s implicit relation prediction task.",,,,ACL
443,2019,SParC: Cross-Domain Semantic Parsing in Context,"Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin","We present SParC, a dataset for cross-domainSemanticParsing inContext that consists of 4,298 coherent question sequences (12k+ individual questions annotated with SQL queries). It is obtained from controlled user interactions with 200 complex databases over 138 domains. We provide an in-depth analysis of SParC and show that it introduces new challenges compared to existing datasets. SParC demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. We experiment with two state-of-the-art text-to-SQL models adapted to the context-dependent, cross-domain setup. The best model obtains an exact match accuracy of 20.2% over all questions and less than10% over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. The dataset, baselines, and leaderboard are released at https://yale-lily.github.io/sparc.",,,,ACL
444,2019,Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation,"Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou","We present a neural approach called IRNet for complex and cross-domain Text-to-SQL. IRNet aims to address two challenges: 1) the mismatch between intents expressed in natural language (NL) and the implementation details in SQL; 2) the challenge in predicting columns caused by the large number of out-of-domain words. Instead of end-to-end synthesizing a SQL query, IRNet decomposes the synthesis process into three phases. In the first phase, IRNet performs a schema linking over a question and a database schema. Then, IRNet adopts a grammar-based neural model to synthesize a SemQL query which is an intermediate representation that we design to bridge NL and SQL. Finally, IRNet deterministically infers a SQL query from the synthesized SemQL query with domain knowledge. On the challenging Text-to-SQL benchmark Spider, IRNet achieves 46.7% accuracy, obtaining 19.5% absolute improvement over previous state-of-the-art approaches. At the time of writing, IRNet achieves the first position on the Spider leaderboard.",,,,ACL
445,2019,EigenSent: Spectral sentence embeddings using higher-order Dynamic Mode Decomposition,"Subhradeep Kayal, George Tsatsaronis","Distributed representation of words, or word embeddings, have motivated methods for calculating semantic representations of word sequences such as phrases, sentences and paragraphs. Most of the existing methods to do so either use algorithms to learn such representations, or improve on calculating weighted averages of the word vectors. In this work, we experiment with spectral methods of signal representation and summarization as mechanisms for constructing such word-sequence embeddings in an unsupervised fashion. In particular, we explore an algorithm rooted in fluid-dynamics, known as higher-order Dynamic Mode Decomposition, which is designed to capture the eigenfrequencies, and hence the fundamental transition dynamics, of periodic and quasi-periodic systems. It is empirically observed that this approach, which we call EigenSent, can summarize transitions in a sequence of words and generate an embedding that can represent well the sequence itself. To the best of the authors’ knowledge, this is the first application of a spectral decomposition and signal summarization technique on text, to create sentence embeddings. We test the efficacy of this algorithm in creating sentence embeddings on three public datasets, where it performs appreciably well. Moreover it is also shown that, due to the positive combination of their complementary properties, concatenating the embeddings generated by EigenSent with simple word vector averaging achieves state-of-the-art results.",,,,ACL
446,2019,SemBleu: A Robust Metric for AMR Parsing Evaluation,"Linfeng Song, Daniel Gildea","Evaluating AMR parsing accuracy involves comparing pairs of AMR graphs. The major evaluation metric, SMATCH (Cai and Knight, 2013), searches for one-to-one mappings between the nodes of two AMRs with a greedy hill-climbing algorithm, which leads to search errors. We propose SEMBLEU, a robust metric that extends BLEU (Papineni et al., 2002) to AMRs. It does not suffer from search errors and considers non-local correspondences in addition to local ones. SEMBLEU is fully content-driven and punishes situations where a system’s output does not preserve most information from the input. Preliminary experiments on both sentence and corpus levels show that SEMBLEU has slightly higher consistency with human judgments than SMATCH. Our code is available at http://github.com/ freesunshine0316/sembleu.",,,,ACL
447,2019,Reranking for Neural Semantic Parsing,"Pengcheng Yin, Graham Neubig","Semantic parsing considers the task of transducing natural language (NL) utterances into machine executable meaning representations (MRs). While neural network-based semantic parsers have achieved impressive improvements over previous methods, results are still far from perfect, and cursory manual inspection can easily identify obvious problems such as lack of adequacy or coherence of the generated MRs. This paper presents a simple approach to quickly iterate and improve the performance of an existing neural semantic parser by reranking an n-best list of predicted MRs, using features that are designed to fix observed problems with baseline models. We implement our reranker in a competitive neural semantic parser and test on four semantic parsing (GEO, ATIS) and Python code generation (Django, CoNaLa) tasks, improving the strong baseline parser by up to 5.7% absolute in BLEU (CoNaLa) and 2.9% in accuracy (Django), outperforming the best published neural parser results on all four datasets.",,,,ACL
448,2019,Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing,"Ben Bogin, Jonathan Berant, Matt Gardner","Research on parsing language to SQL has largely ignored the structure of the database (DB) schema, either because the DB was very simple, or because it was observed at both training and test time. In spider, a recently-released text-to-SQL dataset, new and complex DBs are given at test time, and so the structure of the DB schema can inform the predicted SQL query. In this paper, we present an encoder-decoder semantic parser, where the structure of the DB schema is encoded with a graph neural network, and this representation is later used at both encoding and decoding time. Evaluation shows that encoding the schema structure improves our parser accuracy from 33.8% to 39.4%, dramatically above the current state of the art, which is at 19.7%.",,,,ACL
449,2019,Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark,"Nikita Nangia, Samuel R. Bowman","The GLUE benchmark (Wang et al., 2019b) is a suite of language understanding tasks which has seen dramatic progress in the past year, with average performance moving from 70.0 at launch to 83.9, state of the art at the time of writing (May 24, 2019). Here, we measure human performance on the benchmark, in order to learn whether significant headroom remains for further progress. We provide a conservative estimate of human performance on the benchmark through crowdsourcing: Our annotators are non-experts who must learn each task from a brief set of instructions and 20 examples. In spite of limited training, these annotators robustly outperform the state of the art on six of the nine GLUE tasks and achieve an average score of 87.1. Given the fast pace of progress however, the headroom we observe is quite limited. To reproduce the data-poor setting that our annotators must learn in, we also train the BERT model (Devlin et al., 2019) in limited-data regimes, and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding.",,,,ACL
450,2019,Compositional Semantic Parsing across Graphbanks,"Matthias Lindemann, Jonas Groschwitz, Alexander Koller","Most semantic parsers that map sentences to graph-based meaning representations are hand-designed for specific graphbanks. We present a compositional neural semantic parser which achieves, for the first time, competitive accuracies across a diverse range of graphbanks. Incorporating BERT embeddings and multi-task learning improves the accuracy further, setting new states of the art on DM, PAS, PSD, AMR 2015 and EDS.",,,,ACL
451,2019,Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning,"Tahira Naseem, Abhishek Shah, Hui Wan, Radu Florian, Salim Roukos","Our work involves enriching the Stack-LSTM transition-based AMR parser (Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs. In addition, we also combined several AMR-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification, named entities and contextualized embeddings. We achieve a highly competitive performance that is comparable to the best published results. We show an in-depth study ablating each of the new components of the parser.",,,,ACL
452,2019,BERT Rediscovers the Classical NLP Pipeline,"Ian Tenney, Dipanjan Das, Ellie Pavlick","Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",,,,ACL
453,2019,Simple and Effective Paraphrastic Similarity from Parallel Translations,"John Wieting, Kevin Gimpel, Graham Neubig, Taylor Berg-Kirkpatrick","We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the time-consuming intermediate step of creating para-phrase corpora. Further, we show that the resulting model can be applied to cross lingual tasks where it both outperforms and is orders of magnitude faster than more complex state-of-the-art baselines.",,,,ACL
454,2019,Second-Order Semantic Dependency Parsing with End-to-End Neural Networks,"Xinyu Wang, Jingxian Huang, Kewei Tu","Semantic dependency parsing aims to identify semantic relationships between words in a sentence that form a graph. In this paper, we propose a second-order semantic dependency parser, which takes into consideration not only individual dependency edges but also interactions between pairs of edges. We show that second-order parsing can be approximated using mean field (MF) variational inference or loopy belief propagation (LBP). We can unfold both algorithms as recurrent layers of a neural network and therefore can train the parser in an end-to-end manner. Our experiments show that our approach achieves state-of-the-art performance.",,,,ACL
455,2019,Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper),"Santiago Castro, Devamanyu Hazarika, Verónica Pérez-Rosas, Roger Zimmermann, Rada Mihalcea","Sarcasm is often expressed through several verbal and non-verbal cues, e.g., a change of tone, overemphasis in a word, a drawn-out syllable, or a straight looking face. Most of the recent work in sarcasm detection has been carried out on textual data. In this paper, we argue that incorporating multimodal cues can improve the automatic classification of sarcasm. As a first step towards enabling the development of multimodal approaches for sarcasm detection, we propose a new sarcasm dataset, Multimodal Sarcasm Detection Dataset (MUStARD), compiled from popular TV shows. MUStARD consists of audiovisual utterances annotated with sarcasm labels. Each utterance is accompanied by its context of historical utterances in the dialogue, which provides additional information on the scenario where the utterance occurs. Our initial results show that the use of multimodal information can reduce the relative error rate of sarcasm detection by up to 12.9% in F-score when compared to the use of individual modalities. The full dataset is publicly available for use at https://github.com/soujanyaporia/MUStARD.",,,,ACL
456,2019,Determining Relative Argument Specificity and Stance for Complex Argumentative Structures,"Esin Durmus, Faisal Ladhak, Claire Cardie","Systems for automatic argument generation and debate require the ability to (1) determine the stance of any claims employed in the argument and (2) assess the specificity of each claim relative to the argument context. Existing work on understanding claim specificity and stance, however, has been limited to the study of argumentative structures that are relatively shallow, most often consisting of a single claim that directly supports or opposes the argument thesis. In this paper, we tackle these tasks in the context of complex arguments on a diverse set of topics. In particular, our dataset consists of manually curated argument trees for 741 controversial topics covering 95,312 unique claims; lines of argument are generally of depth 2 to 6. We find that as the distance between a pair of claims increases along the argument path, determining the relative specificity of a pair of claims becomes easier and determining their relative stance becomes harder.",,,,ACL
457,2019,Latent Variable Sentiment Grammar,"Liwen Zhang, Kewei Tu, Yue Zhang","Neural models have been investigated for sentiment classification over constituent trees. They learn phrase composition automatically by encoding tree structures but do not explicitly model sentiment composition, which requires to encode sentiment class labels. To this end, we investigate two formalisms with deep sentiment representations that capture sentiment subtype expressions by latent variables and Gaussian mixture vectors, respectively. Experiments on Stanford Sentiment Treebank (SST) show the effectiveness of sentiment grammar over vanilla neural encoders. Using ELMo embeddings, our method gives the best results on this benchmark.",,,,ACL
458,2019,An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese,"Enkhbold Bataa, Joshua Wu","Text classification approaches have usually required task-specific model architectures and huge labeled datasets. Recently, thanks to the rise of text-based transfer learning techniques, it is possible to pre-train a language model in an unsupervised manner and leverage them to perform effective on downstream tasks. In this work we focus on Japanese and show the potential use of transfer learning techniques in text classification. Specifically, we perform binary and multi-class sentiment classification on the Rakuten product review and Yahoo movie review datasets. We show that transfer learning-based approaches perform better than task-specific models trained on 3 times as much data. Furthermore, these approaches perform just as well for language modeling pre-trained on only 1/30 of the data. We release our pre-trained models and code as open source.",,,,ACL
459,2019,Probing Neural Network Comprehension of Natural Language Arguments,"Timothy Niven, Hung-Yu Kao","We are surprised to find that BERT’s peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.",,,,ACL
460,2019,Recognising Agreement and Disagreement between Stances with Reason Comparing Networks,"Chang Xu, Cecile Paris, Surya Nepal, Ross Sparks","We identify agreement and disagreement between utterances that express stances towards a topic of discussion. Existing methods focus mainly on conversational settings, where dialogic features are used for (dis)agreement inference. We extend this scope and seek to detect stance (dis)agreement in a broader setting, where independent stance-bearing utterances, which prevail in many stance corpora and real-world scenarios, are compared. To cope with such non-dialogic utterances, we find that the reasons uttered to back up a specific stance can help predict stance (dis)agreements. We propose a reason comparing network (RCN) to leverage reason information for stance comparison. Empirical results on a well-known stance corpus show that our method can discover useful reason information, enabling it to outperform several baselines in stance (dis)agreement detection.",,,,ACL
461,2019,Toward Comprehensive Understanding of a Sentiment Based on Human Motives,"Naoki Otani, Eduard Hovy","In sentiment detection, the natural language processing community has focused on determining holders, facets, and valences, but has paid little attention to the reasons for sentiment decisions. Our work considers human motives as the driver for human sentiments and addresses the problem of motive detection as the first step. Following a study in psychology, we define six basic motives that cover a wide range of topics appearing in review texts, annotate 1,600 texts in restaurant and laptop domains with the motives, and report the performance of baseline methods on this new dataset. We also show that cross-domain transfer learning boosts detection performance, which indicates that these universal motives exist across different domains.",,,,ACL
462,2019,Context-aware Embedding for Targeted Aspect-based Sentiment Analysis,"Bin Liang, Jiachen Du, Ruifeng Xu, Binyang Li, Hejiao Huang","Attention-based neural models were employed to detect the different aspects and sentiment polarities of the same target in targeted aspect-based sentiment analysis (TABSA). However, existing methods do not specifically pre-train reasonable embeddings for targets and aspects in TABSA. This may result in targets or aspects having the same vector representations in different contexts and losing the context-dependent information. To address this problem, we propose a novel method to refine the embeddings of targets and aspects. Such pivotal embedding refinement utilizes a sparse coefficient vector to adjust the embeddings of target and aspect from the context. Hence the embeddings of targets and aspects can be refined from the highly correlative words instead of using context-independent or randomly initialized vectors. Experiment results on two benchmark datasets show that our approach yields the state-of-the-art performance in TABSA task.",,,,ACL
463,2019,"Yes, we can! Mining Arguments in 50 Years of US Presidential Campaign Debates","Shohreh Haddadan, Elena Cabrio, Serena Villata","Political debates offer a rare opportunity for citizens to compare the candidates’ positions on the most controversial topics of the campaign. Thus they represent a natural application scenario for Argument Mining. As existing research lacks solid empirical investigation of the typology of argument components in political debates, we fill this gap by proposing an Argument Mining approach to political debates. We address this task in an empirical manner by annotating 39 political debates from the last 50 years of US presidential campaigns, creating a new corpus of 29k argument components, labeled as premises and claims. We then propose two tasks: (1) identifying the argumentative components in such debates, and (2) classifying them as premises and claims. We show that feature-rich SVM learners and Neural Network architectures outperform standard baselines in Argument Mining over such complex data. We release the new corpus USElecDeb60To16 and the accompanying software under free licenses to the research community.",,,,ACL
464,2019,An Empirical Study of Span Representations in Argumentation Structure Parsing,"Tatsuki Kuribayashi, Hiroki Ouchi, Naoya Inoue, Paul Reisert, Toshinori Miyoshi","For several natural language processing (NLP) tasks, span representation design is attracting considerable attention as a promising new technique; a common basis for an effective design has been established. With such basis, exploring task-dependent extensions for argumentation structure parsing (ASP) becomes an interesting research direction. This study investigates (i) span representation originally developed for other NLP tasks and (ii) a simple task-dependent extension for ASP. Our extensive experiments and analysis show that these representations yield high performance for ASP and provide some challenging types of instances to be parsed.",,,,ACL
465,2019,Simple and Effective Text Matching with Richer Alignment Features,"Runqi Yang, Jianhai Zhang, Xing Gao, Feng Ji, Haiqing Chen","In this paper, we present a fast and strong neural approach for general purpose text matching applications. We explore what is sufficient to build a fast and well-performed text matching model and propose to keep three key features available for inter-sequence alignment: original point-wise features, previous aligned features, and contextual features while simplifying all the remaining components. We conduct experiments on four well-studied benchmark datasets across tasks of natural language inference, paraphrase identification and answer selection. The performance of our model is on par with the state-of-the-art on all datasets with much fewer parameters and the inference speed is at least 6 times faster compared with similarly performed ones.",,,,ACL
466,2019,Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs,"Deepak Nathani, Jatin Chauhan, Charu Sharma, Manohar Kaul","The recent proliferation of knowledge graphs (KGs) coupled with incomplete or partial information, in the form of missing relations (links) between entities, has fueled a lot of research on knowledge base completion (also known as relation prediction). Several recent works suggest that convolutional neural network (CNN) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction. However, we observe that these KG embeddings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighborhood surrounding a triple. To this effect, our paper proposes a novel attention-based feature embedding that captures both entity and relation features in any given entity’s neighborhood. Additionally, we also encapsulate relation clusters and multi-hop relations in our model. Our empirical study offers insights into the efficacy of our attention-based model and we show marked performance gains in comparison to state-of-the-art methods on all datasets.",,,,ACL
467,2019,Neural Network Alignment for Sentential Paraphrases,"Jessica Ouyang, Kathy McKeown","We present a monolingual alignment system for long, sentence- or clause-level alignments, and demonstrate that systems designed for word- or short phrase-based alignment are ill-suited for these longer alignments. Our system is capable of aligning semantically similar spans of arbitrary length. We achieve significantly higher recall on aligning phrases of four or more words and outperform state-of-the- art aligners on the long alignments in the MSR RTE corpus.",,,,ACL
468,2019,Duality of Link Prediction and Entailment Graph Induction,"Mohammad Javad Hosseini, Shay B. Cohen, Mark Johnson, Mark Steedman","Link prediction and entailment graph induction are often treated as different problems. In this paper, we show that these two problems are actually complementary. We train a link prediction model on a knowledge graph of assertions extracted from raw text. We propose an entailment score that exploits the new facts discovered by the link prediction model, and then form entailment graphs between relations. We further use the learned entailments to predict improved link prediction scores. Our results show that the two tasks can benefit from each other. The new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores.",,,,ACL
469,2019,A Cross-Sentence Latent Variable Model for Semi-Supervised Text Sequence Matching,"Jihun Choi, Taeuk Kim, Sang-goo Lee","We present a latent variable model for predicting the relationship between a pair of text sequences. Unlike previous auto-encoding–based approaches that consider each sequence separately, our proposed framework utilizes both sequences within a single model by generating a sequence that has a given relationship with a source sequence. We further extend the cross-sentence generating framework to facilitate semi-supervised training. We also define novel semantic constraints that lead the decoder network to generate semantically plausible and diverse sequences. We demonstrate the effectiveness of the proposed model from quantitative and qualitative experiments, while achieving state-of-the-art results on semi-supervised natural language inference and paraphrase identification.",,,,ACL
470,2019,COMET: Commonsense Transformers for Automatic Knowledge Graph Construction,"Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz","We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",,,,ACL
471,2019,Detecting Subevents using Discourse and Narrative Features,"Mohammed Aldawsari, Mark Finlayson","Recognizing the internal structure of events is a challenging language processing task of great importance for text understanding. We present a supervised model for automatically identifying when one event is a subevent of another. Building on prior work, we introduce several novel features, in particular discourse and narrative features, that significantly improve upon prior state-of-the-art performance. Error analysis further demonstrates the utility of these features. We evaluate our model on the only two annotated corpora with event hierarchies: HiEve and the Intelligence Community corpus. No prior system has been evaluated on both corpora. Our model outperforms previous systems on both corpora, achieving 0.74 BLANC F1 on the Intelligence Community corpus and 0.70 F1 on the HiEve corpus, respectively a 15 and 5 percentage point improvement over previous models.",,,,ACL
472,2019,HellaSwag: Can a Machine Really Finish Your Sentence?,"Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi","Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at a piano,” a machine must select the most likely followup: “She sets her fingers on the keys.” With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical ‘Goldilocks’ zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.",,,,ACL
473,2019,Unified Semantic Parsing with Weak Supervision,"Priyanka Agrawal, Ayushi Dalmia, Parag Jain, Abhishek Bansal, Ashish Mittal","Semantic parsing over multiple knowledge bases enables a parser to exploit structural similarities of programs across the multiple domains. However, the fundamental challenge lies in obtaining high-quality annotations of (utterance, program) pairs across various domains needed for training such models. To overcome this, we propose a novel framework to build a unified multi-domain enabled semantic parser trained only with weak supervision (denotations). Weakly supervised training is particularly arduous as the program search space grows exponentially in a multi-domain setting. To solve this, we incorporate a multi-policy distillation mechanism in which we first train domain-specific semantic parsers (teachers) using weak supervision in the absence of the ground truth programs, followed by training a single unified parser (student) from the domain specific policies obtained from these teachers. The resultant semantic parser is not only compact but also generalizes better, and generates more accurate programs. It further does not require the user to provide a domain label while querying. On the standard Overnight dataset (containing multiple domains), we demonstrate that the proposed model improves performance by 20% in terms of denotation accuracy in comparison to baseline techniques.",,,,ACL
474,2019,Every Child Should Have Parents: A Taxonomy Refinement Algorithm Based on Hyperbolic Term Embeddings,"Rami Aly, Shantanu Acharya, Alexander Ossa, Arne Köhn, Chris Biemann","We introduce the use of Poincaré embeddings to improve existing state-of-the-art approaches to domain-specific taxonomy induction from text as a signal for both relocating wrong hyponym terms within a (pre-induced) taxonomy as well as for attaching disconnected terms in a taxonomy. This method substantially improves previous state-of-the-art results on the SemEval-2016 Task 13 on taxonomy extraction. We demonstrate the superiority of Poincaré embeddings over distributional semantic representations, supporting the hypothesis that they can better capture hierarchical lexical-semantic relationships than embeddings in the Euclidean space.",,,,ACL
475,2019,Learning to Rank for Plausible Plausibility,"Zhongyang Li, Tongfei Chen, Benjamin Van Durme","Researchers illustrate improvements in contextual encoding strategies via resultant performance on a battery of shared Natural Language Understanding (NLU) tasks. Many of these tasks are of a categorical prediction variety: given a conditioning context (e.g., an NLI premise), provide a label based on an associated prompt (e.g., an NLI hypothesis). The categorical nature of these tasks has led to common use of a cross entropy log-loss objective during training. We suggest this loss is intuitively wrong when applied to plausibility tasks, where the prompt by design is neither categorically entailed nor contradictory given the context. Log-loss naturally drives models to assign scores near 0.0 or 1.0, in contrast to our proposed use of a margin-based loss. Following a discussion of our intuition, we describe a confirmation study based on an extreme, synthetically curated task derived from MultiNLI. We find that a margin-based loss leads to a more plausible model of plausibility. Finally, we illustrate improvements on the Choice Of Plausible Alternative (COPA) task through this change in loss.",,,,ACL
476,2019,Generalized Tuning of Distributional Word Vectors for Monolingual and Cross-Lingual Lexical Entailment,"Goran Glavaš, Ivan Vulić","Lexical entailment (LE; also known as hyponymy-hypernymy or is-a relation) is a core asymmetric lexical relation that supports tasks like taxonomy induction and text generation. In this work, we propose a simple and effective method for fine-tuning distributional word vectors for LE. Our Generalized Lexical ENtailment model (GLEN) is decoupled from the word embedding model and applicable to any distributional vector space. Yet – unlike existing retrofitting models – it captures a general specialization function allowing for LE-tuning of the entire distributional space and not only the vectors of words seen in lexical constraints. Coupled with a multilingual embedding space, GLEN seamlessly enables cross-lingual LE detection. We demonstrate the effectiveness of GLEN in graded LE and report large improvements (over 20% in accuracy) over state-of-the-art in cross-lingual LE detection.",,,,ACL
477,2019,Attention Is (not) All You Need for Commonsense Reasoning,"Tassilo Klein, Moin Nabi","The recently introduced BERT model exhibits strong performance on several language understanding benchmarks. In this paper, we describe a simple re-implementation of BERT for commonsense reasoning. We show that the attentions produced by BERT can be directly utilized for tasks such as the Pronoun Disambiguation Problem and Winograd Schema Challenge. Our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful. Experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin. While results suggest that BERT seems to implicitly learn to establish complex relationships between entities, solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora.",,,,ACL
478,2019,A Surprisingly Robust Trick for the Winograd Schema Challenge,"Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, Thomas Lukasiewicz","The Winograd Schema Challenge (WSC) dataset WSC273 and its inference counterpart WNLI are popular benchmarks for natural language understanding and commonsense reasoning. In this paper, we show that the performance of three language models on WSC273 consistently and robustly improves when fine-tuned on a similar pronoun disambiguation problem dataset (denoted WSCR). We additionally generate a large unsupervised WSC-like dataset. By fine-tuning the BERT language model both on the introduced and on the WSCR dataset, we achieve overall accuracies of 72.5% and 74.7% on WSC273 and WNLI, improving the previous state-of-the-art solutions by 8.8% and 9.6%, respectively. Furthermore, our fine-tuned models are also consistently more accurate on the “complex” subsets of WSC273, introduced by Trichelair et al. (2018).",,,,ACL
479,2019,Coherent Comments Generation for Chinese Articles with a Graph-to-Sequence Model,"Wei Li, Jingjing Xu, Yancheng He, ShengLi Yan, Yunfang Wu","Automatic article commenting is helpful in encouraging user engagement on online news platforms. However, the news documents are usually too long for models under traditional encoder-decoder frameworks, which often results in general and irrelevant comments. In this paper, we propose to generate comments with a graph-to-sequence model that models the input news as a topic interaction graph. By organizing the article into graph structure, our model can better understand the internal structure of the article and the connection between topics, which makes it better able to generate coherent and informative comments. We collect and release a large scale news-comment corpus from a popular Chinese online news platform Tencent Kuaibao. Extensive experiment results show that our model can generate much more coherent and informative comments compared with several strong baseline models.",,,,ACL
480,2019,Interconnected Question Generation with Coreference Alignment and Conversation Flow Modeling,"Yifan Gao, Piji Li, Irwin King, Michael R. Lyu","We study the problem of generating interconnected questions in question-answering style conversations. Compared with previous works which generate questions based on a single sentence (or paragraph), this setting is different in two major aspects: (1) Questions are highly conversational. Almost half of them refer back to conversation history using coreferences. (2) In a coherent conversation, questions have smooth transitions between turns. We propose an end-to-end neural model with coreference alignment and conversation flow modeling. The coreference alignment modeling explicitly aligns coreferent mentions in conversation history with corresponding pronominal references in generated questions, which makes generated questions interconnected to conversation history. The conversation flow modeling builds a coherent conversation by starting questioning on the first few sentences in a text passage and smoothly shifting the focus to later parts. Extensive experiments show that our system outperforms several baselines and can generate highly conversational questions. The code implementation is released at https://github.com/Evan-Gao/conversaional-QG.",,,,ACL
481,2019,Cross-Lingual Training for Automatic Question Generation,"Vishwajeet Kumar, Nitish Joshi, Arijit Mukherjee, Ganesh Ramakrishnan, Preethi Jyothi","Automatic question generation (QG) is a challenging problem in natural language understanding. QG systems are typically built assuming access to a large number of training instances where each instance is a question and its corresponding answer. For a new language, such training instances are hard to obtain making the QG problem even more challenging. Using this as our motivation, we study the reuse of an available large QG dataset in a secondary language (e.g. English) to learn a QG model for a primary language (e.g. Hindi) of interest. For the primary language, we assume access to a large amount of monolingual text but only a small QG dataset. We propose a cross-lingual QG model which uses the following training regime: (i) Unsupervised pretraining of language models in both primary and secondary languages and (ii) joint supervised training for QG in both languages. We demonstrate the efficacy of our proposed approach using two different primary languages, Hindi and Chinese. Our proposed framework clearly outperforms a number of baseline models, including a fully-supervised transformer-based model trained on the QG datasets in the primary language. We also create and release a new question answering dataset for Hindi consisting of 6555 sentences.",,,,ACL
482,2019,A Hierarchical Reinforced Sequence Operation Method for Unsupervised Text Style Transfer,"Chen Wu, Xuancheng Ren, Fuli Luo, Xu Sun","Unsupervised text style transfer aims to alter text styles while preserving the content, without aligned data for supervision. Existing seq2seq methods face three challenges: 1) the transfer is weakly interpretable, 2) generated outputs struggle in content preservation, and 3) the trade-off between content and style is intractable. To address these challenges, we propose a hierarchical reinforced sequence operation method, named Point-Then-Operate (PTO), which consists of a high-level agent that proposes operation positions and a low-level agent that alters the sentence. We provide comprehensive training objectives to control the fluency, style, and content of the outputs and a mask-based inference algorithm that allows for multi-step revision based on the single-step trained agents. Experimental results on two text style transfer datasets show that our method significantly outperforms recent methods and effectively addresses the aforementioned challenges.",,,,ACL
483,2019,Handling Divergent Reference Texts when Evaluating Table-to-Text Generation,"Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das","Automatically constructed datasets for generating text from semi-structured data (tables), such as WikiBio, often contain reference texts that diverge from the information in the corresponding semi-structured data. We show that metrics which rely solely on the reference texts, such as BLEU and ROUGE, show poor correlation with human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed by Wiseman et al (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.",,,,ACL
484,2019,Unsupervised Question Answering by Cloze Translation,"Patrick Lewis, Ludovic Denoyer, Sebastian Riedel","Obtaining training data for Question Answering (QA) is time-consuming and resource-intensive, and existing QA datasets are only available for limited domains and languages. In this work, we explore to what extent high quality training data is actually required for Extractive QA, and investigate the possibility of unsupervised Extractive QA. We approach this problem by first learning to generate context, question and answer triples in an unsupervised manner, which we then use to synthesize Extractive QA training data automatically. To generate such triples, we first sample random context paragraphs from a large corpus of documents and then random noun phrases or Named Entity mentions from these paragraphs as answers. Next we convert answers in context to “fill-in-the-blank” cloze questions and finally translate them into natural questions. We propose and compare various unsupervised ways to perform cloze-to-natural question translation, including training an unsupervised NMT model using non-aligned corpora of natural questions and cloze questions as well as a rule-based approach. We find that modern QA models can learn to answer human questions surprisingly well using only synthetic training data. We demonstrate that, without using the SQuAD training data at all, our approach achieves 56.4 F1 on SQuAD v1 (64.5 F1 when the answer is a Named Entity mention), outperforming early supervised models.",,,,ACL
485,2019,MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension,"Alon Talmor, Jonathan Berant","A large number of reading comprehension (RC) datasets has been created recently, but little analysis has been done on whether they generalize to one another, and the extent to which existing datasets can be leveraged for improving performance on new ones. In this paper, we conduct such an investigation over ten RC datasets, training on one or more source RC datasets, and evaluating generalization, as well as transfer to a target RC dataset. We analyze the factors that contribute to generalization, and show that training on a source RC dataset and transferring to a target dataset substantially improves performance, even in the presence of powerful contextual representations from BERT (Devlin et al., 2019). We also find that training on multiple source RC datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new RC dataset. Following our analysis, we propose MultiQA, a BERT-based model, trained on multiple RC datasets, which leads to state-of-the-art performance on five RC datasets. We share our infrastructure for the benefit of the research community.",,,,ACL
486,2019,Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives,"Yi Tay, Shuohang Wang, Anh Tuan Luu, Jie Fu, Minh C. Phan","This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on BLEU-4 and 17% relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components.",,,,ACL
487,2019,Explain Yourself! Leveraging Language Models for Commonsense Reasoning,"Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, Richard Socher","Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.",,,,ACL
488,2019,Interpretable Question Answering on Knowledge Bases and Text,"Alona Sydorova, Nina Poerner, Benjamin Roth","Interpretability of machine learning (ML) models becomes more relevant with their increasing adoption. In this work, we address the interpretability of ML based question answering (QA) models on a combination of knowledge bases (KB) and text documents. We adapt post hoc explanation methods such as LIME and input perturbation (IP) and compare them with the self-explanatory attention mechanism of the model. For this purpose, we propose an automatic evaluation paradigm for explanation methods in the context of QA. We also conduct a study with human annotators to evaluate whether explanations help them identify better QA models. Our results suggest that IP provides better explanations than LIME or attention, according to both automatic and human evaluation. We obtain the same ranking of methods in both experiments, which supports the validity of our automatic evaluation paradigm.",,,,ACL
489,2019,A Resource-Free Evaluation Metric for Cross-Lingual Word Embeddings Based on Graph Modularity,"Yoshinari Fujinuma, Jordan Boyd-Graber, Michael J. Paul","Cross-lingual word embeddings encode the meaning of words from different languages into a shared low-dimensional space. An important requirement for many downstream tasks is that word similarity should be independent of language—i.e., word vectors within one language should not be more similar to each other than to words in another language. We measure this characteristic using modularity, a network measurement that measures the strength of clusters in a graph. Modularity has a moderate to strong correlation with three downstream tasks, even though modularity is based only on the structure of embeddings and does not require any external resources. We show through experiments that modularity can serve as an intrinsic validation metric to improve unsupervised cross-lingual word embeddings, particularly on distant language pairs in low-resource settings.",,,,ACL
490,2019,Multilingual and Cross-Lingual Graded Lexical Entailment,"Ivan Vulić, Simone Paolo Ponzetto, Goran Glavaš","Grounded in cognitive linguistics, graded lexical entailment (GR-LE) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale. In this paper, we present the first work on cross-lingual generalisation of GR-LE relation. Starting from HyperLex, the only available GR-LE dataset in English, we construct new monolingual GR-LE datasets for three other languages, and combine those to create a set of six cross-lingual GR-LE datasets termed CL-HYPERLEX. We next present a novel method dubbed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel) for effectively capturing graded (and binary) LE, both monolingually in different languages as well as across languages (i.e., on CL-HYPERLEX). Coupled with a bilingual dictionary, CLEAR leverages taxonomic LE knowledge in a resource-rich language (e.g., English) and propagates it to other languages. Supported by cross-lingual LE transfer, CLEAR sets competitive baseline performance on three new monolingual GR-LE datasets and six cross-lingual GR-LE datasets. In addition, we show that CLEAR outperforms current state-of-the-art on binary cross-lingual LE detection by a wide margin for diverse language pairs.",,,,ACL
491,2019,What Kind of Language Is Hard to Language-Model?,"Sabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian Roark, Jason Eisner","How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that “translationese” is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample.",,,,ACL
492,2019,Analyzing the Limitations of Cross-lingual Word Embedding Mappings,"Aitor Ormazabal, Mikel Artetxe, Gorka Labaka, Aitor Soroa, Eneko Agirre","Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal.",,,,ACL
493,2019,How Multilingual is Multilingual BERT?,"Telmo Pires, Eva Schlinger, Dan Garrette","In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",,,,ACL
494,2019,Bilingual Lexicon Induction through Unsupervised Machine Translation,"Mikel Artetxe, Gorka Labaka, Eneko Agirre","A recent research line has obtained strong results on bilingual lexicon induction by aligning independently trained word embeddings in two languages and using the resulting cross-lingual embeddings to induce word translation pairs through nearest neighbor or related retrieval methods. In this paper, we propose an alternative approach to this problem that builds on the recent work on unsupervised machine translation. This way, instead of directly inducing a bilingual lexicon from cross-lingual embeddings, we use them to build a phrase-table, combine it with a language model, and use the resulting machine translation system to generate a synthetic parallel corpus, from which we extract the bilingual lexicon using statistical word alignment techniques. As such, our method can work with any word embedding and cross-lingual mapping technique, and it does not require any additional resource besides the monolingual corpus used to train the embeddings. When evaluated on the exact same cross-lingual embeddings, our proposed method obtains an average improvement of 6 accuracy points over nearest neighbor and 4 points over CSLS retrieval, establishing a new state-of-the-art in the standard MUSE dataset.",,,,ACL
495,2019,Automatically Identifying Complaints in Social Media,"Daniel Preoţiuc-Pietro, Mihaela Gaman, Nikolaos Aletras","Complaining is a basic speech act regularly used in human and computer mediated communication to express a negative mismatch between reality and expectations in a particular situation. Automatically identifying complaints in social media is of utmost importance for organizations or brands to improve the customer experience or in developing dialogue systems for handling and responding to complaints. In this paper, we introduce the first systematic analysis of complaints in computational linguistics. We collect a new annotated data set of written complaints expressed on Twitter. We present an extensive linguistic analysis of complaining as a speech act in social media and train strong feature-based and neural models of complaints across nine domains achieving a predictive performance of up to 79 F1 using distant supervision.",,,,ACL
496,2019,TWEETQA: A Social Media Focused Question Answering Dataset,"Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulkarni, Mo Yu","With social media becoming increasingly popular on which lots of news and real-time events are reported, developing automated question answering systems is critical to the effective-ness of many applications that rely on real-time knowledge. While previous datasets have concentrated on question answering (QA) for formal text like news and Wikipedia, we present the first large-scale dataset for QA over social media data. To ensure that the tweets we collected are useful, we only gather tweets used by journalists to write news articles. We then ask human annotators to write questions and answers upon these tweets. Unlike otherQA datasets like SQuAD in which the answers are extractive, we allow the answers to be abstractive. We show that two recently proposed neural models that perform well on formal texts are limited in their performance when applied to our dataset. In addition, even the fine-tuned BERT model is still lagging behind human performance with a large margin. Our results thus point to the need of improved QA systems targeting social media text.",,,,ACL
497,2019,"Asking the Crowd: Question Analysis, Evaluation and Generation for Open Discussion on Online Forums","Zi Chai, Xinyu Xing, Xiaojun Wan, Bo Huang","Teaching machines to ask questions is an important yet challenging task. Most prior work focused on generating questions with fixed answers. As contents are highly limited by given answers, these questions are often not worth discussing. In this paper, we take the first step on teaching machines to ask open-answered questions from real-world news for open discussion (openQG). To generate high-qualified questions, effective ways for question evaluation are required. We take the perspective that the more answers a question receives, the better it is for open discussion, and analyze how language use affects the number of answers. Compared with other factors, e.g. topic and post time, linguistic factors keep our evaluation from being domain-specific. We carefully perform variable control on 11.5M questions from online forums to get a dataset, OQRanD, and further perform question analysis. Based on these conclusions, several models are built for question evaluation. For openQG task, we construct OQGenD, the first dataset as far as we know, and propose a model based on conditional generative adversarial networks and our question evaluation model. Experiments show that our model can generate questions with higher quality compared with commonly-used text generation methods.",,,,ACL
498,2019,Tree LSTMs with Convolution Units to Predict Stance and Rumor Veracity in Social Media Conversations,"Sumeet Kumar, Kathleen Carley","Learning from social-media conversations has gained significant attention recently because of its applications in areas like rumor detection. In this research, we propose a new way to represent social-media conversations as binarized constituency trees that allows comparing features in source-posts and their replies effectively. Moreover, we propose to use convolution units in Tree LSTMs that are better at learning patterns in features obtained from the source and reply posts. Our Tree LSTM models employ multi-task (stance + rumor) learning and propagate the useful stance signal up in the tree for rumor classification at the root node. The proposed models achieve state-of-the-art performance, outperforming the current best model by 12% and 15% on F1-macro for rumor-veracity classification and stance classification tasks respectively.",,,,ACL
499,2019,HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization,"Xingxing Zhang, Furu Wei, Ming Zhou","Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. Training the hierarchical encoder with these inaccurate labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders (Devlin et al., 2018), we propose Hibert (as shorthand for HIerachical Bidirectional Encoder Representations from Transformers) for document encoding and a method to pre-train it using unlabeled data. We apply the pre-trained Hibert to our summarization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset. We also achieve the state-of-the-art performance on these two datasets.",,,,ACL
500,2019,Hierarchical Transformers for Multi-Document Summarization,"Yang Liu, Mirella Lapata","In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.",,,,ACL
501,2019,Abstractive Text Summarization Based on Deep Learning and Semantic Content Generalization,"Panagiotis Kouris, Georgios Alexandridis, Andreas Stafylopatis","This work proposes a novel framework for enhancing abstractive text summarization based on the combination of deep learning techniques along with semantic data transformations. Initially, a theoretical model for semantic-based text generalization is introduced and used in conjunction with a deep encoder-decoder architecture in order to produce a summary in generalized form. Subsequently, a methodology is proposed which transforms the aforementioned generalized summary into human-readable form, retaining at the same time important informational aspects of the original text and addressing the problem of out-of-vocabulary or rare words. The overall approach is evaluated on two popular datasets with encouraging results.",,,,ACL
502,2019,Studying Summarization Evaluation Metrics in the Appropriate Scoring Range,Maxime Peyrard,"In summarization, automatic evaluation metrics are usually compared based on their ability to correlate with human judgments. Unfortunately, the few existing human judgment datasets have been created as by-products of the manual evaluations performed during the DUC/TAC shared tasks. However, modern systems are typically better than the best systems submitted at the time of these shared tasks. We show that, surprisingly, evaluation metrics which behave similarly on these datasets (average-scoring range) strongly disagree in the higher-scoring range in which current systems now operate. It is problematic because metrics disagree yet we can’t decide which one to trust. This is a call for collecting human judgments for high-scoring summaries as this would resolve the debate over which metrics to trust. This would also be greatly beneficial to further improve summarization systems and metrics alike.",,,,ACL
503,2019,Simple Unsupervised Summarization by Contextual Matching,"Jiawei Zhou, Alexander Rush","We propose an unsupervised method for sentence summarization using only language modeling. The approach employs two language models, one that is generic (i.e. pretrained), and the other that is specific to the target domain. We show that by using a product-of-experts criteria these are enough for maintaining continuous contextual matching while maintaining output fluency. Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data.",,,,ACL
504,2019,Generating Summaries with Topic Templates and Structured Convolutional Decoders,"Laura Perez-Beltrachini, Yang Liu, Mirella Lapata",Existing neural generation approaches create multi-sentence text as a single sequence. In this paper we propose a structured convolutional decoder that is guided by the content structure of target summaries. We compare our model with existing sequential decoders on three data sets representing different domains. Automatic and human evaluation demonstrate that our summaries have better content coverage.,,,,ACL
505,2019,Morphological Irregularity Correlates with Frequency,"Shijie Wu, Ryan Cotterell, Timothy O’Donnell","We present a study of morphological irregularity. Following recent work, we define an information-theoretic measure of irregularity based on the predictability of forms in a language. Using a neural transduction model, we estimate this quantity for the forms in 28 languages. We first present several validatory and exploratory analyses of irregularity. We then show that our analyses provide evidence for a correlation between irregularity and frequency: higher frequency items are more likely to be irregular and irregular items are more likely be highly frequent. To our knowledge, this result is the first of its breadth and confirms longstanding proposals from the linguistics literature. The correlation is more robust when aggregated at the level of whole paradigms—providing support for models of linguistic structure in which inflected forms are unified by abstract underlying stems or lexemes.",,,,ACL
506,2019,Like a Baby: Visually Situated Neural Language Acquisition,"Alexander Ororbia, Ankur Mali, Matthew Kelly, David Reitter","We examine the benefits of visual context in training neural language models to perform next-word prediction. A multi-modal neural architecture is introduced that outperform its equivalent trained on language alone with a 2% decrease in perplexity, even when no visual context is available at test. Fine-tuning the embeddings of a pre-trained state-of-the-art bidirectional language model (BERT) in the language modeling framework yields a 3.5% improvement. The advantage for training with visual context when testing without is robust across different languages (English, German and Spanish) and different models (GRU, LSTM, Delta-RNN, as well as those that use BERT embeddings). Thus, language models perform better when they learn like a baby, i.e, in a multi-modal environment. This finding is compatible with the theory of situated cognition: language is inseparable from its physical context.",,,,ACL
507,2019,Relating Simple Sentence Representations in Deep Neural Networks and the Brain,"Sharmistha Jat, Hao Tang, Partha Talukdar, Tom Mitchell","What is the relationship between sentence representations learned by deep recurrent models against those encoded by the brain? Is there any correspondence between hidden layers of these recurrent models and brain regions when processing sentences? Can these deep models be used to synthesize brain data which can then be utilized in other extrinsic tasks? We investigate these questions using sentences with simple syntax and semantics (e.g., The bone was eaten by the dog.). We consider multiple neural network architectures, including recently proposed ELMo and BERT. We use magnetoencephalography (MEG) brain recording data collected from human subjects when they were reading these simple sentences. Overall, we find that BERT’s activations correlate the best with MEG brain data. We also find that the deep network representation can be used to generate brain data from new sentences to augment existing brain data. To the best of our knowledge, this is the first work showing that the MEG brain recording when reading a word in a sentence can be used to distinguish earlier words in the sentence. Our exploration is also the first to use deep neural network representations to generate synthetic brain data and to show that it helps in improving subsequent stimuli decoding task accuracy.",,,,ACL
508,2019,Modeling Affirmative and Negated Action Processing in the Brain with Lexical and Compositional Semantic Models,"Vesna Djokic, Jean Maillard, Luana Bulat, Ekaterina Shutova","Recent work shows that distributional semantic models can be used to decode patterns of brain activity associated with individual words and sentence meanings. However, it is yet unclear to what extent such models can be used to study and decode fMRI patterns associated with specific aspects of semantic composition such as the negation function. In this paper, we apply lexical and compositional semantic models to decode fMRI patterns associated with negated and affirmative sentences containing hand-action verbs. Our results show reduced decoding (correlation) of sentences where the verb is in the negated context, as compared to the affirmative one, within brain regions implicated in action-semantic processing. This supports behavioral and brain imaging studies, suggesting that negation involves reduced access to aspects of the affirmative mental representation. The results pave the way for testing alternate semantic models of negation against human semantic processing in the brain.",,,,ACL
509,2019,Word-order Biases in Deep-agent Emergent Communication,"Rahma Chaabouni, Eugene Kharitonov, Alessandro Lazaric, Emmanuel Dupoux, Marco Baroni","Sequence-processing neural networks led to remarkable progress on many NLP tasks. As a consequence, there has been increasing interest in understanding to what extent they process language as humans do. We aim here to uncover which biases such models display with respect to “natural” word-order constraints. We train models to communicate about paths in a simple gridworld, using miniature languages that reflect or violate various natural language trends, such as the tendency to avoid redundancy or to minimize long-distance dependencies. We study how the controlled characteristics of our miniature languages affect individual learning and their stability across multiple network generations. The results draw a mixed picture. On the one hand, neural networks show a strong tendency to avoid long-distance dependencies. On the other hand, there is no clear preference for the efficient, non-redundant encoding of information that is widely attested in natural language. We thus suggest inoculating a notion of “effort” into neural networks, as a possible way to make their linguistic behavior more human-like.",,,,ACL
510,2019,NNE: A Dataset for Nested Named Entity Recognition in English Newswire,"Nicky Ringland, Xiang Dai, Ben Hachey, Sarvnaz Karimi, Cecile Paris","Named entity recognition (NER) is widely used in natural language processing applications and downstream tasks. However, most NER tools target flat annotation from popular datasets, eschewing the semantic information available in nested entity mentions. We describe NNE—a fine-grained, nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank (PTB). Our annotation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting. We hope the public release of this large dataset for English newswire will encourage development of new techniques for nested NER.",,,,ACL
511,2019,Sequence-to-Nuggets: Nested Entity Mention Detection via Anchor-Region Networks,"Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun","Sequential labeling-based NER approaches restrict each word belonging to at most one entity mention, which will face a serious problem when recognizing nested entity mentions. In this paper, we propose to resolve this problem by modeling and leveraging the head-driven phrase structures of entity mentions, i.e., although a mention can nest other mentions, they will not share the same head word. Specifically, we propose Anchor-Region Networks (ARNs), a sequence-to-nuggets architecture for nested mention detection. ARNs first identify anchor words (i.e., possible head words) of all mentions, and then recognize the mention boundaries for each anchor word by exploiting regular phrase structures. Furthermore, we also design Bag Loss, an objective function which can train ARNs in an end-to-end manner without using any anchor word annotation. Experiments show that ARNs achieve the state-of-the-art performance on three standard nested entity mention detection benchmarks.",,,,ACL
512,2019,Improving Textual Network Embedding with Global Attention via Optimal Transport,"Liqun Chen, Guoyin Wang, Chenyang Tao, Dinghan Shen, Pengyu Cheng","Constituting highly informative network embeddings is an essential tool for network analysis. It encodes network topology, along with other useful side information, into low dimensional node-based feature representations that can be exploited by statistical modeling. This work focuses on learning context-aware network embeddings augmented with text data. We reformulate the network embedding problem, and present two novel strategies to improve over traditional attention mechanisms: (i) a content-aware sparse attention module based on optimal transport; and (ii) a high-level attention parsing module. Our approach yields naturally sparse and self-normalized relational inference. It can capture long-term interactions between sequences, thus addressing the challenges faced by existing textual network embedding schemes. Extensive experiments are conducted to demonstrate our model can consistently outperform alternative state-of-the-art methods.",,,,ACL
513,2019,"Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction","Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly","While the fast-paced inception of novel tasks and new datasets helps foster active research in a community towards interesting directions, keeping track of the abundance of research activity in different areas on different datasets is likely to become increasingly difficult. The community could greatly benefit from an automatic system able to summarize scientific results, e.g., in the form of a leaderboard. In this paper we build two datasets and develop a framework (TDMS-IE) aimed at automatically extracting task, dataset, metric and score from NLP papers, towards the automatic construction of leaderboards. Experiments show that our model outperforms several baselines by a large margin. Our model is a first step towards automatic leaderboard construction, e.g., in the NLP domain.",,,,ACL
514,2019,Scaling up Open Tagging from Tens to Thousands: Comprehension Empowered Attribute Value Extraction from Product Title,"Huimin Xu, Wenting Wang, Xin Mao, Xinyu Jiang, Man Lan","Supplementing product information by extracting attribute values from title is a crucial task in e-Commerce domain. Previous studies treat each attribute only as an entity type and build one set of NER tags (e.g., BIO) for each of them, leading to a scalability issue which unfits to the large sized attribute system in real world e-Commerce. In this work, we propose a novel approach to support value extraction scaling up to thousands of attributes without losing performance: (1) We propose to regard attribute as a query and adopt only one global set of BIO tags for any attributes to reduce the burden of attribute tag or model explosion; (2) We explicitly model the semantic representations for attribute and title, and develop an attention mechanism to capture the interactive semantic relations in-between to enforce our framework to be attribute comprehensive. We conduct extensive experiments in real-life datasets. The results show that our model not only outperforms existing state-of-the-art NER tagging models, but also is robust and generates promising results for up to 8,906 attributes.",,,,ACL
515,2019,Incorporating Linguistic Constraints into Keyphrase Generation,"Jing Zhao, Yuxiang Zhang","Keyphrases, that concisely describe the high-level topics discussed in a document, are very useful for a wide range of natural language processing tasks. Though existing keyphrase generation methods have achieved remarkable performance on this task, they generate many overlapping phrases (including sub-phrases or super-phrases) of keyphrases. In this paper, we propose the parallel Seq2Seq network with the coverage attention to alleviate the overlapping phrase problem. Specifically, we integrate the linguistic constraints of keyphrase into the basic Seq2Seq network on the source side, and employ the multi-task learning framework on the target side. In addition, in order to prevent from generating overlapping phrases of keyphrases with correct syntax, we introduce the coverage vector to keep track of the attention history and to decide whether the parts of source text have been covered by existing generated keyphrases. Experimental results show that our method can outperform the state-of-the-art CopyRNN on scientific datasets, and is also more effective in news domain.",,,,ACL
516,2019,A Unified Multi-task Adversarial Learning Framework for Pharmacovigilance Mining,"Shweta Yadav, Asif Ekbal, Sriparna Saha, Pushpak Bhattacharyya","The mining of adverse drug reaction (ADR) has a crucial role in the pharmacovigilance. The traditional ways of identifying ADR are reliable but time-consuming, non-scalable and offer a very limited amount of ADR relevant information. With the unprecedented growth of information sources in the forms of social media texts (Twitter, Blogs, Reviews etc.), biomedical literature, and Electronic Medical Records (EMR), it has become crucial to extract the most pertinent ADR related information from these free-form texts. In this paper, we propose a neural network inspired multi- task learning framework that can simultaneously extract ADRs from various sources. We adopt a novel adversarial learning-based approach to learn features across multiple ADR information sources. Unlike the other existing techniques, our approach is capable to extracting fine-grained information (such as ‘Indications’, ‘Symptoms’, ‘Finding’, ‘Disease’, ‘Drug’) which provide important cues in pharmacovigilance. We evaluate our proposed approach on three publicly available real- world benchmark pharmacovigilance datasets, a Twitter dataset from PSB 2016 Social Me- dia Shared Task, CADEC corpus and Medline ADR corpus. Experiments show that our unified framework achieves state-of-the-art performance on individual tasks associated with the different benchmark datasets. This establishes the fact that our proposed approach is generic, which enables it to achieve high performance on the diverse datasets.",,,,ACL
517,2019,Quantity Tagger: A Latent-Variable Sequence Labeling Approach to Solving Addition-Subtraction Word Problems,"Yanyan Zou, Wei Lu","An arithmetic word problem typically includes a textual description containing several constant quantities. The key to solving the problem is to reveal the underlying mathematical relations (such as addition and subtraction) among quantities, and then generate equations to find solutions. This work presents a novel approach, Quantity Tagger, that automatically discovers such hidden relations by tagging each quantity with a sign corresponding to one type of mathematical operation. For each quantity, we assume there exists a latent, variable-sized quantity span surrounding the quantity token in the text, which conveys information useful for determining its sign. Empirical results show that our method achieves 5 and 8 points of accuracy gains on two datasets respectively, compared to prior approaches.",,,,ACL
518,2019,A Deep Reinforced Sequence-to-Set Model for Multi-Label Classification,"Pengcheng Yang, Fuli Luo, Shuming Ma, Junyang Lin, Xu Sun","Multi-label classification (MLC) aims to predict a set of labels for a given instance. Based on a pre-defined label order, the sequence-to-sequence (Seq2Seq) model trained via maximum likelihood estimation method has been successfully applied to the MLC task and shows powerful ability to capture high-order correlations between labels. However, the output labels are essentially an unordered set rather than an ordered sequence. This inconsistency tends to result in some intractable problems, e.g., sensitivity to the label order. To remedy this, we propose a simple but effective sequence-to-set model. The proposed model is trained via reinforcement learning, where reward feedback is designed to be independent of the label order. In this way, we can reduce the dependence of the model on the label order, as well as capture high-order correlations between labels. Extensive experiments show that our approach can substantially outperform competitive baselines, as well as effectively reduce the sensitivity to the label order.",,,,ACL
519,2019,Joint Slot Filling and Intent Detection via Capsule Neural Networks,"Chenwei Zhang, Yaliang Li, Nan Du, Wei Fan, Philip Yu","Being able to recognize words as slots and detect the intent of an utterance has been a keen issue in natural language understanding. The existing works either treat slot filling and intent detection separately in a pipeline manner, or adopt joint models which sequentially label slots while summarizing the utterance-level intent without explicitly preserving the hierarchical relationship among words, slots, and intents. To exploit the semantic hierarchy for effective modeling, we propose a capsule-based neural network model which accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema. A re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation. Experiments on two real-world datasets show the effectiveness of our model when compared with other alternative model architectures, as well as existing natural language understanding services.",,,,ACL
520,2019,Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision,"Hongliang Dai, Yangqiu Song","Lack of labeled training data is a major bottleneck for neural network based aspect and opinion term extraction on product reviews. To alleviate this problem, we first propose an algorithm to automatically mine extraction rules from existing training examples based on dependency parsing results. The mined rules are then applied to label a large amount of auxiliary data. Finally, we study training procedures to train a neural model which can learn from both the data automatically labeled by the rules and a small amount of data accurately annotated by human. Experimental results show that although the mined rules themselves do not perform well due to their limited flexibility, the combination of human annotated data and rule labeled auxiliary data can improve the neural model and allow it to achieve performance better than or comparable with the current state-of-the-art.",,,,ACL
521,2019,Cost-sensitive Regularization for Label Confusion-aware Event Detection,"Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun","In supervised event detection, most of the mislabeling occurs between a small number of confusing type pairs, including trigger-NIL pairs and sibling sub-types of the same coarse type. To address this label confusion problem, this paper proposes cost-sensitive regularization, which can force the training procedure to concentrate more on optimizing confusing type pairs. Specifically, we introduce a cost-weighted term into the training loss, which penalizes more on mislabeling between confusing label pairs. Furthermore, we also propose two estimators which can effectively measure such label confusion based on instance-level or population-level statistics. Experiments on TAC-KBP 2017 datasets demonstrate that the proposed method can significantly improve the performances of different models in both English and Chinese event detection.",,,,ACL
522,2019,Exploring Pre-trained Language Models for Event Extraction and Generation,"Sen Yang, Dawei Feng, Linbo Qiao, Zhigang Kan, Dongsheng Li","Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",,,,ACL
523,2019,Improving Open Information Extraction via Iterative Rank-Aware Learning,"Zhengbao Jiang, Pengcheng Yin, Graham Neubig","Open information extraction (IE) is the task of extracting open-domain assertions from natural language sentences. A key step in open IE is confidence modeling, ranking the extractions based on their estimated quality to adjust precision and recall of extracted assertions. We found that the extraction likelihood, a confidence measure used by current supervised open IE systems, is not well calibrated when comparing the quality of assertions extracted from different sentences. We propose an additional binary classification loss to calibrate the likelihood to make it more globally comparable, and an iterative learning process, where extractions generated by the open IE model are incrementally included as training samples to help the model learn from trial and error. Experiments on OIE2016 demonstrate the effectiveness of our method. Code and data are available at https://github.com/jzbjyb/oie_rank.",,,,ACL
524,2019,Towards Improving Neural Named Entity Recognition with Gazetteers,"Tianyu Liu, Jin-Ge Yao, Chin-Yew Lin","Most of the recently proposed neural models for named entity recognition have been purely data-driven, with a strong emphasis on getting rid of the efforts for collecting external resources or designing hand-crafted features. This could increase the chance of overfitting since the models cannot access any supervision signal beyond the small amount of annotated data, limiting their power to generalize beyond the annotated entities. In this work, we show that properly utilizing external gazetteers could benefit segmental neural NER models. We add a simple module on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results.",,,,ACL
525,2019,Span-Level Model for Relation Extraction,"Kalpit Dixit, Yaser Al-Onaizan","Relation Extraction is the task of identifying entity mention spans in raw text and then identifying relations between pairs of the entity mentions. Recent approaches for this span-level task have been token-level models which have inherent limitations. They cannot easily define and implement span-level features, cannot model overlapping entity mentions and have cascading errors due to the use of sequential decoding. To address these concerns, we present a model which directly models all possible spans and performs joint entity mention detection and relation extraction. We report a new state-of-the-art performance of 62.83 F1 (prev best was 60.49) on the ACE2005 dataset.",,,,ACL
526,2019,Enhancing Unsupervised Generative Dependency Parser with Contextual Information,"Wenjuan Han, Yong Jiang, Kewei Tu","Most of the unsupervised dependency parsers are based on probabilistic generative models that learn the joint distribution of the given sentence and its parse. Probabilistic generative models usually explicit decompose the desired dependency tree into factorized grammar rules, which lack the global features of the entire sentence. In this paper, we propose a novel probabilistic model called discriminative neural dependency model with valence (D-NDMV) that generates a sentence and its parse from a continuous latent representation, which encodes global contextual information of the generated sentence. We propose two approaches to model the latent representation: the first deterministically summarizes the representation from the sentence and the second probabilistically models the representation conditioned on the sentence. Our approach can be regarded as a new type of autoencoder model to unsupervised dependency parsing that combines the benefits of both generative and discriminative techniques. In particular, our approach breaks the context-free independence assumption in previous generative approaches and therefore becomes more expressive. Our extensive experimental results on seventeen datasets from various sources show that our approach achieves competitive accuracy compared with both generative and discriminative state-of-the-art unsupervised dependency parsers.",,,,ACL
527,2019,Neural Architectures for Nested NER through Linearization,"Jana Straková, Milan Straka, Jan Hajic","We propose two neural network architectures for nested named entity recognition (NER), a setting in which named entities may overlap and also be labeled with more than one label. We encode the nested labels using a linearized scheme. In our first proposed approach, the nested labels are modeled as multilabels corresponding to the Cartesian product of the nested labels in a standard LSTM-CRF architecture. In the second one, the nested NER is viewed as a sequence-to-sequence problem, in which the input sequence consists of the tokens and output sequence of the labels, using hard attention on the word whose label is being predicted. The proposed methods outperform the nested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and Czech CNEC. We also enrich our architectures with the recently published contextual embeddings: ELMo, BERT and Flair, reaching further improvements for the four nested entity corpora. In addition, we report flat NER state-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003 English.",,,,ACL
528,2019,Online Infix Probability Computation for Probabilistic Finite Automata,"Marco Cognetta, Yo-Sub Han, Soon Chan Kwon","Probabilistic finite automata (PFAs) are com- mon statistical language model in natural lan- guage and speech processing. A typical task for PFAs is to compute the probability of all strings that match a query pattern. An impor- tant special case of this problem is computing the probability of a string appearing as a pre- fix, suffix, or infix. These problems find use in many natural language processing tasks such word prediction and text error correction. Recently, we gave the first incremental algorithm to efficiently compute the infix probabilities of each prefix of a string (Cognetta et al., 2018). We develop an asymptotic improvement of that algorithm and solve the open problem of computing the infix probabilities of PFAs from streaming data, which is crucial when process- ing queries online and is the ultimate goal of the incremental approach.",,,,ACL
529,2019,How to Best Use Syntax in Semantic Role Labelling,"Yufei Wang, Mark Johnson, Stephen Wan, Yifang Sun, Wei Wang","There are many different ways in which external information might be used in a NLP task. This paper investigates how external syntactic information can be used most effectively in the Semantic Role Labeling (SRL) task. We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model. We show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL’05 and CoNLL’12 benchmarks.",,,,ACL
530,2019,PTB Graph Parsing with Tree Approximation,"Yoshihide Kato, Shigeki Matsubara","The Penn Treebank (PTB) represents syntactic structures as graphs due to nonlocal dependencies. This paper proposes a method that approximates PTB graph-structured representations by trees. By our approximation method, we can reduce nonlocal dependency identification and constituency parsing into single tree-based parsing. An experimental result demonstrates that our approximation method with an off-the-shelf tree-based constituency parser significantly outperforms the previous methods in nonlocal dependency identification.",,,,ACL
531,2019,Sequence Labeling Parsing by Learning across Representations,"Michalina Strzyz, David Vilares, Carlos Gómez-Rodríguez","We use parsing as sequence labeling as a common framework to learn across constituency and dependency syntactic abstractions.To do so, we cast the problem as multitask learning (MTL). First, we show that adding a parsing paradigm as an auxiliary loss consistently improves the performance on the other paradigm. Secondly, we explore an MTL sequence labeling model that parses both representations, at almost no cost in terms of performance and speed. The results across the board show that on average MTL models with auxiliary losses for constituency parsing outperform single-task ones by 1.05 F1 points, and for dependency parsing by 0.62 UAS points.",,,,ACL
532,2019,A Prism Module for Semantic Disentanglement in Name Entity Recognition,"Kun Liu, Shen Li, Daqi Zheng, Zhengdong Lu, Sheng Gao","Natural Language Processing has been perplexed for many years by the problem that multiple semantics are mixed inside a word, even with the help of context. To solve this problem, we propose a prism module to disentangle the semantic aspects of words and reduce noise at the input layer of a model. In the prism module, some words are selectively replaced with task-related semantic aspects, then these denoised word representations can be fed into downstream tasks to make them easier. Besides, we also introduce a structure to train this module jointly with the downstream model without additional data. This module can be easily integrated into the downstream model and significantly improve the performance of baselines on named entity recognition (NER) task. The ablation analysis demonstrates the rationality of the method. As a side effect, the proposed method also provides a way to visualize the contribution of each word.",,,,ACL
533,2019,Label-Agnostic Sequence Labeling by Copying Nearest Neighbors,"Sam Wiseman, Karl Stratos","Retrieve-and-edit based approaches to structured prediction, where structures associated with retrieved neighbors are edited to form new structures, have recently attracted increased interest. However, much recent work merely conditions on retrieved structures (e.g., in a sequence-to-sequence framework), rather than explicitly manipulating them. We show we can perform accurate sequence labeling by explicitly (and only) copying labels from retrieved neighbors. Moreover, because this copying is label-agnostic, we can achieve impressive performance in zero-shot sequence-labeling tasks. We additionally consider a dynamic programming approach to sequence labeling in the presence of retrieved neighbors, which allows for controlling the number of distinct (copied) segments used to form a prediction, and leads to both more interpretable and accurate predictions.",,,,ACL
534,2019,Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset,"Hannah Rashkin, Eric Michael Smith, Margaret Li, Y-Lan Boureau","One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill. While it is straightforward for humans to recognize and acknowledge others’ feelings in a conversation, this is a significant challenge for AI systems due to the paucity of suitable publicly-available datasets for training and evaluation. This work proposes a new benchmark for empathetic dialogue generation and EmpatheticDialogues, a novel dataset of 25k conversations grounded in emotional situations. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, compared to models merely trained on large-scale Internet conversation data. We also present empirical comparisons of dialogue model adaptations for empathetic responding, leveraging existing models or datasets without requiring lengthy re-training of the full model.",,,,ACL
535,2019,Know More about Each Other: Evolving Dialogue Strategy via Compound Assessment,"Siqi Bao, Huang He, Fan Wang, Rongzhong Lian, Hua Wu","In this paper, a novel Generation-Evaluation framework is developed for multi-turn conversations with the objective of letting both participants know more about each other. For the sake of rational knowledge utilization and coherent conversation flow, a dialogue strategy which controls knowledge selection is instantiated and continuously adapted via reinforcement learning. Under the deployed strategy, knowledge grounded conversations are conducted with two dialogue agents. The generated dialogues are comprehensively evaluated on aspects like informativeness and coherence, which are aligned with our objective and human instinct. These assessments are integrated as a compound reward to guide the evolution of dialogue strategy via policy gradient. Comprehensive experiments have been carried out on the publicly available dataset, demonstrating that the proposed method outperforms the other state-of-the-art approaches significantly.",,,,ACL
536,2019,Training Neural Response Selection for Task-Oriented Dialogue Systems,"Matthew Henderson, Ivan Vulić, Daniela Gerz, Iñigo Casanueva, Paweł Budzianowski","Despite their popularity in the chatbot literature, retrieval-based models have had modest impact on task-oriented dialogue systems, with the main obstacle to their application being the low-data regime of most task-oriented dialogue tasks. Inspired by the recent success of pretraining in language modelling, we propose an effective method for deploying response selection in task-oriented dialogue. To train response selection models for task-oriented dialogue tasks, we propose a novel method which: 1) pretrains the response selection model on large general-domain conversational corpora; and then 2) fine-tunes the pretrained model for the target dialogue domain, relying only on the small in-domain dataset to capture the nuances of the given dialogue domain. Our evaluation on five diverse application domains, ranging from e-commerce to banking, demonstrates the effectiveness of the proposed training method.",,,,ACL
537,2019,Collaborative Dialogue in Minecraft,"Anjali Narayan-Chen, Prashant Jayannavar, Julia Hockenmaier","We wish to develop interactive agents that can communicate with humans to collaboratively solve tasks in grounded scenarios. Since computer games allow us to simulate such tasks without the need for physical robots, we define a Minecraft-based collaborative building task in which one player (A, the Architect) is shown a target structure and needs to instruct the other player (B, the Builder) to build this structure. Both players interact via a chat interface. A can observe B but cannot place blocks. We present the Minecraft Dialogue Corpus, a collection of 509 conversations and game logs. As a first step towards our goal of developing fully interactive agents for this task, we consider the subtask of Architect utterance generation, and show how challenging it is.",,,,ACL
538,2019,Neural Response Generation with Meta-words,"Can Xu, Wei Wu, Chongyang Tao, Huang Hu, Matt Schuerman","We present open domain dialogue generation with meta-words. A meta-word is a structured record that describes attributes of a response, and thus allows us to explicitly model the one-to-many relationship within open domain dialogues and perform response generation in an explainable and controllable manner. To incorporate meta-words into generation, we propose a novel goal-tracking memory network that formalizes meta-word expression as a goal in response generation and manages the generation process to achieve the goal with a state memory panel and a state controller. Experimental results from both automatic evaluation and human judgment on two large-scale data sets indicate that our model can significantly outperform state-of-the-art generation models in terms of response relevance, response diversity, and accuracy of meta-word expression.",,,,ACL
539,2019,Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading,"Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao","Although neural conversational models are effective in learning how to produce fluent responses, their primary challenge lies in knowing what to say to make the conversation contentful and non-vacuous. We present a new end-to-end approach to contentful neural conversation that jointly models response generation and on-demand machine reading. The key idea is to provide the conversation model with relevant long-form text on the fly as a source of external knowledge. The model performs QA-style reading comprehension on this text in response to each conversational turn, thereby allowing for more focused integration of external knowledge than has been possible in prior approaches. To support further research on knowledge-grounded conversation, we introduce a new large-scale conversation dataset grounded in external web pages (2.8M turns, 7.4M sentences of grounding). Both human evaluation and automated metrics show that our approach results in more contentful responses compared to a variety of previous methods, improving both the informativeness and diversity of generated output.",,,,ACL
540,2019,Ordinal and Attribute Aware Response Generation in a Multimodal Dialogue System,"Hardik Chauhan, Mauajama Firdaus, Asif Ekbal, Pushpak Bhattacharyya","Multimodal dialogue systems have opened new frontiers in the traditional goal-oriented dialogue systems. The state-of-the-art dialogue systems are primarily based on unimodal sources, predominantly the text, and hence cannot capture the information present in the other sources such as videos, audios, images etc. With the availability of large scale multimodal dialogue dataset (MMD) (Saha et al., 2018) on the fashion domain, the visual appearance of the products is essential for understanding the intention of the user. Without capturing the information from both the text and image, the system will be incapable of generating correct and desirable responses. In this paper, we propose a novel position and attribute aware attention mechanism to learn enhanced image representation conditioned on the user utterance. Our evaluation shows that the proposed model can generate appropriate responses while preserving the position and attribute information. Experimental results also prove that our proposed approach attains superior performance compared to the baseline models, and outperforms the state-of-the-art approaches on text similarity based evaluation metrics.",,,,ACL
541,2019,Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference,"He Bai, Yu Zhou, Jiajun Zhang, Chengqing Zong","Dialogue contexts are proven helpful in the spoken language understanding (SLU) system and they are typically encoded with explicit memory representations. However, most of the previous models learn the context memory with only one objective to maximizing the SLU performance, leaving the context memory under-exploited. In this paper, we propose a new dialogue logistic inference (DLI) task to consolidate the context memory jointly with SLU in the multi-task framework. DLI is defined as sorting a shuffled dialogue session into its original logical order and shares the same memory encoder and retrieval mechanism as the SLU model. Our experimental results show that various popular contextual SLU models can benefit from our approach, and improvements are quite impressive, especially in slot filling.",,,,ACL
542,2019,Personalizing Dialogue Agents via Meta-Learning,"Andrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, Pascale Fung","Existing personalized dialogue models use human designed persona descriptions to improve dialogue consistency. Collecting such descriptions from existing dialogues is expensive and requires hand-crafted feature designs. In this paper, we propose to extend Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) to personalized dialogue learning without using any persona descriptions. Our model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions. Empirical results on Persona-chat dataset (Zhang et al., 2018) indicate that our solution outperforms non-meta-learning baselines using automatic evaluation metrics, and in terms of human-evaluated fluency and consistency.",,,,ACL
543,2019,Reading Turn by Turn: Hierarchical Attention Architecture for Spoken Dialogue Comprehension,"Zhengyuan Liu, Nancy Chen","Comprehending multi-turn spoken conversations is an emerging research area, presenting challenges different from reading comprehension of passages due to the interactive nature of information exchange from at least two speakers. Unlike passages, where sentences are often the default semantic modeling unit, in multi-turn conversations, a turn is a topically coherent unit embodied with immediately relevant context, making it a linguistically intuitive segment for computationally modeling verbal interactions. Therefore, in this work, we propose a hierarchical attention neural network architecture, combining turn-level and word-level attention mechanisms, to improve spoken dialogue comprehension performance. Experiments are conducted on a multi-turn conversation dataset, where nurses inquire and discuss symptom information with patients. We empirically show that the proposed approach outperforms standard attention baselines, achieves more efficient learning outcomes, and is more robust to lengthy and out-of-distribution test samples.",,,,ACL
544,2019,A Novel Bi-directional Interrelated Model for Joint Intent Detection and Slot Filling,"Haihong E, Peiqing Niu, Zhongfu Chen, Meina Song","A spoken language understanding (SLU) system includes two main tasks, slot filling (SF) and intent detection (ID). The joint model for the two tasks is becoming a tendency in SLU. But the bi-directional interrelated connections between the intent and slots are not established in the existing joint models. In this paper, we propose a novel bi-directional interrelated model for joint intent detection and slot filling. We introduce an SF-ID network to establish direct connections for the two tasks to help them promote each other mutually. Besides, we design an entirely new iteration mechanism inside the SF-ID network to enhance the bi-directional interrelated connections. The experimental results show that the relative improvement in the sentence-level semantic frame accuracy of our model is 3.79% and 5.42% on ATIS and Snips datasets, respectively, compared to the state-of-the-art model.",,,,ACL
545,2019,Dual Supervised Learning for Natural Language Understanding and Generation,"Shang-Yu Su, Chao-Wei Huang, Yun-Nung Chen","Natural language understanding (NLU) and natural language generation (NLG) are both critical research topics in the NLP and dialogue fields. Natural language understanding is to extract the core semantic meaning from the given utterances, while natural language generation is opposite, of which the goal is to construct corresponding sentences based on the given semantics. However, such dual relationship has not been investigated in literature. This paper proposes a novel learning framework for natural language understanding and generation on top of dual supervised learning, providing a way to exploit the duality. The preliminary experiments show that the proposed approach boosts the performance for both tasks, demonstrating the effectiveness of the dual relationship.",,,,ACL
546,2019,SUMBT: Slot-Utterance Matching for Universal and Scalable Belief Tracking,"Hwaran Lee, Jinsik Lee, Tae-Yoon Kim","In goal-oriented dialog systems, belief trackers estimate the probability distribution of slot-values at every dialog turn. Previous neural approaches have modeled domain- and slot-dependent belief trackers, and have difficulty in adding new slot-values, resulting in lack of flexibility of domain ontology configurations. In this paper, we propose a new approach to universal and scalable belief tracker, called slot-utterance matching belief tracker (SUMBT). The model learns the relations between domain-slot-types and slot-values appearing in utterances through attention mechanisms based on contextual semantic vectors. Furthermore, the model predicts slot-value labels in a non-parametric way. From our experiments on two dialog corpora, WOZ 2.0 and MultiWOZ, the proposed model showed performance improvement in comparison with slot-dependent methods and achieved the state-of-the-art joint accuracy.",,,,ACL
547,2019,Robust Zero-Shot Cross-Domain Slot Filling with Example Values,"Darsh Shah, Raghav Gupta, Amir Fayazi, Dilek Hakkani-Tur","Task-oriented dialog systems increasingly rely on deep learning-based slot filling models, usually needing extensive labeled training data for target domains. Often, however, little to no target domain training data may be available, or the training and target domain schemas may be misaligned, as is common for web forms on similar websites. Prior zero-shot slot filling models use slot descriptions to learn concepts, but are not robust to misaligned schemas. We propose utilizing both the slot description and a small number of examples of slot values, which may be easily available, to learn semantic representations of slots which are transferable across domains and robust to misaligned schemas. Our approach outperforms state-of-the-art models on two multi-domain datasets, especially in the low-data setting.",,,,ACL
548,2019,Deep Unknown Intent Detection with Margin Loss,"Ting-En Lin, Hua Xu","Identifying the unknown (novel) user intents that have never appeared in the training set is a challenging task in the dialogue system. In this paper, we present a two-stage method for detecting unknown intents. We use bidirectional long short-term memory (BiLSTM) network with the margin loss as the feature extractor. With margin loss, we can learn discriminative deep features by forcing the network to maximize inter-class variance and to minimize intra-class variance. Then, we feed the feature vectors to the density-based novelty detection algorithm, local outlier factor (LOF), to detect unknown intents. Experiments on two benchmark datasets show that our method can yield consistent improvements compared with the baseline methods.",,,,ACL
549,2019,Modeling Semantic Relationship in Multi-turn Conversations with Hierarchical Latent Variables,"Lei Shen, Yang Feng, Haolan Zhan","Multi-turn conversations consist of complex semantic structures, and it is still a challenge to generate coherent and diverse responses given previous utterances. It’s practical that a conversation takes place under a background, meanwhile, the query and response are usually most related and they are consistent in topic but also different in content. However, little work focuses on such hierarchical relationship among utterances. To address this problem, we propose a Conversational Semantic Relationship RNN (CSRR) model to construct the dependency explicitly. The model contains latent variables in three hierarchies. The discourse-level one captures the global background, the pair-level one stands for the common topic information between query and response, and the utterance-level ones try to represent differences in content. Experimental results show that our model significantly improves the quality of responses in terms of fluency, coherence, and diversity compared to baseline methods.",,,,ACL
550,2019,Rationally Reappraising ATIS-based Dialogue Systems,"Jingcheng Niu, Gerald Penn","The Air Travel Information Service (ATIS) corpus has been the most common benchmark for evaluating Spoken Language Understanding (SLU) tasks for more than three decades since it was released. Recent state-of-the-art neural models have obtained F1-scores near 98% on the task of slot filling. We developed a rule-based grammar for the ATIS domain that achieves a 95.82% F1-score on our evaluation set. In the process, we furthermore discovered numerous shortcomings in the ATIS corpus annotation, which we have fixed. This paper presents a detailed account of these shortcomings, our proposed repairs, our rule-based grammar and the neural slot-filling architectures associated with ATIS. We also rationally reappraise the motivations for choosing a neural architecture in view of this account. Fixing the annotation errors results in a relative error reduction of between 19.4 and 52% across all architectures. We nevertheless argue that neural models must play a different role in ATIS dialogues because of the latter’s lack of variety.",,,,ACL
551,2019,Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic Programming,"Caio Corro, Ivan Titov","We treat projective dependency trees as latent variables in our probabilistic model and induce them in such a way as to be beneficial for a downstream task, without relying on any direct tree supervision. Our approach relies on Gumbel perturbations and differentiable dynamic programming. Unlike previous approaches to latent tree learning, we stochastically sample global structures and our parser is fully differentiable. We illustrate its effectiveness on sentiment analysis and natural language inference tasks. We also study its properties on a synthetic structure induction task. Ablation studies emphasize the importance of both stochasticity and constraining latent structures to be projective trees.",,,,ACL
552,2019,Neural-based Chinese Idiom Recommendation for Enhancing Elegance in Essay Writing,"Yuanchao Liu, Bo Pang, Bingquan Liu","Although the proper use of idioms can enhance the elegance of writing, the active use of various expressions is a challenge because remembering idioms is difficult. In this study, we address the problem of idiom recommendation by leveraging a neural machine translation framework, in which we suppose that idioms are written with one pseudo target language. Two types of real-life datasets are collected to support this study. Experimental results show that the proposed approach achieves promising performance compared with other baseline methods.",,,,ACL
553,2019,Better Exploiting Latent Variables in Text Modeling,Canasai Kruengkrai,"We show that sampling latent variables multiple times at a gradient step helps in improving a variational autoencoder and propose a simple and effective method to better exploit these latent variables through hidden state averaging. Consistent gains in performance on two different datasets, Penn Treebank and Yahoo, indicate the generalizability of our method.",,,,ACL
554,2019,Misleading Failures of Partial-input Baselines,"Shi Feng, Eric Wallace, Jordan Boyd-Graber","Recent work establishes dataset difficulty and removes annotation artifacts via partial-input baselines (e.g., hypothesis-only model for SNLI or question-only model for VQA). A successful partial-input baseline indicates that the dataset is cheatable. But the converse is not necessarily true: failures of partial-input baselines do not mean the dataset is free of artifacts. We first design artificial datasets to illustrate how the trivial patterns that are only visible in the full input can evade any partial-input baseline. Next, we identify such artifacts in the SNLI dataset—a hypothesis-only model augmented with trivial patterns in the premise can solve 15% of previously-thought “hard” examples. Our work provides a caveat for the use and creation of partial-input baselines for datasets.",,,,ACL
555,2019,Soft Contextual Data Augmentation for Neural Machine Translation,"Fei Gao, Jinhua Zhu, Lijun Wu, Yingce Xia, Tao Qin","While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is still very limited. In this paper, we present a novel data augmentation method for neural machine translation.Different from previous augmentation methods that randomly drop, swap or replace words with other words in a sentence, we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words. More accurately, we replace the one-hot representation of a word by a distribution (provided by a language model) over the vocabulary, i.e., replacing the embedding of this word by a weighted combination of multiple semantically similar words. Since the weights of those words depend on the contextual information of the word to be replaced,the newly generated sentences capture much richer information than previous augmentation methods. Experimental results on both small scale and large scale machine translation data sets demonstrate the superiority of our method over strong baselines.",,,,ACL
556,2019,Reversing Gradients in Adversarial Domain Adaptation for Question Deduplication and Textual Entailment Tasks,"Anush Kamath, Sparsh Gupta, Vitor Carvalho","Adversarial domain adaptation has been recently proposed as an effective technique for textual matching tasks, such as question deduplication. Here we investigate the use of gradient reversal on adversarial domain adaptation to explicitly learn both shared and unshared (domain specific) representations between two textual domains. In doing so, gradient reversal learns features that explicitly compensate for domain mismatch, while still distilling domain specific knowledge that can improve target domain accuracy. We evaluate reversing gradients for adversarial adaptation on multiple domains, and demonstrate that it significantly outperforms other methods on question deduplication as well as on recognizing textual entailment (RTE) tasks, achieving up to 7% absolute boost in base model accuracy on some datasets.",,,,ACL
557,2019,Towards Integration of Statistical Hypothesis Tests into Deep Neural Networks,"Ahmad Aghaebrahimian, Mark Cieliebak","We report our ongoing work about a new deep architecture working in tandem with a statistical test procedure for jointly training texts and their label descriptions for multi-label and multi-class classification tasks. A statistical hypothesis testing method is used to extract the most informative words for each given class. These words are used as a class description for more label-aware text classification. Intuition is to help the model to concentrate on more informative words rather than more frequent ones. The model leverages the use of label descriptions in addition to the input text to enhance text classification performance. Our method is entirely data-driven, has no dependency on other sources of information than the training data, and is adaptable to different classification problems by providing appropriate training data without major hyper-parameter tuning. We trained and tested our system on several publicly available datasets, where we managed to improve the state-of-the-art on one set with a high margin and to obtain competitive results on all other ones.",,,,ACL
558,2019,Depth Growing for Neural Machine Translation,"Lijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei Gao","While very deep neural networks have shown effectiveness for computer vision and text classification applications, how to increase the network depth of the neural machine translation (NMT) models for better translation quality remains a challenging problem. Directly stacking more blocks to the NMT model results in no improvement and even drop in performance. In this work, we propose an effective two-stage approach with three specially designed components to construct deeper NMT models, which result in significant improvements over the strong Transformer baselines on WMT14 English→German and English→French translation tasks.",,,,ACL
559,2019,Generating Fluent Adversarial Examples for Natural Languages,"Huangzhao Zhang, Hao Zhou, Ning Miao, Lei Li","Efficiently building an adversarial attacker for natural language processing (NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it is difficult to make small perturbations along the direction of gradients. Secondly, the fluency of the generated examples cannot be guaranteed. In this paper, we propose MHA, which addresses both problems by performing Metropolis-Hastings sampling, whose proposal is designed with the guidance of gradients. Experiments on IMDB and SNLI show that our proposed MHAoutperforms the baseline model on attacking capability. Adversarial training with MHA also leads to better robustness and performance.",,,,ACL
560,2019,Towards Explainable NLP: A Generative Explanation Framework for Text Classification,"Hui Liu, Qingyu Yin, William Yang Wang","Building explainable systems is a critical problem in the field of Natural Language Processing (NLP), since most machine learning models provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. However, the fine-grained information (e.g. textual explanations for the labels) is often ignored, and the systems do not explicitly generate the human-readable explanations. To solve this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and fine-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time.",,,,ACL
561,2019,Combating Adversarial Misspellings with Robust Word Recognition,"Danish Pruthi, Bhuwan Dhingra, Zachary C. Lipton","To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. Our word recognition models build upon the RNN semi-character architecture, introducing several new backoff strategies for handling rare and unseen words. Trained to recognize words corrupted by random adds, drops, swaps, and keyboard mistakes, our method achieves 32% relative (and 3.3% absolute) error reduction over the vanilla semi-character model. Notably, our pipeline confers robustness on the downstream classifier, outperforming both adversarial training and off-the-shelf spell checkers. Against a BERT model fine-tuned for sentiment analysis, a single adversarially-chosen character attack lowers accuracy from 90.3% to 45.8%. Our defense restores accuracy to 75%. Surprisingly, better word recognition does not always entail greater robustness. Our analysis reveals that robustness also depends upon a quantity that we denote the sensitivity.",,,,ACL
562,2019,An Empirical Investigation of Structured Output Modeling for Graph-based Neural Dependency Parsing,"Zhisong Zhang, Xuezhe Ma, Eduard Hovy","In this paper, we investigate the aspect of structured output modeling for the state-of-the-art graph-based neural dependency parser (Dozat and Manning, 2017). With evaluations on 14 treebanks, we empirically show that global output-structured models can generally obtain better performance, especially on the metric of sentence-level Complete Match. However, probably because neural models already learn good global views of the inputs, the improvement brought by structured output modeling is modest.",,,,ACL
563,2019,Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes,"Jie Cao, Michael Tanana, Zac Imel, Eric Poitras, David Atkins","Automatically analyzing dialogue can help understand and guide behavior in domains such as counseling, where interactions are largely mediated by conversation. In this paper, we study modeling behavioral codes used to asses a psychotherapy treatment style called Motivational Interviewing (MI), which is effective for addressing substance abuse and related problems. Specifically, we address the problem of providing real-time guidance to therapists with a dialogue observer that (1) categorizes therapist and client MI behavioral codes and, (2) forecasts codes for upcoming utterances to help guide the conversation and potentially alert the therapist. For both tasks, we define neural network models that build upon recent successes in dialogue modeling. Our experiments demonstrate that our models can outperform several baselines for both tasks. We also report the results of a careful analysis that reveals the impact of the various network design tradeoffs for modeling therapy dialogue.",,,,ACL
564,2019,Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems,"Hung Le, Doyen Sahoo, Nancy Chen, Steven Hoi","Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is conducted based on visual and audio aspects of a given video, is significantly more challenging than traditional image or text-grounded dialogue systems because (1) feature space of videos span across multiple picture frames, making it difficult to obtain semantic information; and (2) a dialogue agent must perceive and process information from different modalities (audio, video, caption, etc.) to obtain a comprehensive understanding. Most existing work is based on RNNs and sequence-to-sequence architectures, which are not very effective for capturing complex long-term dependencies (like in videos). To overcome this, we propose Multimodal Transformer Networks (MTN) to encode videos and incorporate information from different modalities. We also propose query-aware attention through an auto-encoder to extract query-aware features from non-text modalities. We develop a training procedure to simulate token-level decoding to improve the quality of generated responses during inference. We get state of the art performance on Dialogue System Technology Challenge 7 (DSTC7). Our model also generalizes to another multimodal visual-grounded dialogue task, and obtains promising performance.",,,,ACL
565,2019,Target-Guided Open-Domain Conversation,"Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric Xing","Many real-world open-domain conversation applications have specific goals to achieve during open-ended chats, such as recommendation, psychotherapy, education, etc. We study the problem of imposing conversational goals on open-domain chat agents. In particular, we want a conversational system to chat naturally with human and proactively guide the conversation to a designated target subject. The problem is challenging as no public data is available for learning such a target-guided strategy. We propose a structured approach that introduces coarse-grained keywords to control the intended content of system responses. We then attain smooth conversation transition through turn-level supervised learning, and drive the conversation towards the target with discourse-level constraints. We further derive a keyword-augmented conversation dataset for the study. Quantitative and human evaluations show our system can produce meaningful and effective conversations, significantly improving over other approaches",,,,ACL
566,2019,Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good,"Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang","Developing intelligent persuasive conversational agents to change people’s opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. Based on the annotation, we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. Furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals’ demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation. Then, we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals’ personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system.",,,,ACL
567,2019,Improving Neural Conversational Models with Entropy-Based Data Filtering,"Richárd Csáky, Patrik Purgai, Gábor Recski","Current neural network-based conversational models lack diversity and generate boring responses to open-ended utterances. Priors such as persona, emotion, or topic provide additional information to dialog models to aid response generation, but annotating a dataset with priors is expensive and such annotations are rarely available. While previous methods for improving the quality of open-domain response generation focused on either the underlying model or the training objective, we present a method of filtering dialog datasets by removing generic utterances from training data using a simple entropy-based approach that does not require human supervision. We conduct extensive experiments with different variations of our method, and compare dialog models across 17 evaluation metrics to show that training on datasets filtered this way results in better conversational quality as chatbots learn to output more diverse responses.",,,,ACL
568,2019,Zero-shot Word Sense Disambiguation using Sense Definition Embeddings,"Sawan Kumar, Sharmistha Jat, Karan Saxena, Partha Talukdar","Word Sense Disambiguation (WSD) is a long-standing but open problem in Natural Language Processing (NLP). WSD corpora are typically small in size, owing to an expensive annotation process. Current supervised WSD methods treat senses as discrete labels and also resort to predicting the Most-Frequent-Sense (MFS) for words unseen during training. This leads to poor performance on rare and unseen senses. To overcome this challenge, we propose Extended WSD Incorporating Sense Embeddings (EWISE), a supervised model to perform WSD by predicting over a continuous sense embedding space as opposed to a discrete label space. This allows EWISE to generalize over both seen and unseen senses, thus achieving generalized zero-shot learning. To obtain target sense embeddings, EWISE utilizes sense definitions. EWISE learns a novel sentence encoder for sense definitions by using WordNet relations and also ConvE, a recently proposed knowledge graph embedding method. We also compare EWISE against other sentence encoders pretrained on large corpora to generate definition embeddings. EWISE achieves new state-of-the-art WSD performance.",,,,ACL
569,2019,Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation,"Daniel Loureiro, Alípio Jorge","Contextual embeddings represent a new generation of semantic representations learned from Neural Language Modelling (NLM) that addresses the issue of meaning conflation hampering traditional word embeddings. In this work, we show that contextual embeddings can be used to achieve unprecedented gains in Word Sense Disambiguation (WSD) tasks. Our approach focuses on creating sense-level embeddings with full-coverage of WordNet, and without recourse to explicit knowledge of sense distributions or task-specific modelling. As a result, a simple Nearest Neighbors (k-NN) method using our representations is able to consistently surpass the performance of previous systems using powerful neural sequencing models. We also analyse the robustness of our approach when ignoring part-of-speech and lemma features, requiring disambiguation against the full sense inventory, and revealing shortcomings to be improved. Finally, we explore applications of our sense embeddings for concept-level analyses of contextual embeddings and their respective NLMs.",,,,ACL
570,2019,Word2Sense: Sparse Interpretable Word Embeddings,"Abhishek Panigrahi, Harsha Vardhan Simhadri, Chiranjib Bhattacharyya","We present an unsupervised method to generate Word2Sense word embeddings that are interpretable — each dimension of the embedding space corresponds to a fine-grained sense, and the non-negative value of the embedding along the j-th dimension represents the relevance of the j-th sense to the word. The underlying LDA-based generative model can be extended to refine the representation of a polysemous word in a short context, allowing us to use the embedings in contextual tasks. On computational NLP tasks, Word2Sense embeddings compare well with other word embeddings generated by unsupervised methods. Across tasks such as word similarity, entailment, sense induction, and contextual interpretation, Word2Sense is competitive with the state-of-the-art method for that task. Word2Sense embeddings are at least as sparse and fast to compute as prior art.",,,,ACL
571,2019,Modeling Semantic Compositionality with Sememe Knowledge,"Fanchao Qi, Junjie Huang, Chenghao Yang, Zhiyuan Liu, Xiao Chen","Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in models. In this paper, we verify the effectiveness of sememes, the minimum semantic units of human languages, in modeling SC by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe knowledge into SC models, and employ the sememe-incorporated models in learning representations of multiword expressions, a typical task of SC. In experiments, we implement our models by incorporating knowledge from a famous sememe knowledge base HowNet and perform both intrinsic and extrinsic evaluations. Experimental results show that our models achieve significant performance boost as compared to the baseline methods without considering sememe knowledge. We further conduct quantitative analysis and case studies to demonstrate the effectiveness of applying sememe knowledge in modeling SC.All the code and data of this paper can be obtained on https://github.com/thunlp/Sememe-SC.",,,,ACL
572,2019,Predicting Humorousness and Metaphor Novelty with Gaussian Process Preference Learning,"Edwin Simpson, Erik-Lân Do Dinh, Tristan Miller, Iryna Gurevych","The inability to quantify key aspects of creative language is a frequent obstacle to natural language understanding. To address this, we introduce novel tasks for evaluating the creativeness of language—namely, scoring and ranking text by humorousness and metaphor novelty. To sidestep the difficulty of assigning discrete labels or numeric scores, we learn from pairwise comparisons between texts. We introduce a Bayesian approach for predicting humorousness and metaphor novelty using Gaussian process preference learning (GPPL), which achieves a Spearman’s ρ of 0.56 against gold using word embeddings and linguistic features. Our experiments show that given sparse, crowdsourced annotation data, ranking using GPPL outperforms best–worst scaling. We release a new dataset for evaluating humour containing 28,210 pairwise comparisons of 4,030 texts, and make our software freely available.",,,,ACL
573,2019,Empirical Linguistic Study of Sentence Embeddings,"Katarzyna Krasnowska-Kieraś, Alina Wróblewska","The purpose of the research is to answer the question whether linguistic information is retained in vector representations of sentences. We introduce a method of analysing the content of sentence embeddings based on universal probing tasks, along with the classification datasets for two contrasting languages. We perform a series of probing and downstream experiments with different types of sentence embeddings, followed by a thorough analysis of the experimental results. Aside from dependency parser-based embeddings, linguistic information is retained best in the recently proposed LASER sentence embeddings.",,,,ACL
574,2019,Probing for Semantic Classes: Diagnosing the Meaning Content of Word Embeddings,"Yadollah Yaghoobzadeh, Katharina Kann, T. J. Hazen, Eneko Agirre, Hinrich Schütze","Word embeddings typically represent different meanings of a word in a single conflated vector. Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. We present a large dataset based on manual Wikipedia annotations and word senses, where word senses from different words are related by semantic classes. This is the basis for novel diagnostic tests for an embedding’s content: we probe word embeddings for semantic classes and analyze the embedding space by classifying embeddings into semantic classes. Our main findings are: (i) Information about a sense is generally represented well in a single-vector embedding – if the sense is frequent. (ii) A classifier can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) Although rare senses are not well represented in single-vector embeddings, this does not have negative impact on an NLP application whose performance depends on frequent senses.",,,,ACL
575,2019,Deep Neural Model Inspection and Comparison via Functional Neuron Pathways,"James Fiacco, Samridhi Choudhary, Carolyn Rose","We introduce a general method for the interpretation and comparison of neural models. The method is used to factor a complex neural model into its functional components, which are comprised of sets of co-firing neurons that cut across layers of the network architecture, and which we call neural pathways. The function of these pathways can be understood by identifying correlated task level and linguistic heuristics in such a way that this knowledge acts as a lens for approximating what the network has learned to apply to its intended task. As a case study for investigating the utility of these pathways, we present an examination of pathways identified in models trained for two standard tasks, namely Named Entity Recognition and Recognizing Textual Entailment.",,,,ACL
576,2019,Collocation Classification with Unsupervised Relation Vectors,"Luis Espinosa Anke, Steven Schockaert, Leo Wanner","Lexical relation classification is the task of predicting whether a certain relation holds between a given pair of words. In this paper, we explore to which extent the current distributional landscape based on word embeddings provides a suitable basis for classification of collocations, i.e., pairs of words between which idiosyncratic lexical relations hold. First, we introduce a novel dataset with collocations categorized according to lexical functions. Second, we conduct experiments on a subset of this benchmark, comparing it in particular to the well known DiffVec dataset. In these experiments, in addition to simple word vector arithmetic operations, we also investigate the role of unsupervised relation vectors as a complementary input. While these relation vectors indeed help, we also show that lexical function classification poses a greater challenge than the syntactic and semantic relations that are typically used for benchmarks in the literature.",,,,ACL
577,2019,Corpus-based Check-up for Thesaurus,Natalia Loukachevitch,"In this paper we discuss the usefulness of applying a checking procedure to existing thesauri. The procedure is based on the analysis of discrepancies of corpus-based and thesaurus-based word similarities. We applied the procedure to more than 30 thousand words of the Russian wordnet and found some serious errors in word sense description, including inaccurate relationships and missing senses of ambiguous words.",,,,ACL
578,2019,Confusionset-guided Pointer Networks for Chinese Spelling Check,"Dingmin Wang, Yi Tay, Li Zhong","This paper proposes Confusionset-guided Pointer Networks for Chinese Spell Check (CSC) task. More concretely, our approach utilizes the off-the-shelf confusionset for guiding the character generation. To this end, our novel Seq2Seq model jointly learns to copy a correct character from an input sentence through a pointer network, or generate a character from the confusionset rather than the entire vocabulary. We conduct experiments on three human-annotated datasets, and results demonstrate that our proposed generative model outperforms all competitor models by a large margin of up to 20% F1 score, achieving state-of-the-art performance on three datasets.",,,,ACL
579,2019,Generalized Data Augmentation for Low-Resource Translation,"Mengzhou Xia, Xiang Kong, Antonios Anastasopoulos, Graham Neubig","Low-resource language pairs with a paucity of parallel data pose challenges for machine translation in terms of both adequacy and fluency. Data augmentation utilizing a large amount of monolingual data is regarded as an effective way to alleviate the problem. In this paper, we propose a general framework of data augmentation for low-resource machine translation not only using target-side monolingual data, but also by pivoting through a related high-resource language. Specifically, we experiment with a two-step pivoting method to convert high-resource data to the low-resource language, making best use of available resources to better approximate the true distribution of the low-resource language. First, we inject low-resource words into high-resource sentences through an induced bilingual dictionary. Second, we further edit the high-resource data injected with low-resource words using a modified unsupervised machine translation framework. Extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to 1.5 to 8 BLEU points compared to supervised back-translation baselines.",,,,ACL
580,2019,"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned","Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov","Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.",,,,ACL
581,2019,Better OOV Translation with Bilingual Terminology Mining,"Matthias Huck, Viktor Hangya, Alexander Fraser","Unseen words, also called out-of-vocabulary words (OOVs), are difficult for machine translation. In neural machine translation, byte-pair encoding can be used to represent OOVs, but they are still often incorrectly translated. We improve the translation of OOVs in NMT using easy-to-obtain monolingual data. We look for OOVs in the text to be translated and translate them using simple-to-construct bilingual word embeddings (BWEs). In our MT experiments we take the 5-best candidates, which is motivated by intrinsic mining experiments. Using all five of the proposed target language words as queries we mine target-language sentences. We then back-translate, forcing the back-translation of each of the five proposed target-language OOV-translation-candidates to be the original source-language OOV. We show that by using this synthetic data to fine-tune our system the translation of OOVs can be dramatically improved. In our experiments we use a system trained on Europarl and mine sentences containing medical terms from monolingual data.",,,,ACL
582,2019,Simultaneous Translation with Flexible Policy via Restricted Imitation Learning,"Baigong Zheng, Renjie Zheng, Mingbo Ma, Liang Huang","Simultaneous translation is widely useful but remains one of the most difficult tasks in NLP. Previous work either uses fixed-latency policies, or train a complicated two-staged model using reinforcement learning. We propose a much simpler single model that adds a “delay” token to the target vocabulary, and design a restricted dynamic oracle to greatly simplify training. Experiments on Chinese <-> English simultaneous translation show that our work leads to flexible policies that achieve better BLEU scores and lower latencies compared to both fixed and RL-learned policies.",,,,ACL
583,2019,Target Conditioned Sampling: Optimizing Data Selection for Multilingual Neural Machine Translation,"Xinyi Wang, Graham Neubig","To improve low-resource Neural Machine Translation (NMT) with multilingual corpus, training on the most related high-resource language only is generally more effective than us- ing all data available (Neubig and Hu, 2018). However, it remains a question whether a smart data selection strategy can further improve low-resource NMT with data from other auxiliary languages. In this paper, we seek to construct a sampling distribution over all multilingual data, so that it minimizes the training loss of the low-resource language. Based on this formulation, we propose and efficient algorithm, (TCS), which first samples a target sentence, and then conditionally samples its source sentence. Experiments show TCS brings significant gains of up to 2 BLEU improvements on three of four languages we test, with minimal training overhead.",,,,ACL
584,2019,Adversarial Learning of Privacy-Preserving Text Representations for De-Identification of Medical Records,"Max Friedrich, Arne Köhn, Gregor Wiedemann, Chris Biemann","De-identification is the task of detecting protected health information (PHI) in medical text. It is a critical step in sanitizing electronic health records (EHR) to be shared for research. Automatic de-identification classifiers can significantly speed up the sanitization process. However, obtaining a large and diverse dataset to train such a classifier that works well across many types of medical text poses a challenge as privacy laws prohibit the sharing of raw medical records. We introduce a method to create privacy-preserving shareable representations of medical text (i.e. they contain no PHI) that does not require expensive manual pseudonymization. These representations can be shared between organizations to create unified datasets for training de-identification models. Our representation allows training a simple LSTM-CRF de-identification model to an F1 score of 97.4%, which is comparable to a strong baseline that exposes private information in its representation. A robust, widely available de-identification classifier based on our representation could potentially enable studies for which de-identification would otherwise be too costly.",,,,ACL
585,2019,Merge and Label: A Novel Neural Network Architecture for Nested NER,"Joseph Fisher, Andreas Vlachos","Named entity recognition (NER) is one of the best studied tasks in natural language processing. However, most approaches are not capable of handling nested structures which are common in many applications. In this paper we introduce a novel neural network architecture that first merges tokens and/or entities into entities forming nested structures, and then labels each of them independently. Unlike previous work, our merge and label approach predicts real-valued instead of discrete segmentation structures, which allow it to combine word and nested entity embeddings while maintaining differentiability. We evaluate our approach using the ACE 2005 Corpus, where it achieves state-of-the-art F1 of 74.6, further improved with contextual embeddings (BERT) to 82.4, an overall improvement of close to 8 F1 points over previous approaches trained on the same data. Additionally we compare it against BiLSTM-CRFs, the dominant approach for flat NER structures, demonstrating that its ability to predict nested structures does not impact performance in simpler cases.",,,,ACL
586,2019,Low-resource Deep Entity Resolution with Transfer and Active Learning,"Jungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li, Lucian Popa","Entity resolution (ER) is the task of identifying different representations of the same real-world entities across databases. It is a key step for knowledge base creation and text mining. Recent adaptation of deep learning methods for ER mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records. While these methods achieve state-of-the-art performance over benchmark data, they require large amounts of labeled data, which are typically unavailable in realistic ER applications. In this paper, we develop a deep learning-based method that targets low-resource settings for ER through a novel combination of transfer learning and active learning. We design an architecture that allows us to learn a transferable model from a high-resource setting to a low-resource one. To further adapt to the target dataset, we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model. Empirical evaluation demonstrates that our method achieves comparable, if not better, performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels.",,,,ACL
587,2019,A Semi-Markov Structured Support Vector Machine Model for High-Precision Named Entity Recognition,"Ravneet Arora, Chen-Tse Tsai, Ketevan Tsereteli, Prabhanjan Kambadur, Yi Yang","Named entity recognition (NER) is the backbone of many NLP solutions. F1 score, the harmonic mean of precision and recall, is often used to select/evaluate the best models. However, when precision needs to be prioritized over recall, a state-of-the-art model might not be the best choice. There is little in literature that directly addresses training-time modifications to achieve higher precision information extraction. In this paper, we propose a neural semi-Markov structured support vector machine model that controls the precision-recall trade-off by assigning weights to different types of errors in the loss-augmented inference during training. The semi-Markov property provides more accurate phrase-level predictions, thereby improving performance. We empirically demonstrate the advantage of our model when high precision is required by comparing against strong baselines based on CRF. In our experiments with the CoNLL 2003 dataset, our model achieves a better precision-recall trade-off at various precision levels.",,,,ACL
588,2019,Using Human Attention to Extract Keyphrase from Microblog Post,"Yingyi Zhang, Chengzhi Zhang","This paper studies automatic keyphrase extraction on social media. Previous works have achieved promising results on it, but they neglect human reading behavior during keyphrase annotating. The human attention is a crucial element of human reading behavior. It reveals the relevance of words to the main topics of the target text. Thus, this paper aims to integrate human attention into keyphrase extraction models. First, human attention is represented by the reading duration estimated from eye-tracking corpus. Then, we merge human attention with neural network models by an attention mechanism. In addition, we also integrate human attention into unsupervised models. To the best of our knowledge, we are the first to utilize human attention on keyphrase extraction tasks. The experimental results show that our models have significant improvements on two Twitter datasets.",,,,ACL
589,2019,Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision,"Abiola Obamuyide, Andreas Vlachos","In this paper we frame the task of supervised relation classification as an instance of meta-learning. We propose a model-agnostic meta-learning protocol for training relation classifiers to achieve enhanced predictive performance in limited supervision settings. During training, we aim to not only learn good parameters for classifying relations with sufficient supervision, but also learn model parameters that can be fine-tuned to enhance predictive performance for relations with limited supervision. In experiments conducted on two relation classification datasets, we demonstrate that the proposed meta-learning approach improves the predictive performance of two state-of-the-art supervised relation classification models.",,,,ACL
590,2019,Variational Pretraining for Semi-supervised Text Classification,"Suchin Gururangan, Tam Dang, Dallas Card, Noah A. Smith","We introduce VAMPIRE, a lightweight pretraining framework for effective text classification when data and computing resources are limited. We pretrain a unigram document model as a variational autoencoder on in-domain, unlabeled data and use its internal states as features in a downstream classifier. Empirically, we show the relative strength of VAMPIRE against computationally expensive contextual embeddings and other popular semi-supervised baselines under low resource settings. We also find that fine-tuning to in-domain data is crucial to achieving decent performance from contextual embeddings when working with limited supervision. We accompany this paper with code to pretrain and use VAMPIRE embeddings in downstream tasks.",,,,ACL
591,2019,Task Refinement Learning for Improved Accuracy and Stability of Unsupervised Domain Adaptation,"Yftah Ziser, Roi Reichart","Pivot Based Language Modeling (PBLM) (Ziser and Reichart, 2018a), combining LSTMs with pivot-based methods, has yielded significant progress in unsupervised domain adaptation. However, this approach is still challenged by the large pivot detection problem that should be solved, and by the inherent instability of LSTMs. In this paper we propose a Task Refinement Learning (TRL) approach, in order to solve these problems. Our algorithms iteratively train the PBLM model, gradually increasing the information exposed about each pivot. TRL-PBLM achieves stateof- the-art accuracy in six domain adaptation setups for sentiment classification. Moreover, it is much more stable than plain PBLM across model configurations, making the model much better fitted for practical use.",,,,ACL
592,2019,Optimal Transport-based Alignment of Learned Character Representations for String Similarity,"Derek Tam, Nicholas Monath, Ari Kobren, Aaron Traylor, Rajarshi Das","String similarity models are vital for record linkage, entity resolution, and search. In this work, we present STANCE–a learned model for computing the similarity of two strings. Our approach encodes the characters of each string, aligns the encodings using Sinkhorn Iteration (alignment is posed as an instance of optimal transport) and scores the alignment with a convolutional neural network. We evaluate STANCE’s ability to detect whether two strings can refer to the same entity–a task we term alias detection. We construct five new alias detection datasets (and make them publicly available). We show that STANCE (or one of its variants) outperforms both state-of-the-art and classic, parameter-free similarity models on four of the five datasets. We also demonstrate STANCE’s ability to improve downstream tasks by applying it to an instance of cross-document coreference and show that it leads to a 2.8 point improvement in Bˆ3 F1 over the previous state-of-the-art approach.",,,,ACL
593,2019,The Referential Reader: A Recurrent Entity Network for Anaphora Resolution,"Fei Liu, Luke Zettlemoyer, Jacob Eisenstein","We present a new architecture for storing and accessing entity mentions during online text processing. While reading the text, entity references are identified, and may be stored by either updating or overwriting a cell in a fixed-length memory. The update operation implies coreference with the other mentions that are stored in the same cell; the overwrite operation causes these mentions to be forgotten. By encoding the memory operations as differentiable gates, it is possible to train the model end-to-end, using both a supervised anaphora resolution objective as well as a supplementary language modeling objective. Evaluation on a dataset of pronoun-name anaphora demonstrates strong performance with purely incremental text processing.",,,,ACL
594,2019,Interpolated Spectral NGram Language Models,"Ariadna Quattoni, Xavier Carreras","Spectral models for learning weighted non-deterministic automata have nice theoretical and algorithmic properties. Despite this, it has been challenging to obtain competitive results in language modeling tasks, for two main reasons. First, in order to capture long-range dependencies of the data, the method must use statistics from long substrings, which results in very large matrices that are difficult to decompose. The second is that the loss function behind spectral learning, based on moment matching, differs from the probabilistic metrics used to evaluate language models. In this work we employ a technique for scaling up spectral learning, and use interpolated predictions that are optimized to maximize perplexity. Our experiments in character-based language modeling show that our method matches the performance of state-of-the-art ngram models, while being very fast to train.",,,,ACL
595,2019,BAM! Born-Again Multi-Task Networks for Natural Language Understanding,"Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, Quoc V. Le","It can be challenging to train multi-task neural networks that outperform or even match their single-task counterparts. To help address this, we propose using knowledge distillation where single-task models teach a multi-task model. We enhance this training with teacher annealing, a novel method that gradually transitions the model from distillation to supervised learning, helping the multi-task model surpass its single-task teachers. We evaluate our approach by multi-task fine-tuning BERT on the GLUE benchmark. Our method consistently improves over standard single-task and multi-task training.",,,,ACL
596,2019,Curate and Generate: A Corpus and Method for Joint Control of Semantics and Style in Neural NLG,"Shereen Oraby, Vrindavan Harrison, Abteen Ebrahimi, Marilyn Walker","Neural natural language generation (NNLG) from structured meaning representations has become increasingly popular in recent years. While we have seen progress with generating syntactically correct utterances that preserve semantics, various shortcomings of NNLG systems are clear: new tasks require new training data which is not available or straightforward to acquire, and model outputs are simple and may be dull and repetitive. This paper addresses these two critical challenges in NNLG by: (1) scalably (and at no cost) creating training datasets of parallel meaning representations and reference texts with rich style markup by using data from freely available and naturally descriptive user reviews, and (2) systematically exploring how the style markup enables joint control of semantic and stylistic aspects of neural model output. We present YelpNLG, a corpus of 300,000 rich, parallel meaning representations and highly stylistically varied reference texts spanning different restaurant attributes, and describe a novel methodology that can be scalably reused to generate NLG datasets for other domains. The experiments show that the models control important aspects, including lexical choice of adjectives, output length, and sentiment, allowing the models to successfully hit multiple style targets without sacrificing semantics.",,,,ACL
597,2019,Automated Chess Commentator Powered by Neural Chess Engine,"Hongyu Zang, Zhiwei Yu, Xiaojun Wan","In this paper, we explore a new approach for automated chess commentary generation, which aims to generate chess commentary texts in different categories (e.g., description, comparison, planning, etc.). We introduce a neural chess engine into text generation models to help with encoding boards, predicting moves, and analyzing situations. By jointly training the neural chess engine and the generation models for different categories, the models become more effective. We conduct experiments on 5 categories in a benchmark Chess Commentary dataset and achieve inspiring results in both automatic and human evaluations.",,,,ACL
598,2019,Barack’s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling,"Robert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, Sameer Singh","Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark. In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language model’s ability to complete sentences requiring factual knowledge, showing that the KGLM outperforms even very large language models in generating facts.",,,,ACL
599,2019,Controllable Paraphrase Generation with a Syntactic Exemplar,"Mingda Chen, Qingming Tang, Sam Wiseman, Kevin Gimpel","Prior work on controllable text generation usually assumes that the controlled attribute can take on one of a small set of values known a priori. In this work, we propose a novel task, where the syntax of a generated sentence is controlled rather by a sentential exemplar. To evaluate quantitatively with standard metrics, we create a novel dataset with human annotations. We also develop a variational model with a neural module specifically designed for capturing syntactic knowledge and several multitask training objectives to promote disentangled representation learning. Empirically, the proposed model is observed to achieve improvements over baselines and learn to capture desirable characteristics.",,,,ACL
600,2019,Towards Comprehensive Description Generation from Factual Attribute-value Tables,"Tianyu Liu, Fuli Luo, Pengcheng Yang, Wei Wu, Baobao Chang","The comprehensive descriptions for factual attribute-value tables, which should be accurate, informative and loyal, can be very helpful for end users to understand the structured data in this form. However previous neural generators might suffer from key attributes missing, less informative and groundless information problems, which impede the generation of high-quality comprehensive descriptions for tables. To relieve these problems, we first propose force attention (FA) method to encourage the generator to pay more attention to the uncovered attributes to avoid potential key attributes missing. Furthermore, we propose reinforcement learning for information richness to generate more informative as well as more loyal descriptions for tables. In our experiments, we utilize the widely used WIKIBIO dataset as a benchmark. Besides, we create WB-filter based on WIKIBIO to test our model in the simulated user-oriented scenarios, in which the generated descriptions should accord with particular user interests. Experimental results show that our model outperforms the state-of-the-art baselines on both automatic and human evaluation.",,,,ACL
601,2019,Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation,"Ning Dai, Jianze Liang, Xipeng Qiu, Xuanjing Huang","Disentangling the content and style in the latent space is prevalent in unpaired text style transfer. However, two major issues exist in most of the current neural models. 1) It is difficult to completely strip the style information from the semantics for a sentence. 2) The recurrent neural network (RNN) based encoder and decoder, mediated by the latent representation, cannot well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content. In this paper, we propose the Style Transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation.",,,,ACL
602,2019,Generating Sentences from Disentangled Syntactic and Semantic Spaces,"Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou","Variational auto-encoders (VAEs) are widely used in natural language generation due to the regularization of the latent space. However, generating sentences from the continuous latent space does not explicitly model the syntactic information. In this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. Our proposed method explicitly models syntactic information in the VAE’s latent space by using the linearized tree sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntax transfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work.",,,,ACL
603,2019,Learning to Control the Fine-grained Sentiment for Story Ending Generation,"Fuli Luo, Damai Dai, Pengcheng Yang, Tianyu Liu, Baobao Chang","Automatic story ending generation is an interesting and challenging task in natural language generation. Previous studies are mainly limited to generate coherent, reasonable and diversified story endings, and few works focus on controlling the sentiment of story endings. This paper focuses on generating a story ending which meets the given fine-grained sentiment intensity. There are two major challenges to this task. First is the lack of story corpus which has fine-grained sentiment labels. Second is the difficulty of explicitly controlling sentiment intensity when generating endings. Therefore, we propose a generic and novel framework which consists of a sentiment analyzer and a sentimental generator, respectively addressing the two challenges. The sentiment analyzer adopts a series of methods to acquire sentiment intensities of the story dataset. The sentimental generator introduces the sentiment intensity into decoder via a Gaussian Kernel Layer to control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.",,,,ACL
604,2019,Self-Attention Architectures for Answer-Agnostic Neural Question Generation,"Thomas Scialom, Benjamin Piwowarski, Jacopo Staiano","Neural architectures based on self-attention, such as Transformers, recently attracted interest from the research community, and obtained significant improvements over the state of the art in several tasks. We explore how Transformers can be adapted to the task of Neural Question Generation without constraining the model to focus on a specific answer passage. We study the effect of several strategies to deal with out-of-vocabulary words such as copy mechanisms, placeholders, and contextual word embeddings. We report improvements obtained over the state-of-the-art on the SQuAD dataset according to automated metrics (BLEU, ROUGE), as well as qualitative human assessments of the system outputs.",,,,ACL
605,2019,Unsupervised Paraphrasing without Translation,"Aurko Roy, David Grangier","Paraphrasing is an important task demonstrating the ability to abstract semantic content from its surface form. Recent literature on automatic paraphrasing is dominated by methods leveraging machine translation as an intermediate step. This contrasts with humans, who can paraphrase without necessarily being bilingual. This work proposes to learn paraphrasing models only from a monolingual corpus. To that end, we propose a residual variant of vector-quantized variational auto-encoder. Our experiments consider paraphrase identification, and paraphrasing for training set augmentation, comparing to supervised and unsupervised translation-based approaches. Monolingual paraphrasing is shown to outperform unsupervised translation in all contexts. The comparison with supervised MT is more mixed: monolingual paraphrasing is interesting for identification and augmentation but supervised MT is superior for generation.",,,,ACL
606,2019,Storyboarding of Recipes: Grounded Contextual Generation,"Khyathi Chandu, Eric Nyberg, Alan W Black","Information need of humans is essentially multimodal in nature, enabling maximum exploitation of situated context. We introduce a dataset for sequential procedural (how-to) text generation from images in cooking domain. The dataset consists of 16,441 cooking recipes with 160,479 photos associated with different steps. We setup a baseline motivated by the best performing model in terms of human evaluation for the Visual Story Telling (ViST) task. In addition, we introduce two models to incorporate high level structure learnt by a Finite State Machine (FSM) in neural sequential generation process by: (1) Scaffolding Structure in Decoder (SSiD) (2) Scaffolding Structure in Loss (SSiL). Our best performing model (SSiL) achieves a METEOR score of 0.31, which is an improvement of 0.6 over the baseline model. We also conducted human evaluation of the generated grounded recipes, which reveal that 61% found that our proposed (SSiL) model is better than the baseline model in terms of overall recipes. We also discuss analysis of the output highlighting key important NLP issues for prospective directions.",,,,ACL
607,2019,Negative Lexically Constrained Decoding for Paraphrase Generation,Tomoyuki Kajiwara,"Paraphrase generation can be regarded as monolingual translation. Unlike bilingual machine translation, paraphrase generation rewrites only a limited portion of an input sentence. Hence, previous methods based on machine translation often perform conservatively to fail to make necessary rewrites. To solve this problem, we propose a neural model for paraphrase generation that first identifies words in the source sentence that should be paraphrased. Then, these words are paraphrased by the negative lexically constrained decoding that avoids outputting these words as they are. Experiments on text simplification and formality transfer show that our model improves the quality of paraphrasing by making necessary rewrites to an input sentence.",,,,ACL
608,2019,Large-Scale Transfer Learning for Natural Language Generation,"Sergey Golovanov, Rauf Kurbanov, Sergey Nikolenko, Kyryl Truskovskyi, Alexander Tselousov","Large-scale pretrained language models define state of the art in natural language processing, achieving outstanding performance on a variety of tasks. We study how these architectures can be applied and adapted for natural language generation, comparing a number of architectural and training schemes. We focus in particular on open-domain dialog as a typical high entropy generation task, presenting and comparing different architectures for adapting pretrained models with state of the art results.",,,,ACL
609,2019,Automatic Grammatical Error Correction for Sequence-to-sequence Text Generation: An Empirical Study,"Tao Ge, Xingxing Zhang, Furu Wei, Ming Zhou","Sequence-to-sequence (seq2seq) models have achieved tremendous success in text generation tasks. However, there is no guarantee that they can always generate sentences without grammatical errors. In this paper, we present a preliminary empirical study on whether and how much automatic grammatical error correction can help improve seq2seq text generation. We conduct experiments across various seq2seq text generation tasks including machine translation, formality style transfer, sentence compression and simplification. Experiments show the state-of-the-art grammatical error correction system can improve the grammaticality of generated text and can bring task-oriented improvements in the tasks where target sentences are in a formal style.",,,,ACL
610,2019,Improving the Robustness of Question Answering Systems to Question Paraphrasing,"Wee Chung Gan, Hwee Tou Ng","Despite the advancement of question answering (QA) systems and rapid improvements on held-out test sets, their generalizability is a topic of concern. We explore the robustness of QA models to question paraphrasing by creating two test sets consisting of paraphrased SQuAD questions. Paraphrased questions from the first test set are very similar to the original questions designed to test QA models’ over-sensitivity, while questions from the second test set are paraphrased using context words near an incorrect answer candidate in an attempt to confuse QA models. We show that both paraphrased test sets lead to significant decrease in performance on multiple state-of-the-art QA models. Using a neural paraphrasing model trained to generate multiple paraphrased questions for a given source question and a set of paraphrase suggestions, we propose a data augmentation approach that requires no human intervention to re-train the models for improved robustness to question paraphrasing.",,,,ACL
611,2019,RankQA: Neural Question Answering with Answer Re-Ranking,"Bernhard Kratzwald, Anna Eigenmann, Stefan Feuerriegel","The conventional paradigm in neural question answering (QA) for narrative content is limited to a two-stage process: first, relevant text passages are retrieved and, subsequently, a neural network for machine comprehension extracts the likeliest answer. However, both stages are largely isolated in the status quo and, hence, information from the two phases is never properly fused. In contrast, this work proposes RankQA: RankQA extends the conventional two-stage process in neural QA with a third stage that performs an additional answer re-ranking. The re-ranking leverages different features that are directly extracted from the QA pipeline, i.e., a combination of retrieval and comprehension features. While our intentionally simple design allows for an efficient, data-sparse estimation, it nevertheless outperforms more complex QA systems by a significant margin: in fact, RankQA achieves state-of-the-art performance on 3 out of 4 benchmark datasets. Furthermore, its performance is especially superior in settings where the size of the corpus is dynamic. Here the answer re-ranking provides an effective remedy against the underlying noise-information trade-off due to a variable corpus size. As a consequence, RankQA represents a novel, powerful, and thus challenging baseline for future research in content-based QA.",,,,ACL
612,2019,Latent Retrieval for Weakly Supervised Open Domain Question Answering,"Kenton Lee, Ming-Wei Chang, Kristina Toutanova","Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.",,,,ACL
613,2019,Multi-hop Reading Comprehension through Question Decomposition and Rescoring,"Sewon Min, Victor Zhong, Luke Zettlemoyer, Hannaneh Hajishirzi","Multi-hop Reading Comprehension (RC) requires reasoning and aggregation across several paragraphs. We propose a system for multi-hop RC that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models. Since annotations for such decomposition are expensive, we recast subquestion generation as a span prediction problem and show that our method, trained using only 400 labeled examples, generates sub-questions that are as effective as human-authored sub-questions. We also introduce a new global rescoring approach that considers each decomposition (i.e. the sub-questions and their answers) to select the best final answer, greatly improving overall performance. Our experiments on HotpotQA show that this approach achieves the state-of-the-art results, while providing explainable evidence for its decision making in the form of sub-questions.",,,,ACL
614,2019,Combining Knowledge Hunting and Neural Language Models to Solve the Winograd Schema Challenge,"Ashok Prakash, Arpit Sharma, Arindam Mitra, Chitta Baral","Winograd Schema Challenge (WSC) is a pronoun resolution task which seems to require reasoning with commonsense knowledge. The needed knowledge is not present in the given text. Automatic extraction of the needed knowledge is a bottleneck in solving the challenge. The existing state-of-the-art approach uses the knowledge embedded in their pre-trained language model. However, the language models only embed part of the knowledge, the ones related to frequently co-existing concepts. This limits the performance of such models on the WSC problems. In this work, we build-up on the language model based methods and augment them with a commonsense knowledge hunting (using automatic extraction from text) module and an explicit reasoning module. Our end-to-end system built in such a manner improves on the accuracy of two of the available language model based approaches by 5.53% and 7.7% respectively. Overall our system achieves the state-of-the-art accuracy of 71.06% on the WSC dataset, an improvement of 7.36% over the previous best.",,,,ACL
615,2019,Careful Selection of Knowledge to Solve Open Book Question Answering,"Pratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra, Chitta Baral","Open book question answering is a type of natural language based QA (NLQA) where questions are expected to be answered with respect to a given set of open book facts, and common knowledge about a topic. Recently a challenge involving such QA, OpenBookQA, has been proposed. Unlike most other NLQA that focus on linguistic understanding, OpenBookQA requires deeper reasoning involving linguistic understanding as well as reasoning with common knowledge. In this paper we address QA with respect to the OpenBookQA dataset and combine state of the art language models with abductive information retrieval (IR), information gain based re-ranking, passage selection and weighted scoring to achieve 72.0% accuracy, an 11.6% improvement over the current state of the art.",,,,ACL
616,2019,Learning Representation Mapping for Relation Detection in Knowledge Base Question Answering,"Peng Wu, Shujian Huang, Rongxiang Weng, Zaixiang Zheng, Jianbing Zhang","Relation detection is a core step in many natural language process applications including knowledge base question answering. Previous efforts show that single-fact questions could be answered with high accuracy. However, one critical problem is that current approaches only get high accuracy for questions whose relations have been seen in the training data. But for unseen relations, the performance will drop rapidly. The main reason for this problem is that the representations for unseen relations are missing. In this paper, we propose a simple mapping method, named representation adapter, to learn the representation mapping for both seen and unseen relations based on previously learned relation embedding. We employ the adversarial objective and the reconstruction objective to improve the mapping performance. We re-organize the popular SimpleQuestion dataset to reveal and evaluate the problem of detecting unseen relations. Experiments show that our method can greatly improve the performance of unseen relations while the performance for those seen part is kept comparable to the state-of-the-art.",,,,ACL
617,2019,Dynamically Fused Graph Network for Multi-hop Reasoning,"Lin Qiu, Yunxuan Xiao, Yanru Qu, Hao Zhou, Lei Li","Text-based question answering (TBQA) has been studied extensively in recent years. Most existing approaches focus on finding the answer to a question within a single paragraph. However, many difficult questions require multiple supporting evidence from scattered text among two or more documents. In this paper, we propose Dynamically Fused Graph Network (DFGN), a novel method to answer those questions requiring multiple scattered evidence and reasoning over them. Inspired by human’s step-by-step reasoning behavior, DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query, explores along the entity graph dynamically built from the text, and gradually finds relevant supporting entities from the given documents. We evaluate DFGN on HotpotQA, a public TBQA dataset requiring multi-hop reasoning. DFGN achieves competitive results on the public board. Furthermore, our analysis shows DFGN produces interpretable reasoning chains.",,,,ACL
618,2019,NLProlog: Reasoning with Weak Unification for Question Answering in Natural Language,"Leon Weber, Pasquale Minervini, Jannes Münchmeyer, Ulf Leser, Tim Rocktäschel","Rule-based models are attractive for various tasks because they inherently lead to interpretable and explainable decisions and can easily incorporate prior knowledge. However, such systems are difficult to apply to problems involving natural language, due to its large linguistic variability. In contrast, neural models can cope very well with ambiguity by learning distributed representations of words and their composition from data, but lead to models that are difficult to interpret. In this paper, we describe a model combining neural networks with logic programming in a novel manner for solving multi-hop reasoning tasks over natural language. Specifically, we propose to use an Prolog prover which we extend to utilize a similarity function over pretrained sentence encoders. We fine-tune the representations for the similarity function via backpropagation. This leads to a system that can apply rule-based reasoning to natural language, and induce domain-specific natural language rules from training data. We evaluate the proposed system on two different question answering tasks, showing that it outperforms two baselines – BiDAF (Seo et al., 2016a) and FastQA( Weissenborn et al., 2017) on a subset of the WikiHop corpus and achieves competitive results on the MedHop data set (Welbl et al., 2017).",,,,ACL
619,2019,Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions,"Jierui Li, Lei Wang, Jipeng Zhang, Yan Wang, Bing Tian Dai","Several deep learning models have been proposed for solving math word problems (MWPs) automatically. Although these models have the ability to capture features without manual efforts, their approaches to capturing features are not specifically designed for MWPs. To utilize the merits of deep learning models with simultaneous consideration of MWPs’ specific features, we propose a group attention mechanism to extract global features, quantity-related features, quantity-pair features and question-related features in MWPs respectively. The experimental results show that the proposed approach performs significantly better than previous state-of-the-art methods, and boost performance from 66.9% to 69.5% on Math23K with training-test split, from 65.8% to 66.9% on Math23K with 5-fold cross-validation and from 69.2% to 76.1% on MAWPS.",,,,ACL
620,2019,Synthetic QA Corpora Generation with Roundtrip Consistency,"Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, Michael Collins","We introduce a novel method of generating synthetic question answering corpora by combining models of question generation and answer extraction, and by filtering the results to ensure roundtrip consistency. By pretraining on the resulting corpora we obtain significant improvements on SQuAD2 and NQ, establishing a new state-of-the-art on the latter. Our synthetic data generation models, for both question generation and answer extraction, can be fully reproduced by finetuning a publicly available BERT model on the extractive subsets of SQuAD2 and NQ. We also describe a more powerful variant that does full sequence-to-sequence pretraining for question generation, obtaining exact match and F1 at less than 0.1% and 0.4% from human performance on SQuAD2.",,,,ACL
621,2019,Are Red Roses Red? Evaluating Consistency of Question-Answering Models,"Marco Tulio Ribeiro, Carlos Guestrin, Sameer Singh","Although current evaluation of question-answering systems treats predictions in isolation, we need to consider the relationship between predictions to measure true understanding. A model should be penalized for answering “no” to “Is the rose red?” if it answers “red” to “What color is the rose?”. We propose a method to automatically extract such implications for instances from two QA datasets, VQA and SQuAD, which we then use to evaluate the consistency of models. Human evaluation shows these generated implications are well formed and valid. Consistency evaluation provides crucial insights into gaps in existing models, while retraining with implication-augmented data improves consistency on both synthetic and human-generated implications.",,,,ACL
622,2019,MCˆ2: Multi-perspective Convolutional Cube for Conversational Machine Reading Comprehension,Xuanyu Zhang,"Conversational machine reading comprehension (CMRC) extends traditional single-turn machine reading comprehension (MRC) by multi-turn interactions, which requires machines to consider the history of conversation. Most of models simply combine previous questions for conversation understanding and only employ recurrent neural networks (RNN) for reasoning. To comprehend context profoundly and efficiently from different perspectives, we propose a novel neural network model, Multi-perspective Convolutional Cube (MCˆ2). We regard each conversation as a cube. 1D and 2D convolutions are integrated with RNN in our model. To avoid models previewing the next turn of conversation, we also extend causal convolution partially to 2D. Experiments on the Conversational Question Answering (CoQA) dataset show that our model achieves state-of-the-art results.",,,,ACL
623,2019,Reducing Word Omission Errors in Neural Machine Translation: A Contrastive Learning Approach,"Zonghan Yang, Yong Cheng, Yang Liu, Maosong Sun","While neural machine translation (NMT) has achieved remarkable success, NMT systems are prone to make word omission errors. In this work, we propose a contrastive learning approach to reducing word omission errors in NMT. The basic idea is to enable the NMT model to assign a higher probability to a ground-truth translation and a lower probability to an erroneous translation, which is automatically constructed from the ground-truth translation by omitting words. We design different types of negative examples depending on the number of omitted words, word frequency, and part of speech. Experiments on Chinese-to-English, German-to-English, and Russian-to-English translation tasks show that our approach is effective in reducing word omission errors and achieves better translation performance than three baseline methods.",,,,ACL
624,2019,Exploiting Sentential Context for Neural Machine Translation,"Xing Wang, Zhaopeng Tu, Longyue Wang, Shuming Shi","In this work, we present novel approaches to exploit sentential context for neural machine translation (NMT). Specifically, we show that a shallow sentential context extracted from the top encoder layer only, can improve translation performance via contextualizing the encoding representations of individual words. Next, we introduce a deep sentential context, which aggregates the sentential context representations from all of the internal layers of the encoder to form a more comprehensive context representation. Experimental results on the WMT14 English-German and English-French benchmarks show that our model consistently improves performance over the strong Transformer model, demonstrating the necessity and effectiveness of exploiting sentential context for NMT.",,,,ACL
625,2019,Wetin dey with these comments? Modeling Sociolinguistic Factors Affecting Code-switching Behavior in Nigerian Online Discussions,"Innocent Ndubuisi-Obi, Sayan Ghosh, David Jurgens","Multilingual individuals code switch between languages as a part of a complex communication process. However, most computational studies have examined only one or a handful of contextual factors predictive of switching. Here, we examine Naija-English code switching in a rich contextual environment to understand the social and topical factors eliciting a switch. We introduce a new corpus of 330K articles and accompanying 389K comments labeled for code switching behavior. In modeling whether a comment will switch, we show that topic-driven variation, tribal affiliation, emotional valence, and audience design all play complementary roles in behavior.",,,,ACL
626,2019,Accelerating Sparse Matrix Operations in Neural Networks on Graphics Processing Units,"Arturo Argueta, David Chiang","Graphics Processing Units (GPUs) are commonly used to train and evaluate neural networks efficiently. While previous work in deep learning has focused on accelerating operations on dense matrices/tensors on GPUs, efforts have concentrated on operations involving sparse data structures. Operations using sparse structures are common in natural language models at the input and output layers, because these models operate on sequences over discrete alphabets. We present two new GPU algorithms: one at the input layer, for multiplying a matrix by a few-hot vector (generalizing the more common operation of multiplication by a one-hot vector) and one at the output layer, for a fused softmax and top-N selection (commonly used in beam search). Our methods achieve speedups over state-of-the-art parallel GPU baselines of up to 7x and 50x, respectively. We also illustrate how our methods scale on different GPU architectures.",,,,ACL
627,2019,An Automated Framework for Fast Cognate Detection and Bayesian Phylogenetic Inference in Computational Historical Linguistics,"Taraka Rama, Johann-Mattis List","We present a fully automated workflow for phylogenetic reconstruction on large datasets, consisting of two novel methods, one for fast detection of cognates and one for fast Bayesian phylogenetic inference. Our results show that the methods take less than a few minutes to process language families that have so far required large amounts of time and computational power. Moreover, the cognates and the trees inferred from the method are quite close, both to gold standard cognate judgments and to expert language family trees. Given its speed and ease of application, our framework is specifically useful for the exploration of very large datasets in historical linguistics.",,,,ACL
628,2019,Sentence Centrality Revisited for Unsupervised Summarization,"Hao Zheng, Mirella Lapata","Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a) we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.",,,,ACL
629,2019,Discourse Representation Parsing for Sentences and Documents,"Jiangming Liu, Shay B. Cohen, Mirella Lapata",We introduce a novel semantic parsing task based on Discourse Representation Theory (DRT; Kamp and Reyle 1993). Our model operates over Discourse Representation Tree Structures which we formally define for sentences and documents. We present a general framework for parsing discourse structures of arbitrary length and granularity. We achieve this with a neural model equipped with a supervised hierarchical attention mechanism and a linguistically-motivated copy strategy. Experimental results on sentence- and document-level benchmarks show that our model outperforms competitive baselines by a wide margin.,,,,ACL
630,2019,Inducing Document Structure for Aspect-based Summarization,"Lea Frermann, Alexandre Klementiev","Automatic summarization is typically treated as a 1-to-1 mapping from document to summary. Documents such as news articles, however, are structured and often cover multiple topics or aspects; and readers may be interested in only some of them. We tackle the task of aspect-based summarization, where, given a document and a target aspect, our models generate a summary centered around the aspect. We induce latent document structure jointly with an abstractive summarization objective, and train our models in a scalable synthetic setup. In addition to improvements in summarization over topic-agnostic baselines, we demonstrate the benefit of the learnt document structure: we show that our models (a) learn to accurately segment documents by aspect; (b) can leverage the structure to produce both abstractive and extractive aspect-based summaries; and (c) that structure is particularly advantageous for summarizing long documents. All results transfer from synthetic training documents to natural news articles from CNN/Daily Mail and RCV1.",,,,ACL
631,2019,Incorporating Priors with Feature Attribution on Text Classification,"Frederick Liu, Besim Avci","Feature attribution methods, proposed recently, help users interpret the predictions of complex models. Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building. To demonstrate the effectiveness our technique, we apply it to two tasks: (1) mitigating unintended bias in text classifiers by neutralizing identity terms; (2) improving classifier performance in scarce data setting by forcing model to focus on toxic terms. Our approach adds an L2 distance loss between feature attributions and task-specific prior values to the objective. Our experiments show that i) a classifier trained with our technique reduces undesired model biases without a tradeoff on the original task; ii) incorporating prior helps model performance in scarce data settings.",,,,ACL
632,2019,Matching Article Pairs with Graphical Decomposition and Convolutions,"Bang Liu, Di Niu, Haojie Wei, Jinghong Lin, Yancheng He","Identifying the relationship between two articles, e.g., whether two articles published from different sources describe the same breaking news, is critical to many document understanding tasks. Existing approaches for modeling and matching sentence pairs do not perform well in matching longer documents, which embody more complex interactions between the enclosed entities than a sentence does. To model article pairs, we propose the Concept Interaction Graph to represent an article as a graph of concepts. We then match a pair of articles by comparing the sentences that enclose the same concept vertex through a series of encoding techniques, and aggregate the matching signals through a graph convolutional network. To facilitate the evaluation of long article matching, we have created two datasets, each consisting of about 30K pairs of breaking news articles covering diverse topics in the open domain. Extensive evaluations of the proposed methods on the two datasets demonstrate significant improvements over a wide range of state-of-the-art methods for natural language matching.",,,,ACL
633,2019,Hierarchical Transfer Learning for Multi-label Text Classification,"Siddhartha Banerjee, Cem Akkaya, Francisco Perez-Sorrosal, Kostas Tsioutsiouliklis","Multi-Label Hierarchical Text Classification (MLHTC) is the task of categorizing documents into one or more topics organized in an hierarchical taxonomy. MLHTC can be formulated by combining multiple binary classification problems with an independent classifier for each category. We propose a novel transfer learning based strategy, HTrans, where binary classifiers at lower levels in the hierarchy are initialized using parameters of the parent classifier and fine-tuned on the child category classification task. In HTrans, we use a Gated Recurrent Unit (GRU)-based deep learning architecture coupled with attention. Compared to binary classifiers trained from scratch, our HTrans approach results in significant improvements of 1% on micro-F1 and 3% on macro-F1 on the RCV1 dataset. Our experiments also show that binary classifiers trained from scratch are significantly better than single multi-label models.",,,,ACL
634,2019,Bias Analysis and Mitigation in the Evaluation of Authorship Verification,"Janek Bevendorff, Matthias Hagen, Benno Stein, Martin Potthast","The PAN series of shared tasks is well known for its continuous and high quality research in the field of digital text forensics. Among others, PAN contributions include original corpora, tailored benchmarks, and standardized experimentation platforms. In this paper we review, theoretically and practically, the authorship verification task and conclude that the underlying experiment design cannot guarantee pushing forward the state of the art—in fact, it allows for top benchmarking with a surprisingly straightforward approach. In this regard, we present a “Basic and Fairly Flawed” (BAFF) authorship verifier that is on a par with the best approaches submitted so far, and that illustrates sources of bias that should be eliminated. We pinpoint these sources in the evaluation chain and present a refined authorship corpus as effective countermeasure.",,,,ACL
635,2019,Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments,"Chung-Chi Chen, Hen-Hsen Huang, Hiroya Takamura, Hsin-Hsi Chen","In this paper, we attempt to answer the question of whether neural network models can learn numeracy, which is the ability to predict the magnitude of a numeral at some specific position in a text description. A large benchmark dataset, called Numeracy-600K, is provided for the novel task. We explore several neural network models including CNN, GRU, BiGRU, CRNN, CNN-capsule, GRU-capsule, and BiGRU-capsule in the experiments. The results show that the BiGRU model gets the best micro-averaged F1 score of 80.16%, and the GRU-capsule model gets the best macro-averaged F1 score of 64.71%. Besides discussing the challenges through comprehensive experiments, we also present an important application scenario, i.e., detecting exaggerated information, for the task.",,,,ACL
636,2019,Large-Scale Multi-Label Text Classification on EU Legislation,"Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, Ion Androutsopoulos","We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, annotated with ∼4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass BERT’s maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases.",,,,ACL
637,2019,Why Didn’t You Listen to Me? Comparing User Control of Human-in-the-Loop Topic Models,"Varun Kumar, Alison Smith-Renner, Leah Findlater, Kevin Seppi, Jordan Boyd-Graber","To address the lack of comparative evaluation of Human-in-the-Loop Topic Modeling (HLTM) systems, we implement and evaluate three contrasting HLTM modeling approaches using simulation experiments. These approaches extend previously proposed frameworks, including constraints and informed prior-based methods. Users should have a sense of control in HLTM systems, so we propose a control metric to measure whether refinement operations’ results match users’ expectations. Informed prior-based methods provide better control than constraints, but constraints yield higher quality topics.",,,,ACL
638,2019,Encouraging Paragraph Embeddings to Remember Sentence Identity Improves Classification,"Tu Vu, Mohit Iyyer","While paragraph embedding models are remarkably effective for downstream classification tasks, what they learn and encode into a single vector remains opaque. In this paper, we investigate a state-of-the-art paragraph embedding method proposed by Zhang et al. (2017) and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstruction-based objective of Zhang et al. (2017) with our sentence content probe objective in a semi-supervised setting. Despite its simplicity, our objective improves over paragraph reconstruction in terms of (1) downstream classification accuracies on benchmark datasets, (2) faster training, and (3) better generalization ability.",,,,ACL
639,2019,A Multi-Task Architecture on Relevance-based Neural Query Translation,"Sheikh Muhammad Sarwar, Hamed Bonab, James Allan","We describe a multi-task learning approach to train a Neural Machine Translation (NMT) model with a Relevance-based Auxiliary Task (RAT) for search query translation. The translation process for Cross-lingual Information Retrieval (CLIR) task is usually treated as a black box and it is performed as an independent step. However, an NMT model trained on sentence-level parallel data is not aware of the vocabulary distribution of the retrieval corpus. We address this problem and propose a multi-task learning architecture that achieves 16% improvement over a strong baseline on Italian-English query-document dataset. We show using both quantitative and qualitative analysis that our model generates balanced and precise translations with the regularization effect it achieves from multi-task learning paradigm.",,,,ACL
640,2019,Topic Modeling with Wasserstein Autoencoders,"Feng Nan, Ran Ding, Ramesh Nallapati, Bing Xiang","We propose a novel neural topic model in the Wasserstein autoencoders (WAE) framework. Unlike existing variational autoencoder based models, we directly enforce Dirichlet prior on the latent document-topic vectors. We exploit the structure of the latent space and apply a suitable kernel in minimizing the Maximum Mean Discrepancy (MMD) to perform distribution matching. We discover that MMD performs much better than the Generative Adversarial Network (GAN) in matching high dimensional Dirichlet distribution. We further discover that incorporating randomness in the encoder output during training leads to significantly more coherent topics. To measure the diversity of the produced topics, we propose a simple topic uniqueness metric. Together with the widely used coherence measure NPMI, we offer a more wholistic evaluation of topic quality. Experiments on several real datasets show that our model produces significantly better topics than existing topic models.",,,,ACL
641,2019,Dense Procedure Captioning in Narrated Instructional Videos,"Botian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen","Understanding narrated instructional videos is important for both research and real-world web applications. Motivated by video dense captioning, we propose a model to generate procedure captions from narrated instructional videos which are a sequence of step-wise clips with description. Previous works on video dense captioning learn video segments and generate captions without considering transcripts. We argue that transcripts in narrated instructional videos can enhance video representation by providing fine-grained complimentary and semantic textual information. In this paper, we introduce a framework to (1) extract procedures by a cross-modality module, which fuses video content with the entire transcript; and (2) generate captions by encoding video frames as well as a snippet of transcripts within each extracted procedure. Experiments show that our model can achieve state-of-the-art performance in procedure extraction and captioning, and the ablation studies demonstrate that both the video frames and the transcripts are important for the task.",,,,ACL
642,2019,Latent Variable Model for Multi-modal Translation,"Iacer Calixto, Miguel Rios, Wilker Aziz","In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation (MMT) through a latent variable model. This latent variable can be seen as a multi-modal stochastic embedding of an image and its description in a foreign language. It is used in a target-language decoder and also to predict image features. Importantly, our model formulation utilises visual and textual inputs during training but does not require that images be available at test time. We show that our latent variable MMT formulation improves considerably over strong baselines, including a multi-task learning approach (Elliott and Kadar, 2017) and a conditional variational auto-encoder approach (Toyama et al., 2016). Finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a constraint on the KL term to promote models with non-negligible mutual information between inputs and latent variable, and (iii) by training on additional target-language image descriptions (i.e. synthetic data).",,,,ACL
643,2019,Identifying Visible Actions in Lifestyle Vlogs,"Oana Ignat, Laura Burdick, Jia Deng, Rada Mihalcea","We consider the task of identifying human actions visible in online videos. We focus on the widely spread genre of lifestyle vlogs, which consist of videos of people performing actions while verbally describing them. Our goal is to identify if actions mentioned in the speech description of a video are visually present. We construct a dataset with crowdsourced manual annotations of visible actions, and introduce a multimodal algorithm that leverages information derived from visual and linguistic clues to automatically infer which actions are visible in a video.",,,,ACL
644,2019,A Corpus for Reasoning about Natural Language Grounded in Photographs,"Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai","We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.",,,,ACL
645,2019,"Learning to Discover, Ground and Use Words with Segmental Neural Language Models","Kazuya Kawakami, Chris Dyer, Phil Blunsom","We propose a segmental neural language model that combines the generalization power of neural networks with the ability to discover word-like units that are latent in unsegmented character sequences. In contrast to previous segmentation models that treat word segmentation as an isolated task, our model unifies word discovery, learning how words fit together to form sentences, and, by conditioning the model on visual context, how words’ meanings ground in representations of nonlinguistic modalities. Experiments show that the unconditional model learns predictive distributions better than character LSTM models, discovers words competitively with nonparametric Bayesian word segmentation models, and that modeling language conditional on visual context improves performance on both.",,,,ACL
646,2019,What Should I Ask? Using Conversationally Informative Rewards for Goal-oriented Visual Dialog.,"Pushkar Shukla, Carlos Elmadjian, Richika Sharan, Vivek Kulkarni, Matthew Turk","The ability to engage in goal-oriented conversations has allowed humans to gain knowledge, reduce uncertainty, and perform tasks more efficiently. Artificial agents, however, are still far behind humans in having goal-driven conversations. In this work, we focus on the task of goal-oriented visual dialogue, aiming to automatically generate a series of questions about an image with a single objective. This task is challenging since these questions must not only be consistent with a strategy to achieve a goal, but also consider the contextual information in the image. We propose an end-to-end goal-oriented visual dialogue system, that combines reinforcement learning with regularized information gain. Unlike previous approaches that have been proposed for the task, our work is motivated by the Rational Speech Act framework, which models the process of human inquiry to reach a goal. We test the two versions of our model on the GuessWhat?! dataset, obtaining significant results that outperform the current state-of-the-art models in the task of generating questions to find an undisclosed object in an image.",,,,ACL
647,2019,Symbolic Inductive Bias for Visually Grounded Learning of Spoken Language,Grzegorz Chrupała,"A widespread approach to processing spoken language is to first automatically transcribe it into text. An alternative is to use an end-to-end approach: recent works have proposed to learn semantic embeddings of spoken language from images with spoken captions, without an intermediate transcription step. We propose to use multitask learning to exploit existing transcribed speech within the end-to-end setting. We describe a three-task architecture which combines the objectives of matching spoken captions with corresponding images, speech with text, and text with images. We show that the addition of the speech/text task leads to substantial performance improvements on image retrieval when compared to training the speech/image task in isolation. We conjecture that this is due to a strong inductive bias transcribed speech provides to the model, and offer supporting evidence for this.",,,,ACL
648,2019,Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog,"Zhe Gan, Yu Cheng, Ahmed Kholy, Linjie Li, Jingjing Liu","This paper presents a new model for visual dialog, Recurrent Dual Attention Network (ReDAN), using multi-step reasoning to answer a series of questions about an image. In each question-answering turn of a dialog, ReDAN infers the answer progressively through multiple reasoning steps. In each step of the reasoning process, the semantic representation of the question is updated based on the image and the previous dialog history, and the recurrently-refined representation is used for further reasoning in the subsequent step. On the VisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art of 64.47% NDCG score. Visualization on the reasoning process further demonstrates that ReDAN can locate context-relevant visual and textual clues via iterative refinement, which can lead to the correct answer step-by-step.",,,,ACL
649,2019,Lattice Transformer for Speech Translation,"Pei Zhang, Niyu Ge, Boxing Chen, Kai Fan","Recent advances in sequence modeling have highlighted the strengths of the transformer architecture, especially in achieving state-of-the-art machine translation results. However, depending on the up-stream systems, e.g., speech recognition, or word segmentation, the input to translation system can vary greatly. The goal of this work is to extend the attention mechanism of the transformer to naturally consume the lattice in addition to the traditional sequential input. We first propose a general lattice transformer for speech translation where the input is the output of the automatic speech recognition (ASR) which contains multiple paths and posterior scores. To leverage the extra information from the lattice structure, we develop a novel controllable lattice attention mechanism to obtain latent representations. On the LDC Spanish-English speech translation corpus, our experiments show that lattice transformer generalizes significantly better and outperforms both a transformer baseline and a lattice LSTM. Additionally, we validate our approach on the WMT 2017 Chinese-English translation task with lattice inputs from different BPE segmentations. In this task, we also observe the improvements over strong baselines.",,,,ACL
650,2019,Informative Image Captioning with External Sources of Information,"Sanqiang Zhao, Piyush Sharma, Tomer Levinboim, Radu Soricut","An image caption should fluently present the essential information in a given image, including informative, fine-grained entity mentions and the manner in which these entities interact. However, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important “informativeness” dimension. We present a mechanism for integrating image information together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the image in a fluent and informative manner. We introduce a multimodal, multi-encoder model based on Transformer that ingests both image features and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative.",,,,ACL
651,2019,CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication,"Jin-Hwa Kim, Nikita Kitaev, Xinlei Chen, Marcus Rohrbach, Byoung-Tak Zhang","In this work, we propose a goal-driven collaborative task that combines language, perception, and action. Specifically, we develop a Collaborative image-Drawing game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages exchanged between human players. We define protocols and metrics to evaluate learned agents in this testbed, highlighting the need for a novel “crosstalk” evaluation condition which pairs agents trained independently on disjoint subsets of the training data. We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans.",,,,ACL
652,2019,Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning,"Zhihao Fan, Zhongyu Wei, Siyuan Wang, Xuanjing Huang","Image Captioning aims at generating a short description for an image. Existing research usually employs the architecture of CNN-RNN that views the generation as a sequential decision-making process and the entire dataset vocabulary is used as decoding space. They suffer from generating high frequent n-gram with irrelevant words. To tackle this problem, we propose to construct an image-grounded vocabulary, based on which, captions are generated with limitation and guidance. In specific, a novel hierarchical structure is proposed to construct the vocabulary incorporating both visual information and relations among words. For generation, we propose a word-aware RNN cell incorporating vocabulary information into the decoding process directly. Reinforce algorithm is employed to train the generator using constraint vocabulary as action space. Experimental results on MS COCO and Flickr30k show the effectiveness of our framework compared to some state-of-the-art models.",,,,ACL
653,2019,Distilling Translations with Visual Awareness,"Julia Ive, Pranava Madhyastha, Lucia Specia","Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.",,,,ACL
654,2019,VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions,"Pranava Madhyastha, Josiah Wang, Lucia Specia","We address the task of evaluating image description generation systems. We propose a novel image-aware metric for this task: VIFIDEL. It estimates the faithfulness of a generated caption with respect to the content of the actual image, based on the semantic similarity between labels of objects depicted in images and words in the description. The metric is also able to take into account the relative importance of objects mentioned in human reference descriptions during evaluation. Even if these human reference descriptions are not available, VIFIDEL can still reliably evaluate system descriptions. The metric achieves high correlation with human judgments on two well-known datasets and is competitive with metrics that depend on and rely exclusively on human references.",,,,ACL
655,2019,Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation,"Ronghang Hu, Daniel Fried, Anna Rohrbach, Dan Klein, Trevor Darrell","Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.",,,,ACL
656,2019,Multimodal Transformer for Unaligned Multimodal Language Sequences,"Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency","Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.",,,,ACL
657,2019,"Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports","Baoyu Jing, Zeya Wang, Eric Xing","Chest X-Ray (CXR) images are commonly used for clinical screening and diagnosis. Automatically writing reports for these images can considerably lighten the workload of radiologists for summarizing descriptive findings and conclusive impressions. The complex structures between and within sections of the reports pose a great challenge to the automatic report generation. Specifically, the section Impression is a diagnostic summarization over the section Findings; and the appearance of normality dominates each section over that of abnormality. Existing studies rarely explore and consider this fundamental structure information. In this work, we propose a novel framework which exploits the structure information between and within report sections for generating CXR imaging reports. First, we propose a two-stage strategy that explicitly models the relationship between Findings and Impression. Second, we design a novel co-operative multi-agent system that implicitly captures the imbalanced distribution between abnormality and normality. Experiments on two CXR report datasets show that our method achieves state-of-the-art performance in terms of various evaluation metrics. Our results expose that the proposed approach is able to generate high-quality medical reports through integrating the structure information.",,,,ACL
658,2019,Visual Story Post-Editing,"Ting-Yao Hsu, Chieh-Yang Huang, Yen-Chia Hsu, Ting-Hao Huang","We introduce the first dataset for human edits of machine-generated visual stories and explore how these collected edits may be used for the visual story post-editing task. The dataset ,VIST-Edit, includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions. We establish baselines for the task, showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models. We also discuss the weak correlation between automatic evaluation scores and human ratings, motivating the need for new automatic metrics.",,,,ACL
659,2019,Multimodal Abstractive Summarization for How2 Videos,"Shruti Palaskar, Jindřich Libovický, Spandana Gella, Florian Metze","In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to “compress” text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU.",,,,ACL
660,2019,Learning to Relate from Captions and Bounding Boxes,"Sarthak Garg, Joel Ruben Antony Moniz, Anshu Aviral, Priyatham Bollimpalli","In this work, we propose a novel approach that predicts the relationships between various entities in an image in a weakly supervised manner by relying on image captions and object bounding box annotations as the sole source of supervision. Our proposed approach uses a top-down attention mechanism to align entities in captions to objects in the image, and then leverage the syntactic structure of the captions to align the relations. We use these alignments to train a relation classification network, thereby obtaining both grounded captions and dense relationships. We demonstrate the effectiveness of our model on the Visual Genome dataset by achieving a recall@50 of 15% and recall@100 of 25% on the relationships present in the image. We also show that the model successfully predicts relations that are not present in the corresponding captions.",,,,ACL
